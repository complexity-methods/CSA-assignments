---
title: "DCS Assignments Part X"
author: "Fred Hasselman & Maarten Wijnants"
date: "1/14/2018"
output: 
  bookdown::html_document2: 
    variant: markdown+hard_line_breaks
    fig_caption: yes
    highlight: pygments
    keep_md: yes
    number_sections: yes
    theme: spacelab
    toc: yes
    toc_float: true
    collapsed: false
    smooth_scroll: true
    code_folding: show
    bibliography: [refs.bib, packages.bib]
    biblio-style: apalike
    csl: apa.csl
    includes:
        before_body: assignmentstyle.html
    pandoc_args: ["--number-offset","7"]
    
---


```{r, include=FALSE}
knitr::opts_chunk$set(include=TRUE, tidy = FALSE)
```


# **Quick Links** {-}

* [Main Assignments Page](https://darwin.pwo.ru.nl/skunkworks/courseware/1718_DCS/assignments/)
* [Assignments Part 1A: Introduction to the mathematics of change](https://darwin.pwo.ru.nl/skunkworks/courseware/1718_DCS/assignments/ASSIGNMENTS_P1A.html)
* [Assignments Part 1B: Fitting Parameters and Potential Functions](https://darwin.pwo.ru.nl/skunkworks/courseware/1718_DCS/assignments/ASSIGNMENTS_P1B.html)
* [Assignments Part 2: Time Series Analysis: Temporal Correlations and Fractal Scaling](https://darwin.pwo.ru.nl/skunkworks/courseware/1718_DCS/assignments/ASSIGNMENTS_P2.html)
* [Assignments Part 3: Quantifying Recurrences in State Space](https://darwin.pwo.ru.nl/skunkworks/courseware/1718_DCS/assignments/ASSIGNMENTS_P3.html)
  
</br>
</br>


# **Early Warning Signals and Complex Networks**


## **Early Warning Signals in a Clinical Case Study**

Dynamic assessment (e.g. experience sampling) is increasingly used in the mental health field. Process monitoring with dynamic assessment can empower patients and benefit self-insight (van Os et al., 2017). Additionally, the experience sampling (ES) time series can be analyzed and the results can be used to inform the treatment process. 
In this assignment, we will use complex systems theory and methodology to perform a quantitative case study for a real patient. 

### First Steps {.tabset .tabset-fade .tabset-pills}

You will analyze ES data from a patient who received impatient psychotherapy for mood disorders. The patient answered questions from the therapy process questionnaire on a daily basis for the full 101 days of his treatment. 
The learning objectives of the assignments are as follows:
1) You can interpret plots of raw ES time series, as well as complexity resonance diagrams and critical instability diagrams of ES time series.
2) You can describe what possible change processes in complex systems could explain the quantitative results from the case study. 


#### Questions {-}

a) Open R and load the data 'clinical.case.study.csv' 
b) Look at the data frame. Where is time in this data? In the rows or the columns?
c) Factor 7 yields the scores on 'Impairment by Symptoms and Problems' for this patient on each day. Plot the time series. What do you see?


#### Answers {-}




### First think, then analyze (,then aggregate) {.tabset .tabset-fade .tabset-pills}

Think about what you know from the literature and the lecture. Since the goal of psychotherapy is to change the existing state of a patient, we expect to find periods of destabilization in which the attraction of the existing psychopathological state is decreased. During a destabilization period, new, possibly more healthy states can emerge. Clinical improvement than constitutes the phase transition from a pathological to a more healthy state.  

Earlier in the course, we learned that destabilization and phase transitions occur as a function of changing control parameters. Additionally, strong external influences can induce destabilization and phase transitions.

#### Questions {-}

Answer the following questions, there are no wrong answers, just try to think from a complex systems perspective:
a)	Can you think of possible control parameters for phase transitions in psychotherapy? 
b)	Can you think of possible external influences and events that relate to destabilization in psychotherapy?
c)	Based on your answers on a) and b), where in the psychotherapeutic process do you expect to find periods of destabilization?
d)	Take a look again at your plot of factor 7 for our patient. Where in the psychotherapeutic process of our patient do you expect to find periods of destabilization?

#### Answers {-}


### Phase transitions in symptom severity {.tabset .tabset-fade .tabset-pills}

In question 2, you might have hypothesized the occurrence of a phase transition in the impairment by symptoms and problems of our patient. Remember, that a phase transition is an abrupt qualitative shift in the systems behaviour. Unfortunately, we cannot see qualitative shifts in our quantitative data. However, we can try to quantify abrupt shifts in factor 7 of our patient. 

#### Questions {-}

a)	Analyze the time series of factor 7 using the function ts_levels()
b)	Describe verbally the change trajectory in the symptom severity of our patient. 

#### Answers {-}




<!--chapter:end:Assignment_Clinical_Case.Rmd-->

---
title: "Mathematics of Change Assignments - Multivariate Models"
author: "Fred Hasselman"
date: "1/14/2019"
output: 
  bookdown::html_document2: 
    variant: markdown+hard_line_breaks
    fig_caption: yes
    highlight: pygments
    keep_md: yes
    number_sections: yes
    self_contained: yes
    theme: spacelab
    toc: yes
    toc_float: true
    collapsed: false
    smooth_scroll: true
    code_folding: show
    bibliography: [refs.bib, packages.bib]
    biblio-style: apalike
    csl: apa.csl
    includes:
        before_body: assignmentstyle.html
    pandoc_args: ["--number-offset","1"]
editor_options: 
  chunk_output_type: console
---

```{r, include=FALSE}
knitr::opts_chunk$set(include=TRUE, tidy = FALSE)
```


# **Multivariate systems**

By multivariate systems we do not simply mean there is more than 1 iterative process (dynamical variable), but that there is a coupling between at least two iterative processes that makes the outcomes of each individual process at each time step (partially) dependent on the previous time step of other processes.

We'll start by expanding on the previous assignment ([EXTRA: van Geert's growth model](https://docs.google.com/spreadsheets/d/1DAg0u-zMFOIvRSDOZDxqnzyS0HQJg4FIXzJIMvEMwiI/edit?usp=sharing)), in which changes in the parameter values were modelled by implementing an `IF ... THEN` rule that evaluated whether some threshold level was reached. In the first assignment you'll create a kind of **one-directional coupling**, or **supportive interaction**, which is just a precursor to modelling more interesting interaction dynamics.

You'll also model the so-called Lottka-Volterra equations, representing **competetive interactions** between two iterative processes, also known as **predator-prey dynamics**. You can imagine these equations represent populations of *Foxes* and *Rabbits* in which the population of the  predators (Foxes) depends on the available prey (Rabbits) and vice versa. To do model these equations, you'll need to learn a bit about modelling **flows**: Continuous time differential equations. 


## **One directional coupling**

A more realistic way to model a change of growth parameter `r` using `IF ... THEN` statements, is to make the growth of a variable depend on another process. The proper *dynamical* way to do this would be to define a coupled system of difference or differential equations in which the interaction dynamics completely regulate growth (or decay) of the variables directly, such as the predator-prey system in the next assignment. 

In this assignment we'll keep using at least one 'conditional' rule and try to implement emergent levels by creating **supportive interactions** between separate processes. This is probably the most simple version of *interaction dynamics*, *coupling dynamics*, or *multiplicative interactions* with more then one iterative process. Basically, the processes will still be rather independent from one another and the interaction simply means a 'shock' is delivered to an individual process, after which the system just continuous to evolve on its own. 


### Connected growers {.tabset .tabset-fade .tabset-pills}

For this assignment, we'll use a different way to achieve the levels created in the previous assignment in which   `IF ... THEN` statements were used to achieve changes in dynamics. The conditional rules that change the parameters were 'given', we'll try to be a bit more dynamical... 

Have a look at [this spreadsheet](https://docs.google.com/spreadsheets/d/1cbC58yTg-pOoThMQY0UyzIQ3ZiaTosJlqQFkRHt9LJg/edit?usp=sharing) in which a model is implemented that is discussed in the following chapter by Paul van Geert:

[Van Geert, P. (2003). Dynamic systems approaches and modeling of developmental processes. In J. Valsiner and K. J. Conolly (Eds.), Handbook of developmental Psychology. London: Sage. Pp. 640-672](http://www.paulvangeert.nl/publications_files/Handbook%20Valsiner%20and%20Conolly.pdf).

Relevant text pertaining to this assignment:

* BUILDING YOUR OWN MODELS: A SHORT TUTORIAL (p. 665)
    + The model is discussed in "A Model of Hierarchically Connected Growers"

#### Questions {-}

Try to model this example of supportive interaction in `R`. 

```{block2, type="rmdnote"}
The spreadsheetmodel tries to mimic Figures 28.6 to 28.8 in van Geert (2003), but only column `A` receives a 'support' boost As noted in the text:
  
"A similar set of equations can be set up, applying to columns B and C respectively."

```


* You can use the template function below
* You can look in the spreadsheet, or look in the chapter to find the right formula's for `A` and `B`
* Make a plot like in the spreadsheet
    + Note that the function returns a data frame with A, B and C

```{r, eval=FALSE}
vanGeert.support <- function(A0 = 0.01, Ar = .5, AK = 1, 
                             B0 = 0.01, Br = .2, BK = 1, 
                             C0 = 0, C1 = 1, Cr = 1, CK = 1, 
                             AtoC = 0.8, BsupportA = .5, N = 100){
  
  # Create a vector A,B,C of length N, which has value A0,B0,C0 at A[1],B[1],C[1]
  A <- c(A0, rep(NA, N-1))
  B <- c(B0, rep(NA, N-1))
  C <- c(C0, rep(NA, N-1))
  
  # Then, you need create the iteration
  for(i in 1:(N-1)){
    
    # You need to monitor the value of L and change the growth rate according to a rule
    if(A[i]<AtoC){
      C[i+1] <- # Value of C0
    } else {
      C[i+1] <- # Value of C1
    }
    
    A[i+1] <- # Implement A with support from B
    B[i+1] <- # Implement B whose growth depends on C
  }
  
  return(data.frame(A=A,B=B,C=C))
}

```



#### Answers {-}

* The template filled in with the correct functions and parameter settings

```{r, eval=TRUE}
vanGeert.support <- function(A0 = 0.01, Ar = .5, AK = 1, 
                             B0 = 0.01, Br = .2, BK = 1, 
                             C0 = 0, C1 = 1, Cr = 1, CK = 1, 
                             AtoC = 0.8, BsupportA = .5, N = 100){
  
  # Create a vector A,B,C of length N, which has value A0,B0,C0 at A[1],B[1],C[1]
  A <- c(A0, rep(NA, N-1))
  B <- c(B0, rep(NA, N-1))
  C <- c(C0, rep(NA, N-1))
  
  # Then, you need create the iteration
  for(i in 1:(N-1)){
    
    # You need to monitor the value of L and change the growth rate according to a rule
    if(A[i]<AtoC){
      C[[i+1]] <- C0
    } else {
      C[[i+1]] <- C1
    }
    
    A[i+1] <- A[i] + A[i] * Ar * (1 - A[i] / AK) + A[i] * B[i] * BsupportA # A10 + A10 * $A$5 * (1 - A10 / $A$4) + A10 * B10 * $B$2
    B[i+1] <- B[i] + B[i] * Br * (1 - B[i] / BK) * C[i]                    # B10 + B10 * $B$5 * (1 - B10 / $B$4) * C10
    
  }
  
  return(data.frame(A=A,B=B,C=C))
}
```

* Make a plot like in the spreadsheet
    + Note that the function returns a data frame with A, B and C

```{r}
Y <- vanGeert.support()

ts.plot(ts(Y$A), ts(Y$B), ts(Y$C),
        gpars = list(xlab = "time (a.u.)",
                     ylab = expression(Y(t)),
                     main = expression(paste("Hierarchical Connected Growers ")),
                     lwd = rep(2,3),
                     lty = c(1:3),
                     col = c("darkred","darkblue","darkgreen")
                     )
        )
legend(50, .8, c("B will support A by adding .5*B",
                 paste0("B starts to grow at time ",which(Y$A > .8)[1]), 
                 paste0("C turns to 1 at time ",which(Y$A > .8)[1])),
       lwd = rep(2,3), lty = c(1:3), col = c("darkred","darkblue","darkgreen"), merge = TRUE)

```


* This would be the *not-so-dynamical* way to do it:

```{r, eval=TRUE, include=TRUE}
library(casnet)

# Generate 3 timeseries
Y1 <- casnet::growth_ac(k = 2, r =.2, type = "vanGeert")
# Y2 and Y3 start at r = 0.001
Y3 <- Y2 <- casnet::growth_ac(k = 2, r = 0.001, type = "vanGeert")

# Y2 and Y3 start when k is approached
c1 <- 1.6
c2 <- 2.2

# Here we just swap the new values with the old values
Y2[Y1 > c1] <- casnet::growth_ac(r = .3, k = 3, type = "vanGeert", N = sum(Y1 > c1))
Y3[Y2 > c2] <- casnet::growth_ac(r = .5, k = 4, type = "vanGeert", N = sum(Y2 > c2))

# Make a nice plot
ts.plot(Y1, Y2, Y3,
        gpars = list(xlab = "time (a.u.)",
                     ylab = expression(Y(t)),
                     main = expression(paste("'Unconnected' Growers ",Y[t+1]==Y[t]*(1 + r - r*Y[t]))),
                     lwd = rep(2,3),
                     lty = c(1:3),
                     col = c("darkred","darkblue","darkgreen")
                     )
        )
legend(1, 3.8, c("Y1(0):  r = .2",
                 paste0("Y2(",which(Y1 > c1)[1],"): r = .3"), 
                 paste0("Y3(",which(Y2 > c2)[1],"): r = .5")),
       lwd = rep(2,3), lty = c(1:3), col = c("darkred","darkblue","darkgreen"), merge = TRUE)
```


### Other demonstrations of dynamic modeling using spreadsheets

See the website by [Paul Van Geert](http://www.paulvangeert.nl/research.htm), scroll down to see models of:

* Learning and Teaching
* Behaviour Modification
* Connected Growers
* Interaction during Play

</br>



</br>
</br>

## **The predator-prey model**


In this assignment we will look at a 2D coupled dynamical system: **the Predator-Prey model** (aka [Lotka-Volterra equations](https://en.wikipedia.org/wiki/Lotka-Volterra_equations)).      

The dynamical system is given by the following set of first-order differential equations, one represents changes in a population of predators, (e.g., **F**oxes: $f_F(R_t,F_t)$ ), the other represents changes in a population of prey, (e.g., **R**abbits: $f_R(R_t,F_t)$ ).


\begin{align}
\frac{dR}{dt}&=(a-b*F)*R \\
\\
\frac{dF}{dt}&=(c*R-d)*F
(\#eq:lv)
\end{align}


This is not a *difference* equation but a *differential* equation, which means building this system is not as straightforward as was the case in the previous assignments. Simulation requires a numerical method to 'solve' this differential equation for time, which means we need a method to approach, or estimate continuous time in discrete time. Below you will receive a speed course in one of the simplest numerical procedures for integrating differential equations, the [Euler method](https://en.wikipedia.org/wiki/Euler_method) ([also see the notes in the course book](https://darwin.pwo.ru.nl/skunkworks/courseware/1819_DCS/numerical-integration-to-simulate-continuous-time.html)).    


### Euler's Method {-}

A general differential equation is given by:

\begin{equation}
\frac{dx}{dt} = f(x)
(\#eq:diff)
\end{equation}

Read it as saying: "_a change in $x$ over a change in time is a function of $x$ itself_". This can be approximated by considering the change to be over some constant, small time step $\Delta$:

\begin{equation}
\frac{(x_{t+1} = x_t)}{\Delta} = f(x_t)
(\#eq:Euler)
\end{equation}

After rearranging the terms a more familiar form reveals itself:

\begin{align}
x_{t+1} &= x_t &= f(x_t) * \Delta \\
x_{t+1} &= f(x_t) * \Delta + x_t
(\#eq:Euler2)
\end{align}

This looks like an ordinary iterative process, $\Delta$ the *time constant* determines the size of time step taken at every successive iteration. For a 2D system with variables **R** and **F** on would write:
    
\begin{align}
R_{t+1} &= f_R(R_t,Ft) * \Delta + R_t \\
F_{t+1} &= f_F(R_t,F_t) * \Delta + F_t
(\#eq:EulerRF)
\end{align}
    
</br>
</br>

### Foxes and Rabbits in R {.tabset .tabset-fade .tabset-pills}

Use `R` to implement [the model in this spreadsheet](https://docs.google.com/spreadsheets/d/1rZDEo8XYNCzhRWrWOli7DDB6f4m87p-hjYv2_KlBLa4/edit?usp=sharing)


#### Questions{-}

* To get the right equations, you'll have to substitute $f_R(R_t,Ft)$ and $f_F(R_t,F_t)$ with the differential equations for Foxes and Rabbits provided above.
* Use the following parameter settings:
    + `N` = `1000`
    + `a` and `d` are `1`
    + `b` and `c` are `2` 
    + Initial values `R0` and `F0` are `0.1`
* Make a plot of the time series for Foxes and Rabbits
* Make a plot of the 2D state space of this system (aka *phase plane*)
    + Examine how the state space changes for different parameter settings

#### Answers {-}


The Euler set-up:
$$
\begin{align}
R_{t+1} &= f_R(R_t,Ft) * \Delta + R_t \\
F_{t+1} &= f_F(R_t,F_t) * \Delta + F_t
\end{align}
$$

With the equations:
$$
\begin{align}
R_{t+1} &=  (a-b*F_t)*R_t * \Delta + R_t \\
\\
F_{t+1} &=  (c*R_t-d)*F_t * \Delta + F_t
\end{align}
$$


```{r}
library(plyr)
library(lattice)

# Parameters
N  <- 1000
a  <- d <- 1
b  <- c <- 2 
R0 <- F0 <- 0.1
R  <- as.numeric(c(R0, rep(NA,N-1)))
F  <- as.numeric(c(F0, rep(NA,N-1)))

# Time constant
delta <- 0.01

# Numerical integration of the lpredator-prey system
l_ply(seq_along(R), function(t){
    R[[t+1]] <<- (a - b * F[t]) * R[t] * delta + R[t] 
    F[[t+1]] <<- (c * R[t] - d) * F[t] * delta + F[t] 
    })

# Note different behaviour when ts() is applied
lattice::xyplot(cbind(ts(R),ts(F)))
lattice::xyplot(R ~ F, pch = 16)
```


### **Growth functions in continuous time**

The result of applying a method of **numerical integration** such a s Euler's method is called a **numerical solution** of the differential equation. The **analytical solution** is the equation which will give you a value of $Y$ for any point in time, given an initial value $Y_0$. Systems which have an analytical solution can be used to test the accuracy of **numerical solutions**.

The logistic differential equation $\frac{dY}{dt} = r*Y_t*\left(1-\frac{Y_t}{K}\right)$ has an analytic solution given as:

$$
\frac{K * Y_0}{Y_0 + \left(K-Y_0\right) * e^{-r*t} }
$$
We have the implemented discrete time growth function in `growth_ac()`, but could easily adapt all those functions to make them continuous by using Euler's method. 

Below is a comparison of the analytic solution of the continuous logistic growth function, with the result of using Euler's method to approximate the continuous time function.

```{r}
library(plyr)
library(RColorBrewer)
# Parameter settings
delta <- .1
N <- 100

# W'ell make time vector to get a smoot analytis solution
TIME <- seq(1,N,by=1)

r <- 3.52 # Remember this setting? This means Chaos in discrete time, in continous time it means extreme growth.

# Carrying capacity and initial condition are chosen se we get a nice plot
K  <-  8
Y0 <- .01

deltas <- c(delta, delta*.5,delta*.25,delta*.2,delta*.15,delta*.1,delta*.05)
cl <- brewer.pal(length(deltas),"Set2")

# Numerical integration of the logistic differential equation as implemented in growth_ac()
Y.euler <- llply(deltas, function(d){
  # Use the values in deltas to get iterate the logistic flow
  Y <- as.numeric(c(Y0, rep(NA,length(TIME)-1)))
  ts(sapply(seq_along(TIME), function(t) Y[[t+1]] <<- ((r * Y[t] * (K - Y[t])) * d) + Y[t] ))
  })

# Analytical solution
# We have to scale TIME a bit, normally we would use 1/TIME
Y.analytic <- ts( K * Y0 / (Y0 + (K - Y0) * exp(-1*(r*TIME/30))) )

ts.plot(Y.euler[[1]], Y.euler[[2]] ,Y.euler[[3]], Y.euler[[4]], Y.euler[[5]], Y.euler[[6]], Y.euler[[7]], Y.analytic,
        gpars = list(xlab = "time (a.u.)",
                     ylab = expression(Y(t)),
                     main = expression(paste("Analytic vs. Numerical solution to ",frac(dY,dt)==Y[t]*r*(K-Y[t]))),
                     lwd = c(rep(1,length(deltas)),3),
                     lty = 1,
                     col = c(cl,"red3"),
                     ylim = c(0,10),
                     cex.main = .8
                     )
        )

legend(70, 5, c(paste("Euler: delta =",deltas),"Analytic solution (t/30)"),
       lwd = c(rep(1,length(deltas)),2),
       lty = 1,
       col = c(cl,"red3"),
       merge = TRUE, cex = .8)
```

```{block2, type="rmdnote"}
If you want to learn how to put mathematical operators in legends and axes using `expression()`, see the webpage [Mathematical Annotation in R](http://vis.supstat.com/2013/04/mathematical-annotation-in-r/)
```

When `delta` is relatively large, the fluctuations still resemble deterministic chaos, as delta gets closer to 0, this 'smooths out' the fluctuations and the analytic solution is approached. As a consequence, it takes longer and longer to reach `K`. However, 'longer' is of course relative to the unit of time represented on the x-axis, because we're simulating, they are arbitrary units.



## **Beyond Foxes and Rabbits**

In the [course book](https://complexity-methods.github.io/book/advmodels.html) we listed some more advanced models that build on the basic models we discussed here.

----


</br>
</br>



<!--chapter:end:ASSIGNMENTS_D1_2_mvSystems.Rmd-->

---
title: "Mathematics of Change Assignments - Univariate Systems"
author: "Fred Hasselman"
date: "1/14/2019"
output: 
  bookdown::html_document2: 
    variant: markdown+hard_line_breaks
    fig_caption: yes
    highlight: pygments
    keep_md: yes
    number_sections: yes
    self_contained: yes
    theme: spacelab
    toc: yes
    toc_float: true
    collapsed: false
    smooth_scroll: true
    code_folding: show
    bibliography: [refs.bib, packages.bib]
    biblio-style: apalike
    csl: apa.csl
    includes:
        before_body: assignmentstyle.html
    pandoc_args: ["--number-offset","0"]
editor_options: 
  chunk_output_type: console
---

```{r, include=FALSE}
knitr::opts_chunk$set(include=TRUE, tidy = FALSE)
```


# **Modelling (non-linear) growth and Deterministic Chaos**

In this assignment you will examine two (relatively) simple one-dimensional maps based on the models implemented in a spreadsheet. Just copy the linked Google Sheet to a new on-line Google Sheet, or, save it to your computer and use your favourite spreadsheet software (e.g., Excel, Numbers) to explore the behaviour of the model.    
    
We will start with the *Linear Map* and then proceed to the slightly more complicated *Logistic Map* (aka *Quadratic map*). You can also use `R` to model the dynamical processes, but for now we will assume you'll use the spreadsheets linked to in the assignments.

</br>

```{block2, type='rmdnote'}
Check the solutions of the assignments **after** you tried to do them on your own!    
```

</br>

## **The Linear Map**

</br>

Equation \@ref(eq:linmap) is the ordinary difference equation (ODE) discussed in [the course book](https://complexity-methods.github.io/book/introduction-to-the-mathematics-of-change.html) and is called the *Linear Map*:
  
\begin{equation}
Y_{i+1} = Y_{i=0} + r*Y_i
(\#eq:linmap)
\end{equation}
  
In these exercises you will *simulate* time series produced by the change process in equation \@ref(eq:linmap) for different parameter settings of the growth-rate parameter $r$ (the *control parameter*) and the initial conditions $Y_0$. *Simulation* is obviously different from a statistical analysis in which parameters are estimated from a data set. The goal of these assignments is to get a feeling for what a dynamical model is, and in what way it is different from a linear statistical regression model like GLM.

</br>

```{block2, type = "rmdnote"}
It is common practice to use $Y_{i}$ for the current state and $Y_{i+1}$ for the next state of a discrete time model (a map, a difference equation). For a continuous time model (a flow, a differential equation), sometimes $Y_{t}$ and $\hat{Y_t}$ are used. More common is the $\delta$, or *'change in'* notation: $\frac{dY}{dt} = Y_0 + r * Y(t)$, which you can pronounce as *'a change in Y over a change in time is a function of ...'*. However, different authors have different preferences and may use other notations.
```
  
</br>

### Growth in a Spreadsheet
  
</br>

```{block2, spreadset1, type='rmdimportant'}
If you want to modify the spreadsheets and use them as a template for other assignments, be sure to check the following settings: 
  
* Open a Microsoft Excel worksheet or a [Google sheet](https://www.google.com/docs/about/)
* Check whether the spreadsheet uses a 'decimal-comma' ($0,05$) or 'decimal-point' ($0.05$) notation. 
    + The numbers given in the assignments of this course all use a 'decimal-point' notation.
* Check if the `$` symbol fixes rows and columns when it used in a formula in your preferred spreadsheet program. 
    + This is the default setting in Microsoft Excel and Google Sheets. If you use one of those programs you are all set.
```

</br>

First study the behaviour of the **Linear map** in a spreadsheet and if you want to, try to implement it in `R`.
  
* Open the [GoogleSheet](https://docs.google.com/spreadsheets/d/1EEpUFxEmelJDPDT61jyT_xfntLo_MYBTW0MZuRSC770/edit?usp=sharing) and look at the **Linear map** tab.
* The `r` in cell `A5` is the *control parameter*, it currently has the value $1.08$ in cell`B5`.
* The cell labelled $Y_0$ in `A6` is the *initial value* at $i=0$. It has the value $0.1$ in cell `B6`.   
* The `A` column also contains $Y_i$, the output of the linear map. 
    + Click on cell `A10`, in the *formula bar* you can see that this cell simply refers to the initial value given in `B6`. So, this will be the first input value to the iterative process.
    + Click on `A11`, this corresponds to time step $i=1$. In the formula bar you can see the very simple implementation of the linear map which calculates the value at $i=1$ from the initial value at $i=0$ and the value of $r$ in `B5`. The value of cell `A11` is the product of the value in cell `B5` (parameter `r`) and the value of cell `A10` ($Y_{i=0}$).
    
</br>

```{block2, type="rmdkennen"}
A reminder of what the model represents:     
      
* We are calculating $Y_{i+1}$ (i.e. 'behaviour' in the future, given the current state)
* This value is *completely determined* by $Y_{t=0}$ (i.e., the first 'input') and the value of the control parameter `r` (the growth rate)
* There are no stochastic components involved in generating the temporal evolution of `Y`
```
  
</br>

### Change the control parameter, label the order parameter  {.tabset .tabset-fade .tabset-pills}

If you make a copy of the Google Sheet you should be able to edit the cells. If this does not work, [download the spreadsheet from GitHub](https://github.com/complexity-methods/CSA-assignments/blob/master/assignment_data/Iterating1Dmaps/Assignment%20DCS_%20Iterating%201D%20Maps.xlsx)
  
* Change the values in cells `B5` and `B6` and you will see an immediate change in the graph. To study model behaviour, try the following growth parameters:
    + $r =  1.08$
    + $r = -1.08$
    + $r =  1$
    + $r = -1$
    + $r =  0.95$
    + $r = -0.95$

#### Questions {-}
* Are there any *qualitative differences* in the dynamical behaviour of the system that can be attributed parameter values?
* Provide a description for each clearly distinct behaviour displayed by this system (only for the parameter settings you examined). 

#### Answers {-}
* Yes!
    + $r =  1.08$ - Unlimited exponential growth, $Y$ will grow infinitely large
    + $r = -1.08$ - Unstable limit cycle, exponential 2-period oscillation $Y$ will grow infinitely large
    + $r =  1$ - Point attractor, fixed point at $Y = 1$
    + $r = -1$ - Limit cycle, closed period-2 orbit at $Y = [-1,1]$
    + $r =  0.95$ - Exponential decay, $Y$ will approach $0$ in the limit
    + $r = -0.95$ - Point attractor, exponentially decaying 2-period oscillation, $Y$ will approach $0$


```{block2, type="rmdkennen"}
The labels assigned to behavioural modes that are clearly qualitatively distinct from one another, are know as the **order parameter**. The **order parameter** will change its value to indicate the system has transitioned to a new behavioural regime. Such changes often occur suddenly with gradual changes in the **control parameter**.     
For example, the (most familiar) phases that describe matter are an **order parameter** (solid, liquid, gas and plasma). We know water will transition from a liquid state to a gas state if we increase the **control parameter** beyond a certain point (e.g. temperature rises above $100^\circ\text{C}$).

Other names you might encounter that refer to the change of the label of an order parameter:
  
* Phase transition/shift
* Order transition
* Critical transition
* Change of behavioural mode/regime
* Change of attractor state/landscape
* Shift between self-organised/stable states/attractors
```

</br>

### Dependence on initial conditions? {.tabset .tabset-fade .tabset-pills}
  
* Change the initial value $Y_0$ in cell `B6` to $10$ and compare to $0.1$ (if you want to directly compare different settings, just create a new copy of the sheet)
* Subsequently give the growth parameter in cell `B5` the following values:
    + $r =  1.08$
    + $r = -1.08$
    + $r =  1$
    + $r = -1$
    + $r =  0.95$
    + $r = -0.95$ 
  
#### Questions {-}
* Are there any *qualitative differences* in the dynamical behaviour of the system for each parameter setting, that can be attributed to a difference in initial conditions?
  
#### Answers {-}
* No, there are no differences in the overall dynamics due to changes in initial conditions. Of course, the absolute values change, but the temporal pattern of change is the same. 


### **EXTRA:** Using `R` to simulate the Linear Map {.tabset .tabset-fade .tabset-pills}
  
The best (and easiest) way to simulate these simple models in `R` is to create a function, which takes as input arguments the parameters ($Y_0$, $r$) and a variable indicating the length of the time series.

#### Questions {-}

For the Linear Map you could use this template ([also see the notes in the course book](https://complexity-methods.github.io/book/implementing-iterative-functions.html)):

```{r, eval=FALSE, include=TRUE, tidy=FALSE}
# TEMPLATE
# Modify this function
linearMap <- function(Y0 = 0, r = 1, N = 100){
                    # Y0, r and N are arguments to this function, they have been given default values
                    # When you call the function you can pass different values to these arguments
    
    # Initialize Y as vector of Y0, followed by N-1 empty slots (NA).
    Y <- c(Y0, rep(NA,N-1))
    
    # Then, you need create the iteration using a for(... in ...){...} loop
    for(i in 1:(N-1)){
        
    Y[i+1] <-       # !!Implement the code for the linear or Quadratic map here!!
        
    }
    
    return(Y)
}
```

</br>

* Copy the code to `R` and implement the Linear Map by looking at the formula, or the spreadsheet.
* To test it, you need to initialize the function by selecting the function code and running it. 
    + The environment will now contain a function called `linearMap`
* Generate some data by calling the function using $Y0=0.1$, $r=1.08$ and $N=100$ and store the result in a variable. 

#### Answers {-}

```{r, eval=TRUE, include=TRUE, tidy=FALSE}
# Linear Map
linearMap <- function(Y0 = 0, r = 1, N = 100){
  
  # Initialize Y as vector of Y0, followed by N-1 empty slots (NA).
  Y <- c(Y0, rep(NA,N-1))
  
  # Then, you need create the iteration
  for(i in 1:(N-1)){
    
    Y[i+1] <- r * Y[i] # The linear difference equation
    
    }
  
  return(Y)
}
```

</br>

### Plot the time series {.tabset .tabset-fade .tabset-pills}

#### Questions {-}

Creating a time series plot is easy if the function `linearMap` returns the time series as a numeric vector. You could just use:

```{r, eval=FALSE,tidy=FALSE}
plot(linearMap(Y0=0.1,r=1.08,N=100), type = "l")
```

But perhaps better to first save the output of the function and then plot it:

```{r, eval=FALSE,tidy=FALSE}
Y <- linearMap(Y0=0.1,r=1.08,N=100)
plot(Y, type = "l")
```


```{block2, useR, type='rmdnote'}
Have a look at the examples on different ways to [plot time series in `R` in the course book](https://darwin.pwo.ru.nl/skunkworks/courseware/book/1819_CMBS/plotTS.html) 
```

#### Answers {-}

```{r, eval=TRUE,tidy=FALSE}
plot(linearMap(Y0=0.1,r=1.08,N=100), type = "l")
```

`R` has specialized objects to represent time series, and functions and packages for time series analysis. These are especially convenient for plotting time and date information on the X-axis. See the examples on plotting [time series in `R` in Chapter 3](https://complexity-methods.github.io/book/plotTS.html).
  
</br>
</br>

## **The Logistic Map** 
  
The Logistic Map takes the following functional form: 
  
\begin{equation}
Y_{i+1} = r*Y_i*(1-Y_i)
(\#eq:logmap)
\end{equation}
  
### Chaos in a spreadsheet 
  
* Open the [GoogleSheet](https://docs.google.com/spreadsheets/d/1EEpUFxEmelJDPDT61jyT_xfntLo_MYBTW0MZuRSC770/edit?usp=sharing) and look at the **Logistic map** tab.
* The set-up is the same as for the linear map, except of course the implemented change process.
* The `A` column contains $Y_i$, the output of the Logistic map. With $r=1.3$ the behaviour looks a lot like an S-curve, a logistic function.
* Check the following:
    + Click on `A10`, in the formula bar you can see this refers to the initial value `B6`.
    + Click on `A11`, in the formula bar you can see the very simple implementation of the Logistic map.
    + The value of cell `A11` ($Y_{i=1}$) is calculated by multiplying the value of cell `B5` (parameter `r`) with the value of cell `A10` (the current value $Y_i$, in this case $Y_{i=0}$). The only difference is we are also multiplying by $(1-Y_i)$.

</br>

```{block2, QLinMap, type='rmdselfThink'}
We established that `r` controls growth, what could be the function of $(1-Y_t)$?
```

</br>

### Change the control parameter, label the order parameter {.tabset .tabset-fade .tabset-pills}
  
To study the behaviour of the Logistic Map you can start changing the values in `B5`. 
  
* Try the following settings for $r$:
    + $r = 0.9$
    + $r = 1.9$
    + $r = 2.9$
    + $r = 3.3$
    + $r = 3.52$
    + $r = 3.9$

#### Questions {-}
  
* Are there any *qualitative differences* in the dynamical behaviour of the system that can be attributed parameter values?
* Provide a description for each clearly distinct behaviour (the labels of the order parameter) displayed by this system (only for the parameter settings you examined). 
  
#### Answers {-}

* Yes!
* With $Y_0 = 0.01$:
    + $r = 0.9$ - Point attractor, fixed point at $Y = 0$
    + $r = 1.9$ - Point attractor, fixed point at $Y = 0.4737$
    + $r = 2.9$ - Point attractor, fixed point at $Y = 0.6552$
    + $r = 3.3$ - Limit cycle attractor, closed period 2 orbit at $Y =  [0.8236, 0.4794]$
    + $r = 3.52$ - Limit cycle attractor, closed period 4 orbit at $Y =  [0.5121, 0.8795, 0.3731, 0.8233]$
    + $r = 3.9$ - Strange attractor, a-periodic orbit all values between $\approx0.01$ and $\approx0.99$


  
### The return plot {.tabset .tabset-fade .tabset-pills}
  
#### Questions {-}

* Set $r$ at $4.0$:
    + How would you describe the dynamics of the time series? Periodic? Something else?
    + Check the values, is there any value that is recurring, for example like when $r=3.3$? Perhaps if you increase the length of the simulation?
* Select the values in both columns under $Y(i)$ and $Y(i+1)$, (`A10` to `B309`) and create a scatter-plot (no lines, just points).
    + The points should line up to represent a familiar shape...

</br>

```{block2, QuadReturnPlot1, type='rmdkennen'}
The plot you just produced is a so called **return plot**, in which you have plotted $Y_{i+1}$ against $Y_i$. We shifted the time series by 1 time step, which is why this is also called a **lag 1 plot**.    

Can you explain the pattern you see (a 'parabola') by looking closely at the functional form of the Logistic Map? (hint: another name for this equation is the **Quadratic Map**).

If you change the control parameter to a lower value the return plot will seem to display a shape that is no longer a parabola, but this is not the case, all values produced by by the Logistic Map **must** be a point on a parabola. Some parameter settings just do not produce enought different points for the parabola to show itself in the return plot.
```

</br>

* What do you expect the return plot of the **Linear Map** will look like? Go ahead and try it!

</br>

```{block2, QuadReturnPlot2, type='rmdselfThink'}
The return plot is an important tool for getting a 'first impression' of the nature of the dynamic process that generated the observed values:
  
* Try to imagine what the lag 1 plot of a cubic difference equation would look like
* What about the return plot for a timeseries of independent random numbers (white noise)?     
* We are of course not limited to a lag of 1, what happens at lag 2, or lag 10 in the return plots of the Linear and Quadratic Map?
```

#### Answers {-}

* Set $r$ at $4.0$:
    + The dynamics are called a-periodic, or quasi-periodic, or, chaotic.
    + None of the values will exactly recur!!
* Select the values in both columns under $Y(i)$ and $Y(i+1)$, (`A10` to `B309`) and create a scatter-plot (no lines, just points).
    + The points form a parabola
    + The return plot for both maps can be seen [here](https://docs.google.com/spreadsheets/d/17PBTvGa1Hu9spqEQGuhpwWXGeY7Nujkm09pxcppKPn4/edit?usp=sharing) 
    
* Why a parabola? Think about what the iterative process represents: It is the rule that changes the value of Y at one point in time, into a value at the next point in time. The only relationship that can exist between two points is the difference equation... work out the brackets, what kind of equation is this?

* For the linear map, the same thing holds, what kind of equation is the linear map? Now you can probably also deduce what the return maps of a cubic and quartic equation look like... try it!


</br>

### Sensitive dependence on initial conditions?  {.tabset .tabset-fade .tabset-pills}
  
Go to the following [GoogleSheet](https://docs.google.com/spreadsheets/d/1xaZJLYfZzkxg_PXCAa7QT7p4R_JTnUIxzs_Qs1-RAOQ/edit?usp=sharing) and download or copy it.
  
#### Questions {-}
  
* Imagine these are observed time series from two subjects in a response time experiment. These subjects are perfect 'twins':
    + The formula describing their behaviour is exactly the same (it's the Quadratic Map, check it!)
    + The control parameter, controlling the behavioural mode is exactly the same ($r=4$).
    + The only difference is that they have a tiny discrepancy in initial conditions in cell `D6` of $0.01$.
* Not tiny enough? Change it to become even smaller:
    + $0.001$
    + $0.0001$
    + $0.00001$
    + etc.
* What happens as you make the discrepancy smaller and smaller?

</br>


#### Answers {-}

By making the discrepancy smaller the two time series will become more similar.

```{block2, type="rmdkennen"}
This sheet displays the extreme sensitivity to changes in initial conditions displayed by the Logistic Map for specific parameter settings. This specific phenomenon is more commonly referred to as **The Butterfly Effect**. It is a characteristic of a very interesting and rather mysterious behaviour displayed by deterministic dynamical equations known as **deterministic chaos**.

However, perhaps even more important to notice here, is the fact that a wide range of qualitatively different behaviourial modes can be understood as originating from one and the same, rather simple, dynamical system. The different behavioural modes were observed for different values of a control parameter, which, in real-world models, often represents an observable (physical) quantity (e.g. temperature, available resources, chemical gradient, etc.) 

The Logistic Map is the simplest nontrivial model that can display deterministic chaos.
```

</br>

### Use `R` to simulate the Logistic Map {.tabset .tabset-fade .tabset-pills}

The best (and easiest) way to simulate the discrete time models is to create a function which takes as input the parameters ($Y_0$, $r$) and a variable indicating the length of the time series.

#### Questions {-}

To model the Logistic Map, use this template:

```{r, eval=FALSE, include=TRUE, tidy=FALSE}
# TEMPLATE
# Modify this function
logisticMap <- function(Y0 = 0.01, r = 1, N = 100){
    
    # Initialize Y as vector of Y0, followed by N-1 empty slots (NA).
    Y <- c(Y0, rep(NA,N-1))
    
    # Then, you need create the iteratation
    for(i in 1:(N-1)){
        
    Y[i+1] <-      # !!Implement the linear or logistic map here!!
        
    }
    
    return(Y)
}
```

</br>

* Copy the code above and implement the Logistic Map.
* When you are done, you need to initialize the function, select the code and run it. 
    + The environment will now contain a function called `logisticMap`
    + Generate some data by calling the function using $Y0=0.1$, $r=4$ and $N=100$ (or any other values you want to explore) and store the result in a variable. 
    

#### Answers {-}

```{r, include=TRUE, tidy=FALSE}
# Logistic Map
logisticMap <- function(Y0 = 0.01, r = 1, N = 100){
    
    # Initialize Y as vector of Y0, followed by N-1 empty slots (NA).
    Y <- c(Y0, rep(NA,N-1))
    
    # Then, you need create the iteratation
    for(i in 1:(N-1)){
        
    Y[i+1] <- r * Y[i] * (1 - Y[i]) # The quadratic difference equation
       
    }
    
    return(Y)
}
```


### Plot the time series {.tabset .tabset-fade .tabset-pills}

#### Questions {-}

Creating a time series plot is easy if the function `linearMap` returns the time series as a numeric vector. You could just use:

```{r, eval=FALSE,tidy=FALSE}
plot(logisticMap(Y0=0.01,r=1.9,N=100), type = "l")
```

* Also try to create a graph that demonstrates the sensitive dependence on initial conditions
* Create the lag 1 return plot.
      + Also try to create a lag 2, lag 3 and lag 4 return plot.
      + Can you explain the patterns?


#### Answers {-}

```{r, tidy=FALSE}
library(plyr)
rs <- c(0.9,1.9,2.9,3.3,3.52,3.9)
op<-par(mfrow=c(2,3))
l_ply(rs,function(r) plot(logisticMap(r=r),ylab = paste("r =",r) ,type = "l"))
par(op)
```

* Also try to create a graph that demonstrates the sensitive dependence on initial conditions
* Create the lag 1 return plot.
      + Also try to create a lag 2, lag 3 and lag 4 return plot (increase N to 500 or 1000).
      + Can you explain the patterns?

```{r, tidy=FALSE}
lags <- c(1,2,3,4)
N <- 1000
op <- par(mfrow = c(2,2), pty = "s")
l_ply(lags, function(l) {plot(dplyr::lag(logisticMap(r=4,N=N),l), logisticMap(r=4,N=N), pch = ".", xlim = c(0,1), ylim = c(0,1), xlab = "Y(t)", ylab = "Y(t+1)",  main = paste("lag =",l))})
par(op)
```


### Using casnet {-}

`casnet` has some built in functions called 'toy models' you can use to generate the models in these assignments:
* [`growth_ac()`](https://fredhasselman.github.io/casnet/reference/growth_ac.html)
* [`growth_ac_cond()`](https://fredhasselman.github.io/casnet/reference/growth_ac_cond.html)

</br>


# **EXTRA**: van Geert's growth model

In this assignment you'll build a more sophisticated growth model and look at its properties. The model will be the growth model by Van Geert (1991). You can try to code the models following the hints in section \@ref(moc1R).


## The growth model by Van Geert (1991) {-}

The growth model by Van Geert has the following form: 

\begin{equation}
L_{t+1} = L_t * (1 + r - r * \frac{L_t}{K})
(\#eq:vanG)
\end{equation}

Note the similarities to the logistic map.

### The growth model in a spreadsheet {.tabset .tabset-fade .tabset-pills}

* Go to the following [GoogleSheet](https://docs.google.com/spreadsheets/d/1DAg0u-zMFOIvRSDOZDxqnzyS0HQJg4FIXzJIMvEMwiI/edit?usp=sharing) and download or copy it to a new browser tab.


#### Questions {-}

* The constants are defined as: 
    + $r$ in `A5`
    + $L_0$ in `A6`
    + $K$ in cell `A7`. This is the *carrying capacity*. It is set to the value $1$ in cell `B7`.
* Start with the following values:
    + $r = 1.2$
    + $L_0 = 0.01$
* You can start playing with the values for the parameters and the initial values in cells `B5`, `B6` and `B7`. To study its behavior, be sure to try the following growth parameters:
    + $r = 1.2$
    + $r = 2.2$
    + $r = 2.5$
    + $r = 2.7$
    + $r = 2.9$
* For the carrying capacity $K$ (cell `B7`) you can try the following values:
    + $K = 1.5$
    + $K = 0.5$. (Leave $r = 2.9$. Look at the value on the vertical axis!)
* Changes in the values of $K$ have an effect on the height of the graph. The pattern itself also changes a bit. 
    + Can you explain why this is so?

#### Answers {-}

* Changes in the values of $K$ have an effect on the height of the graph. The pattern itself also changes a bit. 
    + Can you explain why this is so?

Larger values can enter the iterative process if $K$ increased.


## Conditional growth: Jumps and Stages 

### Auto-conditional jumps {-}

Suppose we want to model that the growth rate $r$ increases after a certain amount has been learned. In general, this is a very common phenomenon, for instance: when becoming proficient at a skill, growth (in proficiency) is at first slow, but then all of a sudden there can be a jump to the appropriate (and sustained) level of proficiency.

The second tab of the google sheet contains an example.

#### Questions {-}

*  Suppose we want the parameter to change when a growth level of $0.2$ is reached. This is modelled using an `IF` statement which looks something like this: `IF` $L > 0.2$ then use the parameter value in `C5`, otherwise use the parameter value in `B5`. 
    + Excel has a built in `IF` function (may be `ALS` in Dutch). Check the cells to figure out how the level change is impemented. 
    + Try changing the value of $r$ in `C5` into: $1, 2, 2.8, 2.9, 3$. 

### Auto-conditional stages {-}

Another conditional change we might want to explore is that when a certain growth level is reached the carrying capacity $K$ increases, reflecting that new resources have become available to support further growth. This is modelled on the third and fourth tab of spreadsheet [van Geert (2 stages); van Geert (3 stages)]

#### Questions {-}

* Now we want $K$ to change, so type $1$ in `B7`, $2$ in `C7`.
* When $L > 0.99$, $K$ changes to the value in `C7`. Keep $r = 0.2$ (`B5`).
* Change $K$ in `C7` to other values.
* The growth rate `r` changes after reaching $L > 0.99$. Start with a value of $0.3$ in `C5` and set $K$ in `C7` to $2$ again. 
    + Also try setting `r` in `C5` to $1, 2.1, 2.5, 2.6, 3$.
* Using this method you can of course add stages, see the tab `van Geert (3 stages)`


#### Demonstrations of dynamic modeling using spreadsheets

See the website by [Paul Van Geert](http://www.paulvangeert.nl/research.htm), scroll down to see models of:

* Learning and Teaching
* Behaviour Modification
* Connected Growers
* Interaction during Play


<!--chapter:end:ASSIGNMENTS_D1_2_uvSystems.Rmd-->

---
title: "Math of Change Assignments"
author: "Fred Hasselman"
date: "1/14/2019"
output: 
  bookdown::html_document2: 
    variant: markdown+hard_line_breaks
    fig_caption: yes
    highlight: pygments
    keep_md: yes
    number_sections: yes
    self_contained: yes
    theme: spacelab
    toc: yes
    toc_float: true
    collapsed: false
    smooth_scroll: true
    code_folding: show
    bibliography: [refs.bib, packages.bib]
    biblio-style: apalike
    csl: apa.csl
    includes:
        before_body: assignmentstyle.html
    pandoc_args: ["--number-offset","0"]
editor_options: 
  chunk_output_type: console
---

```{r, include=FALSE}
knitr::opts_chunk$set(include=TRUE, tidy = FALSE)
```


# **Modelling (nonlinear) growth and Deterministic Chaos**

In this assignment you will build two (relatively) simple one-dimensional maps based on the models implemented in a spreadsheet. Just copy the linked Google Sheet to a new on-line Google Sheet, or, save it to your computer and use your favourite spreadsheet software (e.g., Excel, Numbers) to explore the behaviour of the model.    
    
We will start by modelling the *Linear Map* and then proceed to the slightly more complicated *Logistic Map* (aka *Quadratic map*). You can use `R`, `Matlab`, or `Python` to model the dynamical processes, but for now we will assume you use `R`.

</br>

```{block2, useR, type='rmdnote'}
Check the solutions of the assignments **after** you tried to do them on your own!    
  
Also, have a look at the examples on [plotting time series in `R` in the course book](https://darwin.pwo.ru.nl/skunkworks/courseware/1819_DCS/plotTS.html) on different ways to visualize time series in `R`.
```

</br>

## **The Linear Map**

</br>

Equation \@ref(eq:linmap) is the ordinary difference equation (ODE) discussed in [the course book](https://darwin.pwo.ru.nl/skunkworks/courseware/1819_DCS/introduction-to-the-mathematics-of-change.html) and is called the *Linear Map*:
  
\begin{equation}
Y_{i+1} = Y_{i=0} + r*Y_i
(\#eq:linmap)
\end{equation}
  
In these exercises you will *simulate* time series produced by the change process in equation \@ref(eq:linmap) for different parameter settings of the growth-rate parameter $r$ (the *control parameter*) and the initial conditions $Y_0$. *Simulation* is obviously different from a statistical analysis in which parameters are estimated from a data set. The goal of these assignments is to get a feeling for what a dynamical model is, and in what way it is different from a linear statistical regression model like GLM.

</br>

```{block2, type = "rmdnote"}
It is common practice to use $Y_{i}$ for the current state and $Y_{i+1}$ for the next state of a discrete time model (a map, a difference equation). For a continuous time model (a flow, a differential equation), sometimes $Y_{t}$ and $\hat{Y_t}$ are used. More common is the $\delta$, or *'change in'* notation: $\frac{dY}{dt} = Y_0 + r * Y(t)$, which you can pronounce as *'a change in Y over a change in time is a function of ...'*. However, different authors have different preferences and may use other notations.
```
  
</br>

### Growth in a Spreadsheet
  
</br>

```{block2, spreadset1, type='rmdimportant'}
If you want to modify the spreadsheets and use them as a template for other assignments, be sure to check the following settings: 
  
* Open a Microsoft Excel worksheet or a [Google sheet](https://www.google.com/docs/about/)
* Check whether the spreadsheet uses a 'decimal-comma' ($0,05$) or 'decimal-point' ($0.05$) notation. 
    + The numbers given in the assignments of this course all use a 'decimal-point' notation.
* Check if the `$` symbol fixes rows and columns when it used in a formula in your preferred spreadsheet program. 
    + This is the default setting in Microsoft Excel and Google Sheets. If you use one of those programs you are all set.
```

</br>

First study the behaviour of the **Linear map** in a spreadsheet and then try to implement it in `R`.
  
* Open the [GoogleSheet](https://docs.google.com/spreadsheets/d/1EEpUFxEmelJDPDT61jyT_xfntLo_MYBTW0MZuRSC770/edit?usp=sharing) and look at the **Linear map** tab.
* The `r` in cell `A5` is the *control parameter*. It currently has the value $1.08$ in cell`B5`.
* The cell labelled $Y_0$ in `A6` is the *initial value* at $i=0$. It has the value $0.1$ in cell `B6`.   
* The `A` column also contains $Y_i$, the output of the linear map. 
    + Click on `A10`, in the formula bar you can see this refers to the initial value in `B6`. This will be the first input to the iterative process.
    + Click on `A11`, in the formula bar you can see the very simple implementation of the linear map.
    + The value of cell `A11` (i.e. $Y_{i=1}$) will be calculated by multiplying the value of cell `B5` (parameter `r`) with the value of cell `A10` (the previous value, which happens to be: $Y_{i=0}$).
    
</br>

```{block2, type="rmdkennen"}
A reminder of what the model represents:     
      
* We are calculating $Y_{i+1}$ (i.e. 'behaviour' in the future, given the current state)
* This value is *completely determined* by $Y_{t=0}$ (i.e., the first 'input') and the value of the control parameter `r` (the growth rate)
* There are no stochastic components involved in generating the temporal evolution of `Y`
```
  
</br>

### Change the control parameter, label the order parameter  {.tabset .tabset-fade .tabset-pills}

If you make a copy of the Google Sheet you should be able to edit the cells. If this does not work, [download the spreadsheet from GitHub](https://github.com/FredHasselman/The-Complex-Systems-Approach-Book/blob/master/assignments/assignment_data/Iterating1Dmaps/Assignment%20DCS_%20Iterating%201D%20Maps.xlsx)
  
* Change the values in cells `B5` and `B6` and you will see an immediate change in the graph. To study model behaviour, try the following growth parameters:
    + $r =  1.08$
    + $r = -1.08$
    + $r =  1$
    + $r = -1$
    + $r =  0.95$
    + $r = -0.95$

#### Questions {-}
* Are there any *qualitative differences* in the dynamical behaviour of the system that can be attributed parameter values?
* Provide a description for each clearly distinct behaviour displayed by this system (only for the parameter settings you examined). 

#### Answers {-}
* Yes!
    + $r =  1.08$ - Unlimited exponential growth, $Y$ will grow infinitely large
    + $r = -1.08$ - Unstable limit cycle, exponential 2-period oscillation $Y$ will grow infinitely large
    + $r =  1$ - Point attractor, fixed point at $Y = 1$
    + $r = -1$ - Limit cycle, closed period-2 orbit at $Y = [-1,1]$
    + $r =  0.95$ - Exponential decay, $Y$ will approach $0$ in the limit
    + $r = -0.95$ - Point attractor, exponentially decaying 2-period oscillation, $Y$ will approach $0$


```{block2, type="rmdkennen"}
The labels assigned to behavioural modes that are clearly qualitatively distinct from one another, are know as the **order parameter**. The **order parameter** will change its value to indicate the system has transitioned to a new behavioural regime. Such changes often occur suddenly with gradual changes in the **control parameter**.     
For example, the four phases that describe matter are an **order parameter** (solid, liquid, gas and plasma). We know water will transition from a liquid state to a gas state if we increase the **control parameter** beyond a certain point (e.g. temperature rises above $100^\circ\text{C}$).

Other names you might encounter that refer to the change of the label of an order parameter:
  
* Phase transition/shift
* Order transition
* Critical transition
* Change of behavioural mode/regime
* Change of attractor state/landscape
* Shift between self-organised/stable states/attractors
```

</br>

### Dependence on initial conditions? {.tabset .tabset-fade .tabset-pills}
  
* Change the initial value $Y_0$ in cell `B6` to $10$ and compare to $0.1$ (if you want to directly compare different settings, just create a new copy of the sheet)
* Subsequently give the growth parameter in cell `B5` the following values:
    + $r =  1.08$
    + $r = -1.08$
    + $r =  1$
    + $r = -1$
    + $r =  0.95$
    + $r = -0.95$ 
  
#### Questions {-}
* Are there any *qualitative differences* in the dynamical behaviour of the system for each parameter setting, that can be attributed to a difference in initial conditions?
  
#### Answers {-}
* No, there are no differences in the overall dynamics due to changes in initial conditions. Of course, the absolute values change, but the temporal pattern of change is the same.

  
### Using `R` to simulate the Linear Map {.tabset .tabset-fade .tabset-pills}
  
The best (and easiest) way to simulate these simple models in `R` is to create a function, which takes as input arguments the parameters ($Y_0$, $r$) and a variable indicating the length of the time series.

#### Questions {-}

For the Linear Map you could use this template ([also see the notes in the course book](https://darwin.pwo.ru.nl/skunkworks/courseware/1819_DCS/implementing-iterative-functions.html)):

```{r, eval=FALSE, include=TRUE, tidy=FALSE}
# TEMPLATE
# Modify this function
linearMap <- function(Y0 = 0, r = 1, N = 100){
    
    # Initialize Y as vector of Y0, followed by N-1 empty slots (NA).
    Y <- c(Y0, rep(NA,N-1))
    
    # Then, you need create the iteration
    for(i in 1:(N-1)){
        
    Y[i+1] <- # !!Implement the linear or Quadratic map here!!
        
    }
    
    return(Y)
}
```

</br>

* Copy the code to `R` and implement the Linear Map by looking at the formula, or the spreadsheet.
* To test it, you need to initialize the function by selecting the code and running it. 
    + The environment will now contain a function called `linearMap`
* Generate some data by calling the function using $Y0=0.1$, $r=1.08$ and $N=100$ and store the result in a variable. 

#### Answers {-}

```{r, eval=TRUE, include=TRUE, tidy=FALSE}
# Linear Map
linearMap <- function(Y0 = 0, r = 1, N = 100){
  
  # Initialize Y as vector of Y0, followed by N-1 empty slots (NA).
  Y <- c(Y0, rep(NA,N-1))
  
  # Then, you need create the iteration
  for(i in 1:(N-1)){
    
    Y[i+1] <- r * Y[i] # The linear difference equation
    
    }
  
  return(Y)
}
```

</br>

### Plot the timeseries {.tabset .tabset-fade .tabset-pills}

#### Questions {-}

Creating a time series plot is easy if the function `linearMap` returns the time series as a numeric vector. You could just use:

```{r, eval=FALSE,tidy=FALSE}
plot(linearMap(Y0=0.1,r=1.08,N=100), type = "l")
```

#### Answers {-}

```{r, eval=TRUE,tidy=FALSE}
plot(linearMap(Y0=0.1,r=1.08,N=100), type = "l")
```

`R` and `Matlab` have specialized objects to represent time series, and functions and packages for time series analysis. These are especially convenient for plotting time and date information on the X-axis. See the examples on plotting [time series in `R` in Chapter 2](https://darwin.pwo.ru.nl/skunkworks/courseware/1819_DCS/02-MathematicsofChange.html#plotTS).
  
</br>
</br>

## **The Logistic Map** 
  
The Logistic Map takes the following functional form: 
  
\begin{equation}
Y_{i+1} = r*Y_i*(1-Y_i)
(\#eq:logmap)
\end{equation}
  
### Chaos in a spreadsheet 
  
* Open the [GoogleSheet](https://docs.google.com/spreadsheets/d/1EEpUFxEmelJDPDT61jyT_xfntLo_MYBTW0MZuRSC770/edit?usp=sharing) and look at the **Logistic map** tab.
* The set-up is the same as for the linear map, except of course the implemented change process.
* The `A` column contains $Y_i$, the output of the Logistic map. With $r=1.3$ the behaviour looks a lot like an S-curve, a logistic function.
* Check the following:
    + Click on `A10`, in the formula bar you can see this refers to the initial value `B6`.
    + Click on `A11`, in the formula bar you can see the very simple implementation of the Logistic map.
    + The value of cell `A11` ($Y_{i=1}$) is calculated by multiplying the value of cell `B5` (parameter `r`) with the value of cell `A10` (the current value $Y_i$, in this case $Y_{i=0}$). The only difference is we are also multiplying by $(1-Y_i)$.

</br>

```{block2, QLinMap, type='rmdselfThink'}
We established that `r` controls growth, what could be the function of $(1-Y_t)$?
```

</br>

### Change the control parameter, label the order parameter {.tabset .tabset-fade .tabset-pills}
  
To study the behaviour of the Logistic Map you can start changing the values in `B5`. 
  
* Try the following settings for $r$:
    + $r = 0.9$
    + $r = 1.9$
    + $r = 2.9$
    + $r = 3.3$
    + $r = 3.52$
    + $r = 3.9$

#### Questions {-}
  
* Are there any *qualitative differences* in the dynamical behaviour of the system that can be attributed parameter values?
* Provide a description for each clearly distinct behaviour (the labels of the order parameter) displayed by this system (only for the parameter settings you examined). 
  
#### Answers {-}

* Yes!
* With $Y_0 = 0.01$:
    + $r = 0.9$ - Point attractor, fixed point at $Y = 0$
    + $r = 1.9$ - Point attractor, fixed point at $Y = 0.4737$
    + $r = 2.9$ - Point attractor, fixed point at $Y = 0.6552$
    + $r = 3.3$ - Limit cycle attractor, closed period 2 orbit at $Y =  [0.8236, 0.4794]$
    + $r = 3.52$ - Limit cycle attractor, closed period 4 orbit at $Y =  [0.5121, 0.8795, 0.3731, 0.8233]$
    + $r = 3.9$ - Strange attractor, a-periodic orbit all values between $\approx0.01$ and $\approx0.99$


  
### The return plot {.tabset .tabset-fade .tabset-pills}
  
#### Questions {-}

* Set $r$ at $4.0$:
    + How would you describe the dynamics of the time series? Periodic? Something else?
    + Check the values, is there any value that is recurring, for example like when $r=3.3$? Perhaps if you increase the length of the simulation?
* Select the values in both columns under $Y(i)$ and $Y(i+1)$, (`A10` to `B309`) and create a scatter-plot (no lines, just points).
    + The points should line up to represent a familiar shape...

</br>

```{block2, QuadReturnPlot1, type='rmdkennen'}
The plot you just produced is a so called **return plot**, in which you have plotted $Y_{i+1}$ against $Y_i$. We shifted the time series by 1 time step, which is why this is also called a **lag 1 plot**.    

Can you explain the pattern you see (a 'parabola') by looking closely at the functional form of the Logistic Map? (hint: another name for this equation is the **Quadratic Map**).

If you change the control parameter to a lower value the return plot will seem to display a shape that is no longer a parabola, but this is not the case, all values produced by by the Logistic Map **must** be a point on a parabola. Some parameter settings just do not produce enought different points for the parabola to show itself in the return plot.
```

</br>

* What do you expect the return plot of the **Linear Map** will look like? Go ahead and try it!

</br>

```{block2, QuadReturnPlot2, type='rmdselfThink'}
The return plot is an important tool for getting a 'first impression' of the nature of the dynamic process that generated the observed values:
  
* Try to imagine what the lag 1 plot of a cubic difference equation would look like
* What about the return plot for a timeseries of independent random numbers (white noise)?     
* We are of course not limited to a lag of 1, what happens at lag 2, or lag 10 in the return plots of the Linear and Quadratic Map?
```

#### Answers {-}

* Set $r$ at $4.0$:
    + The dynamics are called a-periodic, or quasi-periodic, or, chaotic.
    + None of the values will exactly recur!!
* Select the values in both columns under $Y(i)$ and $Y(i+1)$, (`A10` to `B309`) and create a scatter-plot (no lines, just points).
    + The points form a parabola
    + The return plot for both maps can be seen [here](https://docs.google.com/spreadsheets/d/17PBTvGa1Hu9spqEQGuhpwWXGeY7Nujkm09pxcppKPn4/edit?usp=sharing) 


</br>

### Sensitive dependence on initial conditions?  {.tabset .tabset-fade .tabset-pills}
  
Go to the following [GoogleSheet](https://docs.google.com/spreadsheets/d/1xaZJLYfZzkxg_PXCAa7QT7p4R_JTnUIxzs_Qs1-RAOQ/edit?usp=sharing) and download or copy it.
  
#### Questions {-}
  
* Imagine these are observed time series from two subjects in a response time experiment. These subjects are perfect 'twins':
    + The formula describing their behaviour is exactly the same (it's the Quadratic Map, check it!)
    + The control parameter, controlling the behavioural mode is exactly the same ($r=4$).
    + The only difference is that they have a tiny discrepancy in initial conditions in cell `D6` of $0.01$.
* Not tiny enough? Change it to become even smaller:
    + $0.001$
    + $0.0001$
    + $0.00001$
    + etc.
* What happens as you make the discrepancy smaller and smaller?

</br>


#### Answers {-}

By making the discrepancy smaller the two time series will become more similar.

```{block2, type="rmdkennen"}
This sheet displays the extreme sensitivity to changes in initial conditions displayed by the Logistic Map for specific parameter settings. This specific phenomenon is more commonly referred to as **The Butterfly Effect**. It is a characteristic of a very interesting and rather mysterious behaviour displayed by deterministic dynamical equations known as **deterministic chaos**.

However, perhaps even more important to notice here, is the fact that a wide range of qualitatively different behaviourial modes can be understood as originating from one and the same, rather simple, dynamical system. The different behavioural modes were observed for different values of a control parameter, which, in real-world models, often represents an observable (physical) quantity (e.g. temperature, available resources, chemical gradient, etc.) 

The Logistic Map is the simplest nontrivial model that can display deterministic chaos.
```

</br>

### Use `R` to simulate the Logistic Map {.tabset .tabset-fade .tabset-pills}

The best (and easiest) way to simulate the discrete time models is to create a function which takes as input the parameters ($Y_0$, $r$) and a variable indicating the length of the time series.

#### Questions {-}

To model the Logistic Map, use this template:

```{r, eval=FALSE, include=TRUE, tidy=FALSE}
# TEMPLATE
# Modify this function
logisticMap <- function(Y0 = 0.01, r = 1, N = 100){
    
    # Initialize Y as vector of Y0, followed by N-1 empty slots (NA).
    Y <- c(Y0, rep(NA,N-1))
    
    # Then, you need create the iteratation
    for(i in 1:(N-1)){
        
    Y[i+1] <- # !!Implement the linear or logistic map here!!
        
    }
    
    return(Y)
}
```

</br>

* Copy the code above and implement the Logistic Map.
* When you are done, you need to initialize the function, select the code and run it. 
    + The environment will now contain a function called `logisticMap`
    + Generate some data by calling the function using $Y0=0.1$, $r=4$ and $N=100$ (or any other values you want to explore) and store the result in a variable. 
    

#### Answers {-}

```{r, include=TRUE, tidy=FALSE}
# Logistic Map
logisticMap <- function(Y0 = 0.01, r = 1, N = 100){
    
    # Initialize Y as vector of Y0, followed by N-1 empty slots (NA).
    Y <- c(Y0, rep(NA,N-1))
    
    # Then, you need create the iteratation
    for(i in 1:(N-1)){
        
    Y[i+1] <- r * Y[i] * (1 - Y[i]) # The quadratic difference equation
       
    }
    
    return(Y)
}
```


### Plot the timeseries {.tabset .tabset-fade .tabset-pills}

#### Questions {-}

Creating a time series plot is easy if the function `linearMap` returns the time series as a numeric vector. You could just use:

```{r, eval=FALSE,tidy=FALSE}
plot(logisticMap(Y0=0.01,r=1.9,N=100), type = "l")
```

* Also try to create a graph that demonstrates the sensitive dependence on initial conditions
* Create the lag 1 return plot.
      + Also try to create a lag 2, lag 3 and lag 4 return plot.
      + Can you explain the patterns?


#### Answers {-}

```{r, tidy=FALSE}
library(plyr)
rs <- c(0.9,1.9,2.9,3.3,3.52,3.9)
op<-par(mfrow=c(2,3))
l_ply(rs,function(r) plot(logisticMap(r=r),ylab = paste("r =",r) ,type = "l"))
par(op)
```

* Also try to create a graph that demonstrates the sensitive dependence on initial conditions
* Create the lag 1 return plot.
      + Also try to create a lag 2, lag 3 and lag 4 return plot (increase N to 500 or 1000).
      + Can you explain the patterns?

```{r, tidy=FALSE}
lags <- c(1,2,3,4)
N <- 1000
op <- par(mfrow = c(2,2), pty = "s")
l_ply(lags, function(l) {plot(dplyr::lag(logisticMap(r=4,N=N),l), logisticMap(r=4,N=N), pch = ".", xlim = c(0,1), ylim = c(0,1), xlab = "Y(t)", ylab = "Y(t+1)",  main = paste("lag =",l))})
par(op)
```

</br>

----

</br>
</br>

## **A model of cogntitive growth** 

In this assignment you'll build a slightly more sophisticated growth model and study at its properties. The model will be the growth model introduced by [Paul van Geert (1991)](https://eric.ed.gov/?id=EJ494106).

This growth model was used to study growth of language skills in young children, such as vocabulary, and was intended to be to represent 'an ecology of growers' referring to the many motor, perceptual and cognitive skills a child needs to develop in order to become proficient at using a language. It has the following form: 

\begin{equation}
L_{t+1} = L_t * (1 + r - r * \frac{L_t}{K})
(\#eq:vanG)
\end{equation}

</br>

```{block2, type="rmdkennen"}
Note the similarities to Equation \@ref(eq:logmap), the (stylized) logistic map. The main difference is that the number $1$ in $(1-Y_i)$ has become a parameter $K$, or, the *carrying capacity* whch indicates how far the function can grow. A more general term used for models of this kind is: *restricted growth model*. 
```

</br>

### The growth model in a spreadsheet {.tabset .tabset-fade .tabset-pills}

The model is [implemented in a spreadsheet](https://docs.google.com/spreadsheets/d/1DAg0u-zMFOIvRSDOZDxqnzyS0HQJg4FIXzJIMvEMwiI/edit?usp=sharing), select the first tab sheet.

* You can start by changing the values for the parameters and the initial values in cells `B5`, `B6` and `B7`. To study its behaviour, be sure to try the following growth parameters:
    + $r = 1.2$
    + $r = 2.2$
    + $r = 2.5$
    + $r = 2.7$
    + $r = 2.9$
    
* For the carrying capacity $K$ (cell `B7`) you can try the following values:
    + $K = 1.5$
    + $K = 0.5$. (Leave $r = 2.9$. Mind the value on the vertical axis!)
    
    
#### Questions {-}

* Changes in the values of $K$ have an effect on the height of the graph. The pattern itself also changes a bit.
    + Can you explain why this is so?
    
* Implement the model in `R`. 
    + You should be able to do this by slightly modifying you model of the Logistic Map, but you need to add a parameter, $K$.
    + Make a lag 1 return plot.
    
#### Answers {-}

* Changes in the values of $K$ have an effect on the height of the graph. The pattern itself also changes a bit.
    + If $K$ is changed, the range of values that can go into the iterative process changes, so the state evolution will also change.
    
* Implement the model in `R`. 
    + You should be able to do this by slightly modifying you model of the Logistic Map.

```{r, include=TRUE, tidy=FALSE}
# Van Geert's cognitive growth model
vanGeert <- function(L0 = 0.01, r = 1.2, K = 2, N = 100){
  # Initialize L as vector of L0, followed by N-1 empty slots (NA).
  L <- c(L0, rep(NA, N-1))
  # Then, you need create the iteratation
  for(i in 1:(N-1)){
    L[i+1] <- L[i] * (1 + r - r * L[i] / K)
  }
  return(L)
}


# Make the plots ----
library(plyr)

op<-par(mfrow=c(2,4))
# Different r
rs <- c(1.2, 2.2, 2.5, 2.7, 2.9)
l_ply(rs, function(r) plot(vanGeert(r=r),ylab = paste("r =",r," | K = 2") ,type = "l"))

# Different Carrying Capacity K
Ks <- c(1.5, 0.5)
l_ply(Ks, function(k) plot(vanGeert(r=2.9,K=k),ylab = paste("r = 2.9 | K =",k) ,type = "l"))

par(op)
```

    + Make a lag 1 return plot.

```{r, tidy=FALSE}
lags <- c(1,2,3,4)
N <- 1000
op <- par(mfrow = c(2,2), pty = "s")
l_ply(lags, function(l) {plot(dplyr::lag(vanGeert(r=2.9,N=N),l), vanGeert(r=2.9,N=N), pch = ".", xlim = c(0,3), ylim = c(0,3), xlab = "Y(t)", ylab = "Y(t+1)",  main = paste("lag =",l))})
par(op)
```

</br>
</br>


## **Conditional growth: Jumps and Stages**


### Auto-conditional jumps {.tabset .tabset-fade .tabset-pills}

Suppose we want to model that the growth rate $r$ increases after a certain amount has been learned. In general, this is a very common phenomenon, for instance: When becoming proficient at a skill, growth (in proficiency) is at first slow, but then all of a sudden there can be a jump to the appropriate (and sustained) level of proficiency.

#### Questions {-}

* Take the model of the Logistic Map as a starting point.
* Look at the spreadsheet `Van Geert (Jumps)` 
* Suppose we want our parameter to change when a growth level of $0.2$ is reached. We'll need an `IF` statement which looks something like this: `IF` $L > 0.2$ then use the new parameter value, otherwise use the parameter value we started with. 
    + in Excel this becomes quite complicated using the `IF` function (`ALS` in Dutch). 
    + Implementing the conditional jumps `R` will take on a different (much easier) form.

```{r, eval=FALSE, include=TRUE, tidy=FALSE}
# TEMPLATE
# Modify this function
vanGeert <- function(L0 = 0, r = 1, K = 2, N = 100){
    
    # Initialize L as vector of Y0, followed by N-1 empty slots (NA).
    L <- c(L0, rep(NA,N-1))
    
    # Then, you need create the iteration
    for(i in 1:(N-1)){
        
    # You need to monitor the value of L and change the growth rate according to a rule
    if(L[i]>=ThresholdValue){
      r <- 
    } 

    L[i+1] <- # Implement the model here!!
        
    }
    
    return(L)
}
```


* Copy the code above and implement the Van Geert's model.
* When you are done, you need to initialize the function, select the code and run it. 
    + The environment will now contain a function called `vanGeert`
    + Generate some data, e.g. with $r$ set to $1, 2, 2.8, 2.9, 3$. 
    + *Extra:* Consider making `ThresholdValue` an argument of the function so you can play around with the values.

#### Answers {-}

Using the template:

```{r}
vanGeert.jump <- function(L0 = 0.01, r = 0.1, K = 2, ThresholdValue = 0.2, N = 100){
  
  # Create a vector L of length N, which has value L0 at L[1]
  L <- c(L0, rep(NA, N-1))
  
    # Then, you need create the iteratation
    for(i in 1:(N-1)){
      
      # You need to monitor the value of L and change the growth rate according to a rule
      if(L[i]>=ThresholdValue){
        r <- 2
      }
        
    # Van Geert growth model
    L[i+1] <- L[i] * (1 + r - r * L[i] / K)
  }
  return(ts(L))
}

# Plot with the default settings, this time let's use the lattice package
library(lattice)
xyplot(vanGeert.jump(),ylab = "Change r to 2 if L >= 0.2")
```


The code below defines the function in such a way that we can add more than one conditional statement.

```{r}
vanGeert.cond <- function(L0 = 0.01, r = 0.1, K = 2, cond = cbind.data.frame(L = 0.2, par = "r", val = 2), N = 100){
    
  # Create a vector L of length N, which has value L0 at L[1]
    L <- c(L0, rep(NA, N-1))
    
    # Iterate N steps of the difference equation with values passed for L0, K and r.
    cnt <- 1
    for(i in seq_along(L)){
      
        # Check if the current value of L is greater than the threshold for the current conditional rule in cond
        if(L[i] > cond$L[cnt]){
          
            # If the threshold is surpassed, change the parameter settings by evaluating: cond$par = cond$val 
            eval(parse(text = paste(cond$par[cnt], "=", cond$val[cnt])))
          
            # Update the counter if there is another conditional rule in cond
            if(cnt < nrow(cond)){cnt <- cnt + 1}
        }
      
        # Van Geert growth model
        L[i+1] <- L[i] * (1 + r - r * L[i] / K)
    }
    return(ts(L))
}


# Plot with the default settings.
library(lattice)
xyplot(vanGeert.cond())
```

The 'trick' used here is to define the function such that it can take a set of conditional rules and apply them sequentially during the iterations. The conditional rule is passed as a `data.frame`, but one could also use a `list` object. 

```{r}
(cond <- cbind.data.frame(L = c(0.2, 0.6), par = c("r", "r"), val = c(0.5, 0.1)))
plot(vanGeert.cond(cond=cond))
points(which(vanGeert.cond(cond=cond)>0.2)[1],0.2,col="red",pch=16)
points(which(vanGeert.cond(cond=cond)>0.6)[1],0.6,col="red",pch=16)
legend(60,1,legend = c("L>0.2 - change r to 0.5","L>0.6 - change r to .1"),col = c("red","red"),pch=16)

```


</br>


### Auto-conditional stages {.tabset .tabset-fade .tabset-pills}

Another conditional change we might want to explore is that when a certain growth level is reached the carrying capacity $K$ increases, reflecting that new resources have become available to support further growth.

#### Questions {-}

* Build your model in such a way that when $L > 0.99$, $K$ changes to the value 2, keep $r = 0.1$
* Try to also to change the growth rate r after reaching $L > 0.99$ 
* Recreate the model on the tab `Van Geert (3 stages)` in `R`


#### Answers {-}

We'll continue with the more general function.

```{r, fig.show='asis'}
(cond <- cbind.data.frame(L = 0.99, par = "K", val = 2))
plot(vanGeert.cond(r=0.1,cond=cond))
points(which(vanGeert.cond(cond=cond)>0.99)[1],0.99,col="red",pch=16)
legend(60,1, legend = "L>0.99 - change r to 2", col = "red", pch=16)
```


Or, combine a change of `r` and a change of `k`
```{r, fig.show='asis'}
(cond <- cbind.data.frame(L = c(0.99,0.99,1.99,1.99), par = c("r","K","r","K"), val = c(0.1,2,2.1,3)))
plot(vanGeert.cond(r=0.2,cond=cond))
points(which(vanGeert.cond(r=0.2,cond=cond)>cond$L[1])[1],cond$L[1],col="red",pch=21)
points(which(vanGeert.cond(r=0.2,cond=cond)>cond$L[2])[1],cond$L[2],col="blue",pch=46,cex=4)
points(which(vanGeert.cond(r=0.2,cond=cond)>cond$L[3])[1],cond$L[3],col="red",pch=21)
points(which(vanGeert.cond(r=0.2,cond=cond)>cond$L[4])[1],cond$L[4],col="blue",pch=46,cex=4)
legend(60,1,legend = paste0("L>",cond$L," - change ",cond$par," to ",cond$val),col = c("red","blue"),pch=c(21,46), pt.cex = c(1,4))


# A fantasy growth process
(cond <- cbind.data.frame(L = c(0.1, 1.99, 1.999, 2.5, 2.9), par = c("r", "K", "r", "r","K"), val = c(0.3, 3, 0.9, 0.1, 1.3)))
xyplot(vanGeert.cond(cond=cond))
```


### Growth models in package `casnet`

To model the auto catalytic growth equations we provide a function `growth.ac()`, which is able to simulate all of the processes discussed in the lectures. Using just a few lines of code, each of the 4 difference equations used in the assignments can be simulated. Basically the code block below contains the solutions to the Linear Map, the stylized Logistic Map and the Van Geert model for cognitive growth.

```{r, echo=TRUE, message=FALSE, warning=FALSE, tidy=FALSE}
growth_ac <- function(Y0 = 0.01, r = 1, k = 1, N = 100, type = c("driving", "damping", "logistic", "vanGeert")[1]){
    # Create a vector Y of length N, which has value Y0 at Y[1]
    if(N>1){
    Y <- as.numeric(c(Y0, rep(NA,N-2)))
    # Conditional on the value of type ... 
    switch(type, 
           # Iterate N steps of the difference function with values passed for Y0, k and r.
           driving  = sapply(seq_along(Y), function(t) Y[[t+1]] <<- r * Y[t] ),
           damping  = k + sapply(seq_along(Y), function(t) Y[[t+1]] <<- - r * Y[t]^2 / k),
           logistic = sapply(seq_along(Y), function(t) Y[[t+1]] <<- r * Y[t] * ((k - Y[t]) / k)),
           vanGeert = sapply(seq_along(Y), function(t) Y[[t+1]] <<- Y[t] * (1 + r - r * Y[t] / k)) 
    )}
    return(ts(Y))
}

# Call the function with default settings and r = 1.1
Y <- growth_ac(r = 1.1)
plot(Y)
```

Some notes about this function:

* To select which growth process to simulate, the argument `type` is defined which takes the values `driving` (default), `damping`, `logistic` and `vanGeert`. 
    + The statement `switch(type, ...)` will iterate an equation based on the value of `type`.
* A `time series` object is returned due to the function `ts()`. This is a convenient way to represent time series data, it can also store the sample rate of the signal and start and end times.
    + Most of the basic functions, like `plot()` and `summary()` will recognise a time series object when it is passed as an argument and use settings appropriate for time series data.
* The `sapply()` function iterates $t$ from $1$ to the number of elements in $Y$ (`seq_along(Y)`) and then applies the function.
* The double headed arrow `<<-` is necessary because we want to update vector $Y$, which is defined outside the `sapply()` environment.

</br>
</br>


----

</br>
</br>

# **Multivariate systems**

By multivariate systems we do not simply mean there is more than 1 iterative process (dynamical variable), but that there is a coupling between at least two iterative processes that makes the outcomes of each individual process at each time step (partially) dependent on the previous time step of other processes.

You'll start by expanding on the previous assignments, in which you modelled changes in the parameter values by implementing an `IF ... THEN` rule that evaluated whether some threshold level was reached. In the assignment you'll create a kind of **one-directional coupling**, or **supportive interaction**, which is just a precursor to modelling more interesting interaction dynamics.

 You'll model the so-called Lottka-Volterra equations, representing **competetive interactions** between two iterative processes, also known as **predator-prey dynamics**. You can imagine these equations represent populations of *Foxes* and *Rabbits* in which the population of the  predators (Foxes) depends on the available prey (Rabbits) and vice versa. To do model these equations, you'll need to learn a bit about modelling **flows**: Continuous time differential equations. 


## **One directional coupling**

A more realistic way to model a change of growth parameter `r` using `IF ... THEN` statements, is to make the growth of a variable depend on another process. The proper *dynamical* way to do this would be to define a coupled system of difference or differential equations in which the interaction dynamics completely regulate growth (or decay) of the variables directly, such as the predator-prey system in the next assignment. 

In this assignment we'll keep using at least one 'conditional' rule and try to implement emergent levels by creating **supportive interactions** between separate processes. This is probably the most simple version of *interaction dynamics*, *coupling dynamics*, or *multiplicative interactions* with more then one iterative process. Basically, the processes will still be rather independent from one another and the interaction simply means a 'shock' is delivered to an individual process, after which the system just continuous to evolve on its own. 


### Connected growers {.tabset .tabset-fade .tabset-pills}

For this assignment, we'll use a different way to achieve the levels you created in the previous assignment in which you used `IF ... THEN` statements to achieve changes in dynamics. The conditional rules that change the parameters were 'given', we'll try to be a bit more dynamical... 

Have a look at [this spreadsheet](https://docs.google.com/spreadsheets/d/1cbC58yTg-pOoThMQY0UyzIQ3ZiaTosJlqQFkRHt9LJg/edit?usp=sharing) in which a model is implemented that is discussed in the following chapter by Paul van Geert:

[Van Geert, P. (2003). Dynamic systems approaches and modeling of developmental processes. In J. Valsiner and K. J. Conolly (Eds.), Handbook of developmental Psychology. London: Sage. Pp. 640-672](http://www.paulvangeert.nl/publications_files/Handbook%20Valsiner%20and%20Conolly.pdf).

Relevant text pertaining to this assignment:

* BUILDING YOUR OWN MODELS: A SHORT TUTORIAL (p. 665)
    + The model is discussed in "A Model of Hierarchically Connected Growers"

#### Questions {-}

Try to model this example of supportive interaction in `R`. 

```{block2, type="rmdnote"}
The spreadsheetmodel tries to mimic Figures 28.6 to 28.8 in van Geert (2003), but only column `A` receives a 'support' boost As noted in the text:
  
"A similar set of equations can be set up, applying to columns B and C respectively."

```


* You can use the template function below
* You can look in the spreadsheet, or look in the chapter to find the right formula's for `A` and `B`
* Make a plot like in the spreadsheet
    + Note that the function returns a data frame with A, B and C

```{r, eval=FALSE}
vanGeert.support <- function(A0 = 0.01, Ar = .5, AK = 1, 
                             B0 = 0.01, Br = .2, BK = 1, 
                             C0 = 0, C1 = 1, Cr = 1, CK = 1, 
                             AtoC = 0.8, BsupportA = .5, N = 100){
  
  # Create a vector A,B,C of length N, which has value A0,B0,C0 at A[1],B[1],C[1]
  A <- c(A0, rep(NA, N-1))
  B <- c(B0, rep(NA, N-1))
  C <- c(C0, rep(NA, N-1))
  
  # Then, you need create the iteration
  for(i in 1:(N-1)){
    
    # You need to monitor the value of L and change the growth rate according to a rule
    if(A[i]<AtoC){
      C[i+1] <- # Value of C0
    } else {
      C[i+1] <- # Value of C1
    }
    
    A[i+1] <- # Implement A with support from B
    B[i+1] <- # Implement B whose growth depends on C
  }
  
  return(data.frame(A=A,B=B,C=C))
}

```



#### Answers {-}

* The template filled in with the correct functions and parameter settings

```{r, eval=TRUE}
vanGeert.support <- function(A0 = 0.01, Ar = .5, AK = 1, 
                             B0 = 0.01, Br = .2, BK = 1, 
                             C0 = 0, C1 = 1, Cr = 1, CK = 1, 
                             AtoC = 0.8, BsupportA = .5, N = 100){
  
  # Create a vector A,B,C of length N, which has value A0,B0,C0 at A[1],B[1],C[1]
  A <- c(A0, rep(NA, N-1))
  B <- c(B0, rep(NA, N-1))
  C <- c(C0, rep(NA, N-1))
  
  # Then, you need create the iteration
  for(i in 1:(N-1)){
    
    # You need to monitor the value of L and change the growth rate according to a rule
    if(A[i]<AtoC){
      C[[i+1]] <- C0
    } else {
      C[[i+1]] <- C1
    }
    
    A[i+1] <- A[i] + A[i] * Ar * (1 - A[i] / AK) + A[i] * B[i] * BsupportA # A10 + A10 * $A$5 * (1 - A10 / $A$4) + A10 * B10 * $B$2
    B[i+1] <- B[i] + B[i] * Br * (1 - B[i] / BK) * C[i]                    # B10 + B10 * $B$5 * (1 - B10 / $B$4) * C10
    
  }
  
  return(data.frame(A=A,B=B,C=C))
}
```

* Make a plot like in the spreadsheet
    + Note that the function returns a data frame with A, B and C

```{r}
Y <- vanGeert.support()

ts.plot(ts(Y$A), ts(Y$B), ts(Y$C),
        gpars = list(xlab = "time (a.u.)",
                     ylab = expression(Y(t)),
                     main = expression(paste("Hierarchical Connected Growers ")),
                     lwd = rep(2,3),
                     lty = c(1:3),
                     col = c("darkred","darkblue","darkgreen")
                     )
        )
legend(50, .8, c("B will support A by adding .5*B",
                 paste0("B starts to grow at time ",which(Y$A > .8)[1]), 
                 paste0("C turns to 1 at time ",which(Y$A > .8)[1])),
       lwd = rep(2,3), lty = c(1:3), col = c("darkred","darkblue","darkgreen"), merge = TRUE)

```


* This would be the *not-so-dynamical* way to do it:

```{r, eval=TRUE, include=TRUE}
# Generate 3 timeseries
Y1 <- growth_ac(k = 2, r =.2, type = "vanGeert")
# Y2 and Y3 start at r = 0.001
Y3 <- Y2 <- growth_ac(k = 2, r = 0.001, type = "vanGeert")

# Y2 and Y3 start when k is approached
c1 <- 1.6
c2 <- 2.2

# Here we just swap the new values with the old values
Y2[Y1 > c1] <- growth_ac(r = .3, k = 3, type = "vanGeert", N = sum(Y1 > c1))
Y3[Y2 > c2] <- growth_ac(r = .5, k = 4, type = "vanGeert", N = sum(Y2 > c2))

# Make a nice plot
ts.plot(Y1, Y2, Y3,
        gpars = list(xlab = "time (a.u.)",
                     ylab = expression(Y(t)),
                     main = expression(paste("'Unconnected' Growers ",Y[t+1]==Y[t]*(1 + r - r*Y[t]))),
                     lwd = rep(2,3),
                     lty = c(1:3),
                     col = c("darkred","darkblue","darkgreen")
                     )
        )
legend(1, 3.8, c("Y1(0):  r = .2",
                 paste0("Y2(",which(Y1 > c1)[1],"): r = .3"), 
                 paste0("Y3(",which(Y2 > c2)[1],"): r = .5")),
       lwd = rep(2,3), lty = c(1:3), col = c("darkred","darkblue","darkgreen"), merge = TRUE)
```


### Other demonstrations of dynamic modeling using spreadsheets

See the website by [Paul Van Geert](http://www.paulvangeert.nl/research.htm), scroll down to see models of:

* Learning and Teaching
* Behaviour Modification
* Connected Growers
* Interaction during Play

</br>



</br>
</br>

## **The predator-prey model**


In this assignment we will look at a 2D coupled dynamical system: **the Predator-Prey model** (aka [Lotka-Volterra equations](https://en.wikipedia.org/wiki/Lotka???Volterra_equations)).      

The dynamical system is given by the following set of first-order differential equations, one represents changes in a population of predators, (e.g., **F**oxes: $f_F(R_t,F_t)$ ), the other represents changes in a population of prey, (e.g., **R**abbits: $f_R(R_t,F_t)$ ).


\begin{align}
\frac{dR}{dt}&=(a-b*F)*R \\
\\
\frac{dF}{dt}&=(c*R-d)*F
(\#eq:lv)
\end{align}


This is not a *difference* equation but a *differential* equation, which means building this system is not as straightforward as was the case in the previous assignments. Simulation requires a numerical method to 'solve' this differential equation for time, which means we need a method to approach, or estimate continuous time in discrete time. Below you will receive a speed course in one of the simplest numerical procedures for integrating differential equations, the [Euler method](https://en.wikipedia.org/wiki/Euler_method) ([also see the notes in the course book](https://darwin.pwo.ru.nl/skunkworks/courseware/1819_DCS/numerical-integration-to-simulate-continuous-time.html)).    


### Euler's Method {-}

A general differential equation is given by:

\begin{equation}
\frac{dx}{dt} = f(x)
(\#eq:diff)
\end{equation}

Read it as saying: "_a change in $x$ over a change in time is a function of $x$ itself_". This can be approximated by considering the change to be over some constant, small time step $\Delta$:

\begin{equation}
\frac{(x_{t+1} = x_t)}{\Delta} = f(x_t)
(\#eq:Euler)
\end{equation}

After rearranging the terms a more familiar form reveals itself:

\begin{align}
x_{t+1} &= x_t &= f(x_t) * \Delta \\
x_{t+1} &= f(x_t) * \Delta + x_t
(\#eq:Euler2)
\end{align}

This looks like an ordinary iterative process, $\Delta$ the *time constant* determines the size of time step taken at every successive iteration. For a 2D system with variables **R** and **F** on would write:
    
\begin{align}
R_{t+1} &= f_R(R_t,Ft) * \Delta + R_t \\
F_{t+1} &= f_F(R_t,F_t) * \Delta + F_t
(\#eq:EulerRF)
\end{align}
    
</br>
</br>

### Foxes and Rabbits in R {.tabset .tabset-fade .tabset-pills}

Use `R` to implement [the model in this spreadsheet](https://docs.google.com/spreadsheets/d/1rZDEo8XYNCzhRWrWOli7DDB6f4m87p-hjYv2_KlBLa4/edit?usp=sharing)


#### Questions{-}

* To get the right equations, you'll have to substitute $f_R(R_t,Ft)$ and $f_F(R_t,F_t)$ with the differential equations for Foxes and Rabbits provided above.
* Use the following parameter settings:
    + `N` = `1000`
    + `a` and `d` are `1`
    + `b` and `c` are `2` 
    + Initial values `R0` and `F0` are `0.1`
* Make a plot of the time series for Foxes and Rabbits
* Make a plot of the 2D state space of this system (aka *phase plane*)
    + Examine how the state space changes for different parameter settings

#### Answers {-}


The Euler set-up:
$$
\begin{align}
R_{t+1} &= f_R(R_t,Ft) * \Delta + R_t \\
F_{t+1} &= f_F(R_t,F_t) * \Delta + F_t
\end{align}
$$

With the equations:
$$
\begin{align}
R_{t+1} &=  (a-b*F_t)*R_t * \Delta + R_t \\
\\
F_{t+1} &=  (c*R_t-d)*F_t * \Delta + F_t
\end{align}
$$


```{r}
library(plyr)

# Parameters
N  <- 1000
a  <- d <- 1
b  <- c <- 2 
R0 <- F0 <- 0.1
R  <- as.numeric(c(R0, rep(NA,N-1)))
F  <- as.numeric(c(F0, rep(NA,N-1)))

# Time constant
delta <- 0.01

# Numerical integration of the lpredator-prey system
l_ply(seq_along(R), function(t){
    R[[t+1]] <<- (a - b * F[t]) * R[t] * delta + R[t] 
    F[[t+1]] <<- (c * R[t] - d) * F[t] * delta + F[t] 
    })

# Note different behaviour when ts() is applied
xyplot(cbind(ts(R),ts(F)))
xyplot(R ~ F, pch = 16)
```


### **Growth functions in continuous time**

The result of applying a method of **numerical integration** such a s Euler's method is called a **numerical solution** of the differential equation. The **analytical solution** is the equation which will give you a value of $Y$ for any point in time, given an initial value $Y_0$. Systems which have an analytical solution can be used to test the accuracy of **numerical solutions**.

The logistic differential equation $\frac{dY}{dt} = r*Y_t*\left(1-\frac{Y_t}{K}\right)$ has an analytic solution given as:

$$
\frac{K * Y_0}{Y_0 + \left(K-Y_0\right) * e^{-r*t} }
$$
We have the implemented discrete time growth function in `growth_ac()`, but could easily adapt all those functions to make them continuous by using Euler's method. 

Below is a comparison of the analytic solution of the continuous logistic growth function, with the result of using Euler's method to approximate the continuous time function.

```{r}
library(plyr)
library(RColorBrewer)
# Parameter settings
delta <- .1
N <- 100

# W'ell make time vector to get a smoot analytis solution
TIME <- seq(1,N,by=1)

r <- 3.52 # Remember this setting? This means Chaos in discrete time, in continous time it means extreme growth.

# Carrying capacity and initial condition are chosen se we get a nice plot
K  <-  8
Y0 <- .01

deltas <- c(delta, delta*.5,delta*.25,delta*.2,delta*.15,delta*.1,delta*.05)
cl <- brewer.pal(length(deltas),"Set2")

# Numerical integration of the logistic differential equation as implemented in growth_ac()
Y.euler <- llply(deltas, function(d){
  # Use the values in deltas to get iterate the logistic flow
  Y <- as.numeric(c(Y0, rep(NA,length(TIME)-1)))
  ts(sapply(seq_along(TIME), function(t) Y[[t+1]] <<- ((r * Y[t] * (K - Y[t])) * d) + Y[t] ))
  })

# Analytical solution
# We have to scale TIME a bit, normally we would use 1/TIME
Y.analytic <- ts( K * Y0 / (Y0 + (K - Y0) * exp(-1*(r*TIME/30))) )

ts.plot(Y.euler[[1]], Y.euler[[2]] ,Y.euler[[3]], Y.euler[[4]], Y.euler[[5]], Y.euler[[6]], Y.euler[[7]], Y.analytic,
        gpars = list(xlab = "time (a.u.)",
                     ylab = expression(Y(t)),
                     main = expression(paste("Analytic vs. Numerical solution to ",frac(dY,dt)==Y[t]*r*(K-Y[t]))),
                     lwd = c(rep(1,length(deltas)),3),
                     lty = 1,
                     col = c(cl,"red3"),
                     ylim = c(0,10),
                     cex.main = .8
                     )
        )

legend(70, 5, c(paste("Euler: delta =",deltas),"Analytic solution (t/30)"),
       lwd = c(rep(1,length(deltas)),2),
       lty = 1,
       col = c(cl,"red3"),
       merge = TRUE, cex = .8)
```

```{block2, type="rmdnote"}
If you want to learn how to put mathematical operators in legends and axes using `expression()`, see the webpage [Mathematical Annotation in R](http://vis.supstat.com/2013/04/mathematical-annotation-in-r/)
```

When `delta` is relatively large, the fluctuations still resemble deterministic chaos, as delta gets closer to 0, this 'smooths out' the fluctuations and the analytic solution is approached. As a consequence, it takes longer and longer to reach `K`. However, 'longer' is of course relative to the unit of time represented on the x-axis, because we're simulating, they are arbitrary units.



## **Beyond Foxes and Rabbits**

In the [course book](https://darwin.pwo.ru.nl/skunkworks/courseware/1819_DCS/advmodels.html
) we listed some more advanced models that build on the basic models we discussed here.

----


</br>
</br>



<!--chapter:end:ASSIGNMENTS_D1_2.Rmd-->

---
title: "Day1 Intro Math of Change: Fitting Parameters of Analytic Solutions [Extra]"
author: "Fred Hasselman"
date: "1/14/2019"
output: 
  bookdown::html_document2: 
    variant: markdown+hard_line_breaks
    fig_caption: yes
    highlight: pygments
    keep_md: yes
    self_contained: yes
    number_sections: yes
    theme: spacelab
    toc: yes
    toc_float: true
    collapsed: false
    smooth_scroll: true
    code_folding: show
    bibliography: [refs.bib]
    biblio-style: apalike
    csl: apa.csl
    includes:
        before_body: assignmentstyle.html
    pandoc_args: ["--number-offset","0"]

---

```{r, include=FALSE}
knitr::opts_chunk$set(include=TRUE)
```


<!-- </br> -->
<!-- </br> -->


# **Fitting Parameters of Analytic Solutions [EXTRA]**


## **Non-linear Growth curves**


### Fitting the Logistic Growth model in SPSS {.tabset .tabset-fade .tabset-pills}

#### Questions {-}

Open the file [Growthregression.sav](https://github.com/FredHasselman/The-Complex-Systems-Approach-Book/blob/master/assignments/assignment_data/BasicTSA_nonlinreg/GrowthRegression.sav), it contains two variables: `Time` and `Y(t)`. 

This is data from an iteration of the logistic growth differential equation you are familiar with by now, but lets pretend its data from one subject measured on 100 occasions.

* Plot Y(t) against Time Recognize the shape?
* To get the growth parameter well try to fit the solution of the logistic flow with SPSS non-linear regression
     + Select non-linear from the `Analysis` >> `Regression` menu.
     + Here we can build the solution equation. We need three parameters:
            a. **Yzero**, the initial condition.
            b. *K*, the carrying capacity.
            c. *r*, the growth rate.
     + Fill these in where it says `parameters` give all parameters a starting value of  $0.01$

Take a good look at the analytic solution of the (stylized) logistic flow:

$$
Y(t)  =  \frac{K * Y_0}{Y_0 + \left(K-Y_{0}\right) * e^{-r*t} }
$$

Try to build this equation, the function for $e$ is called `EXP` in `SPSS` (`Function Group` >> `Arithmetic`)
Group terms by using parentheses as shown in the equation.

* If you think you have built the model correctly, click on `Save` choose `predicted values`. Then paste your syntax and run it!
    + Check the estimated parameter values.
    + Check $R^2$!!!

* Plot a line graph of both the original data and the predicted values. (Smile)

You can try do the same using a polynomial approach.

* A polynomial fishing expedition:
    + Create time-varying 'covariates' of $Y(t)$:
```
COMPUTE T1=Yt * Time.
COMPUTE T2=Yt * (Time ** 2). 
COMPUTE T3=Yt * (Time ** 3). 
COMPUTE T4=Yt * (Time ** 4). 
EXECUTE.
```
    + Use these variables as predictors of $Y(t)$ in a regular linear regression analysis. This is called a *polynomial regression*: Fitting combinations of curves of different shapes on the data.
    + Before you run the analysis: Click `Save` Choose `Predicted Values: Unstandardized`

```{block2, type = "rmdselfthink"}
Look at $R^2$. This is also almost 1!

Which model is better? 

Think about this: Based on the results of the linear regression what can you tell about the *growth rate*, the *carrying capacity* or the *initial condition*?
```

*	Create a line graph of $Y(t)$, plot the predicted values of the non-linear regression and the unstandardised predicted values of the linear polynomial regression against `time` in one figure.
    + Now you can see that the shape is approximated by the polynomials, but it is not quite the same. Is this really a model of a growth process as we could encounter it in nature?


#### Answers {-}

The solutions are also provided as an `SPSS` [syntax file](https://github.com/FredHasselman/The-Complex-Systems-Approach-Book/blob/master/assignments/assignment_data/BasicTSA_nonlinreg/GrowthRegression_sol.sps) file.

Or copy the block below:
```
GRAPH
  /LINE(SIMPLE)=VALUE(Yt) BY Time.

* NonLinear Regression.
MODEL PROGRAM  Yzero=0.01 r=0.01 K=0.01.
COMPUTE  PRED_=K*Yzero/(Yzero + (K-Yzero) * EXP(-1*(r * Time))).
NLR Yt
  /PRED PRED_
  /SAVE PRED
  /CRITERIA SSCONVERGENCE 1E-8 PCON 1E-8.

GRAPH
  /LINE(MULTIPLE)=VALUE(Yt PRED_) BY Time.

COMPUTE T1=Yt * Time.
COMPUTE T2=Yt * (Time ** 2).
COMPUTE T3=Yt * (Time ** 3).
COMPUTE T4=Yt * (Time ** 4).
EXECUTE.

REGRESSION
  /MISSING LISTWISE
  /STATISTICS COEFF OUTS R ANOVA
  /CRITERIA=PIN(.05) POUT(.10)
  /NOORIGIN 
  /DEPENDENT Yt
  /METHOD=ENTER T1 T2 T3 T4
  /SAVE PRED.


GRAPH
  /LINE(MULTIPLE)=VALUE(Yt PRED_ PRE_1) BY Time.
```

* The point here is that the polynomial regression approach is "just" curve fitting ... adding components until a nice fit is found ... but what does component $Y_t^4$ represent? A quartic sub-process?
* Fitting the solution of the the logistic function will give us parameters we can interpret unambiguously: Carrying capacity, growth rate, starting value.


### Fitting the Logistic Growth model in R {.tabset .tabset-fade .tabset-pills}

There are several packages that can perform non-linear regression analysis, the function most resembling the approach used by `SPSS` is `nls` in the default `stats` package.  

The easiest way to do this is to first define your function (i.e., the solution) and then fit it using starting values for the parameters.

```{r, cache=FALSE}
library(rio)
df <- import("https://github.com/FredHasselman/The-Complex-Systems-Approach-Book/raw/master/assignments/assignment_data/BasicTSA_nonlinreg/GrowthRegression.sav", setclass = "tbl_df")

# Create the function for the analytic solution
# Same as SPSS syntax: PRED_=K*Yzero/(Yzero + (K-Yzero) * EXP(-1*(r * Time))).
log.eq <- function(Yzero, r, K, Time) {
    K*Yzero/(Yzero + (K-Yzero) * exp(-1*(r * Time)))
}
```

There is one drawback and you can read about in the help pages:

> Warning. Do not use nls on artificial "zero-residual" data.

This means, "do not use it on data generated by a deterministic model which has no residual error", which is exactly what the time series in this assignment is, it is the output of the logistic flow.

So, this will give an error:
```{r, echo=TRUE, message=TRUE, warning=TRUE, cache=FALSE, eval=FALSE}
# Fit this function ... gives an error
# The list after 'start' provides the initial values
m.log <- nls(Yt ~ log.eq(Yzero, r, K, Time), data = df, start = list(Yzero=.01, r=.01, K=1), trace = TRUE)
```



We'll provide you the answers right away, but be sure to figure out what is going on.



#### Answers{-}

It is possible to fit these ideal data using package `minpack.lm`, which contains function `nlsM`.

```{r}
library(minpack.lm)

m.log <- nlsLM(Yt ~ log.eq(Yzero, r, K, Time), data = df, start = list(Yzero = .01, r=.01, K=0.1))

summary(m.log)
```

In order to look at the model prediction, we use `predict()` which is defined for almost all model fitting functions in `R`
```{r}
Ypred <- predict(m.log)

plot(ts(df$Yt), col="gray40", lwd=5, ylab = ("Yt | Ypred"))
lines(Ypred, col="gray80", lwd=2, lty=2)
```

Then we do a polynomial regression using `lm`:
```{r, echo=TRUE, message=FALSE, warning=FALSE}
# Mimic the SPSS syntax
attach(df)
  df$T1 <- Yt * Time
  df$T2 <- Yt * (Time^2) 
  df$T3 <- Yt * (Time^3) 
  df$T4 <- Yt * (Time^4)
detach(df)

m.poly <- lm(Yt ~ T1 + T2 + T3 + T4, data=df)
summary(m.poly)
```

Then, predict and plot!
```{r}
Ypoly <- predict(m.poly)

plot(ts(Ypoly), col="blue1", lwd=2, ylab = ("Ypoly (blue) | Ypred (red)"))
lines(Ypred, col="red1", lwd=2)
```

SPSS computes an $r^2$ value for non-linear regression models, which doesn't make a lot of sense if you think about it. Here we van just compare the residual errors:

* Polynomial regression: $0.005506$
* Analytic solution: $0.002865$

Slightly less residual error for the analytic solution, using less parameters to fit the model (3 vs. 5). **More important:**, the parameters of the analytic solution have a direct interpretation in terms of growth processes.



<!--chapter:end:ASSIGNMENTS_D1_3_FitParams.Rmd-->

---
title: "Day2 Basic Nonlinear Time Series Analysis"
author: "Fred Hasselman"
date: "1/14/2019"
output: 
  bookdown::html_document2: 
    variant: markdown+hard_line_breaks
    fig_caption: yes
    highlight: pygments
    keep_md: yes
    self_contained: yes
    number_sections: yes
    theme: spacelab
    toc: yes
    toc_float: true
    collapsed: false
    smooth_scroll: true
    code_folding: show
    bibliography: [refs.bib, packages.bib]
    biblio-style: apalike
    csl: apa.csl
    includes:
        before_body: assignmentstyle.html
    pandoc_args: ["--number-offset","1"]
editor_options: 
  chunk_output_type: console
---


```{r, include=FALSE}
knitr::opts_chunk$set(include=TRUE, tidy = FALSE)
```


# **Basic Nonlinear Timeseries Analysis** 

Many nonlinear analyses can be considered "descriptive" techniques, that is, the aim is not to fit the parameters of a model, but to describe, quantitatively, some aspects of how one value changes into another value over time.


### Intuitive notion of Fractal Dimension {-}

A qualitative description of the fractal dimension of a time series (or 1D curve) can be given by deciding whether the curve looks/behaves like a line, or, like a plane. 

As can be seen in the figure below, if slow processes (low frequencies) dominate the signal, they are more *line-like* and will have a fractal dimension closer to `1`. If fast processes (high frequencies) dominate the signal, they are more *plane-like* and will have a fractal dimension closer to `2`. 


```{r fig.height=10}
library(casnet)
library(plyr)

N    <- 512
noises <- round(seq(-3,3,by=.5),1)

yy <- llply(noises, function(a){cbind(noise_powerlaw(alpha = a, N = 512, seed = 1234))})
names(yy) <- noises
tmp<- data.frame(yy,check.names = FALSE)

plotTS_multi(tmp, ylabel = "Scaling exponent alpha")

```


## Three time series

We are going to analyse 3 different time series using different techniques. The goal is to describe the dynamics, temporal patterns (if any) and perhaps other interesting characteristics of these time series.


**Importing data**

Two ways:

A. By downloading:

  1. Follow the link, e.g. for [`series.xlsx`](https://github.com/complexity-methods/CSA-assignments/blob/master/assignment_data/BasicTSA_arma/series.xlsx).
  2. On the Github page, find a button marked **Download** (or **Raw** for textfiles).
  3. Download the file
  4. Load it into `R` using the code below       

```{r, eval=FALSE}
library(rio)
series <- rio::import("series.xlsx")
```
     
      
B. By importing from Github:

   1. Copy the `url` associated with the **Download**  button [on Github](https://github.com/complexity-methods/CSA-assignments/blob/master/assignment_data/BasicTSA_arma/series.csv) (right-clik).
   2. The copied path should contain the word 'raw' somewhere in the url.
   3. Call `rio::import(url)`
        
```{r}
library(rio)
series <- rio::import("https://github.com/complexity-methods/CSA-assignments/raw/master/assignment_data/BasicTSA_arma/series.xlsx")
```



### Relative Roughness   {.tabset .tabset-fade .tabset-pills}

Relative Roughness is calculated using the following formula:    

\begin{equation}
RR = 2\left[1 + \frac{\gamma_1(x_i)}{Var(x_i)}\right]
(\#eq:RR)
\end{equation}

The numerator in the formula stands for the `lag 1` autocovariance of the time series $x_i$, this is the unstanderdised lag1 autocorrelation. Check the function `acf()` to figure out how to get it. The denominator stands for the (global) variance of $x_i$ which all statistics packages can calculate. Another way to describe the variance is: `lag 0` autocovariance.


#### Questions {-}

*	Construct a graphical representation of the time series, and inspect their dynamics visually (plot the time series).
* Write down your first guesses (i.e., which one looks more like a line than a plane, which one looks more 'smooth' than 'rough'?).
*	Next, explore some measures of central tendency and dispersion (calculate the mean, variance standard deviation)
*	Compute the Relative Roughness for each time series.
   - You can try to figure it out for yourself by using Equation \@ref(eq:RR)
   - There is also a function in `casnet` that will do everything for you: `fd_RR()`


Use Figure \@ref(fig:RRf3) to lookup which value of $RR$ corresponds to which type of dynamics:


```{r RRf3, fig.cap="Coloured Noise versus Relative Roughness", fig.align='center',echo=FALSE, include=TRUE}
knitr::include_graphics('images/RRfig3.png')
```


### Answers {-}




### Sample Entropy  {.tabset .tabset-fade .tabset-pills}

Use the `sample_entropy()` function in package `pracma`.

#### Questions {-}

* Calculate the Sample Entropy of the two sets of three time series you now have.
    + Use your favourite function to estimate the sample entropy of the three time series. Use for instance a segment length `edim` of 3 data points, and a tolerance range `r` of 1 * the standard deviation of the series. What values do you observe?
    + Can you change the absolute SampEn outcomes by 'playing' with the m parameter? If so, how does the outcome change, and why?
    + Can you change the absolute SampEn outcomes by 'playing' with the r parameter If so, how does the outcome change, and why?
    + Do changes in the relative SampEn outcome change the outcomes for the three time series relative to each other?

*	Extra: Go back to the assignment where you generated simulated time series from the logistic map.


#### Answers {-}

Change some of the parameters.

```{r,collapse=TRUE}
library(rio)
library(pracma)

# ACF assignment data `series`
cat(paste0("\nseries.TS1 m=3\n",sample_entropy(series$TS_1, edim = 3, r = sd(series$TS_1))))
cat(paste0("\nseries.TS2 m=3\n",sample_entropy(series$TS_2, edim = 3, r = sd(series$TS_2))))
cat(paste0("\nseries.TS3 m=3\n",sample_entropy(series$TS_3, edim = 3, r = sd(series$TS_3))))


# ACF assignment data `series`
cat(paste0("\nseries.TS1 m=6\n",sample_entropy(series$TS_1, edim = 6, r = sd(series$TS_1))))
cat(paste0("\nseries.TS2 m=6\n",sample_entropy(series$TS_2, edim = 6, r = sd(series$TS_2))))
cat(paste0("\nseries.TS3 m=6\n",sample_entropy(series$TS_3, edim = 6, r = sd(series$TS_3))))


# ACF assignment data `series`
cat(paste0("\nseries.TS1 m=6, r=.5\n",sample_entropy(series$TS_1, edim = 3, r = .5*sd(series$TS_1))))
cat(paste0("\nseries.TS2 m=6, r=.5\n",sample_entropy(series$TS_2, edim = 3, r = .5*sd(series$TS_2))))
cat(paste0("\nseries.TS3 m=6, r=.5\n",sample_entropy(series$TS_3, edim = 3, r = .5*sd(series$TS_3))))

```

The change of `m` keeps the relative order, change of `r` for the same `m` does not.

**Values of other time series**

```{r message=FALSE, warning=FALSE, collapse=TRUE}

# Logistic map
library(casnet)
cat("\nLogistic map\nr=2.9\n")
y1<-growth_ac(r = 2.9,type="logistic")
sample_entropy(y1, edim = 3, r = sd(y1))
cat("\nLogistic map\nr=4\n")
y2<-growth_ac(r = 4,type="logistic")
sample_entropy(y2, edim = 3, r = sd(y2))


```



<!--chapter:end:ASSIGNMENTS_D2_basicNLTSA.Rmd-->

---
title: "Basic (Non-linear) Time Series Analysis"
author: "Fred Hasselman"
date: "1/14/2019"
output: 
  bookdown::html_document2: 
    variant: markdown+hard_line_breaks
    fig_caption: yes
    highlight: pygments
    keep_md: yes
    self_contained: yes
    number_sections: yes
    theme: spacelab
    toc: yes
    toc_float: true
    collapsed: false
    smooth_scroll: true
    code_folding: show
    bibliography: [refs.bib, packages.bib]
    biblio-style: apalike
    csl: apa.csl
    includes:
        before_body: assignmentstyle.html
    pandoc_args: ["--number-offset","1"]
editor_options: 
  chunk_output_type: console
---


```{r message=FALSE, warning=FALSE, include=FALSE}
knitr::opts_chunk$set(include=TRUE, message = FALSE, warning = FALSE, tidy = FALSE, collapse = FALSE)
library(tidyverse)
```


# Time series analyses

## **Basic TSA** 

> "order is essentially the arrival of redundancy in a system, a reduction of possibilities"
>
> --- Von Ferster (2003)

In this course we will not discuss the type of linear time series models known as Autoregressive Models (e.g. AR, ARMA, ARiMA, ARfiMA) summarised on [this Wikipedia page on timeseries](https://en.wikipedia.org/wiki/Time_series#Models). There are many extensions to these linear models, check the [`CRAN Task View` on `Time Series Analysis`](https://cran.r-project.org/web/views/TimeSeries.html) to learn more (e.g. about package `zoo` and `forecast`). We will in fact be discussing a lot of methods in a book the Wiki page refers to for *'Further references on non-linear time series analysis'*: [**Nonlinear Time Series Analysis** by Kantz & Schreiber](https://www.cambridge.org/core/books/nonlinear-time-series-analysis/519783E4E8A2C3DCD4641E42765309C7). You do not need to buy the book, but it can be a helpful reference if you want to go beyond the formal level (= mathematics) used in this course. Some of the packages we use are based on the accompanying software to the book [**TiSEAN**](https://www.pks.mpg.de/~tisean/Tisean_3.0.1/index.html) which is written in `C` and `Fortran` and can be called from the command line (Windows / Linux).



### Three time series {-}

We are going to analyse 3 different time series using different techniques. The goal is to describe the dynamics, temporal patterns (if any) and perhaps other interesting characteristics of these time series.


**Importing data**

Two ways:

A. By downloading:

  1. Follow the link, e.g. for [`series.xlsx`](https://github.com/complexity-methods/CSA-assignments/blob/master/assignment_data/BasicTSA_arma/series.xlsx).
  2. On the Github page, find a button marked **Download** (or **Raw** for text files).
  3. Download the file
  4. Load it into `R` using the code below       

```{r, eval=TRUE}
library(rio)
series <- rio::import("series.xlsx")
```
     
      
B. By directly importing the file in R from Github:

   1. Copy the `url` associated with the **Download**  button [on Github](https://github.com/complexity-methods/CSA-assignments/blob/master/assignment_data/BasicTSA_arma/series.csv) (right-click).
   2. The copied path should contain the word 'raw' somewhere in the url.
   3. Call `rio::import(url)`
        
```{r,eval=FALSE}
library(rio)
series <- rio::import("https://github.com/complexity-methods/CSA-assignments/raw/master/assignment_data/BasicTSA_arma/series.xlsx")
```


### Level, Fluctuation, and Visualisation  {.tabset .tabset-fade .tabset-pills}

`series.xlsx` contains three time series as columns `TS_1`, `TS_2` and `TS_3` . 


#### Questions {-}

* As a first step look at the mean and the standard deviation  of the three series, rounded to 2 decimals (use `mean()`, `sd()`, `round()`).
    + Suppose these were time series from three different subjects in an experiment, what would you conclude based on the means and SD's? 
* Let's visualize these data. See the [appendix on plotting time series](https://complexity-methods.github.io/book/plotTS.html) for all the different options.
    + Does the visual representation of these series correspond to what you expected based on the statistical properties of these series?
    + We'll examine whether there are different levels in the data using function `ts_levels()` in *casnet*. Look at the examples in the manual entry of this function to see how to plot the level data.


#### Answers {-}

* As a first step look at the mean and the standard deviation of the three series (`summary()`, `sd()`).
  + Suppose these were time series from three different subjects in an experiment, what would you conclude based on the means and SD's?

----

```{r, collapse=TRUE}
library(casnet)

# TS1
round(mean(series$TS_1),2)
round(sd(series$TS_1),2)

# TS2
round(mean(series$TS_2),2)
round(sd(series$TS_2),2)

# TS3
round(mean(series$TS_3),2)
round(sd(series$TS_3),2)

# Similar means and SDs rounded to 2 decimal points
```

----

* Let's visualize these data. See the [chapter on plotting time series](https://complexity-methods.github.io/book/plotTS.html) for all the different options to do this.
    + Does the visual representation of these series correspond to what you expected based on the statistical properties of these series?
    + We'll examine whether there are different levels in the data using function `ts_levels()`. Look at the examples in the manual entry of this function to see how to plot the level data.
    + Time series 2 could also be considered to oscillate around a relatively stable level. Try to modify the arguments of `ts_levels()` so it will return this level.      
      
      
```{r}

lvl1 <- ts_levels(series$TS_1)
plot(ts(lvl1$pred$y))
lines(lvl1$pred$p, col="red3", lwd=3)

lvl2 <- ts_levels(series$TS_2)
plot(ts(lvl2$pred$y))
lines(lvl2$pred$p, col="red3", lwd=3)

lvl3 <- ts_levels(series$TS_3)
plot(ts(lvl3$pred$y))
lines(lvl3$pred$p, col="red3", lwd=3)

### Time series 2
### You can set 'changeSensitivity' to a stricter value.

lvl2a <- ts_levels(series$TS_2, changeSensitivity = .1)
plot(ts(lvl2a$pred$y))
lines(lvl2a$pred$p, col="red3", lwd=3)

```



### Correlation functions  {.tabset .tabset-fade .tabset-pills}

Correlation functions are intuitive tools for quantifying the temporal structure in a time series. As you know, the correlation measure can only quantify linear regularities between variables, which is why we discuss them here as `basic` tools for time series analysis. So what are the variables? In the simplest case, the variables between which we calculate a correlation are between a data point at time *t* and a data point that is separated in time by some *lag*, for example, if you would calculate the correlation in a lag-1 return plot, you would have calculated the 1st value of the correlation function (actually, it is 2nd value, the 1st value is the correlation of time series with itself, the lag-0 correlation, which is of course $r = 1$). 

You can do the analyses in SPSS, `R` or JAMOVI. This analysis is very common, so you'll find functions called `acf`, `pacf`and `ccf` in many other statistical software packages, In package `casnet` you can use the function `plotRED_acf()` (plot REDundancies).


#### Questions {-}

* Let's look at the autocorrelation function and partial autocorrelation function (use `acf()`, `pacf()`, and try `plotRED_acf()`)
  + Describe the correlational structure of these time series


#### Answers {-}

* Let's look at the autocorrelation function and partial autocorrelation function (use `acf()`, `pacf()`, and try `plotRED_acf()`)
  + Describe the correlational structure of these time series

----

```{r}
library(casnet)

acf(series$TS_1, lag.max = 50)
acf(series$TS_1, lag.max = 50, type = "partial")
plotRED_acf(y = series$TS_1,Lmax = 50, returnCorFun = FALSE)

acf(series$TS_2, lag.max = 50)
acf(series$TS_2, lag.max = 50, type = "partial")
plotRED_acf(y = series$TS_2,Lmax = 50, returnCorFun = FALSE)

acf(series$TS_3, lag.max = 50)
acf(series$TS_3, lag.max = 50, type = "partial")
plotRED_acf(y = series$TS_3,Lmax = 50, returnCorFun = FALSE)

```


## **Basic Non-linear TSA** 

Many non-linear analyses can be considered "descriptive" techniques, that is, the aim is not to fit the parameters of a model, but to describe, quantitatively, some aspects of how one value changes into another value over time.


### Intuitive notion of Fractal Dimension {-}

A qualitative description of the fractal dimension of a time series (or 1D curve) can be given by deciding whether the curve looks/behaves like a line, or, like a plane. 

As can be seen in the figure below, if slow processes (low frequencies) dominate the signal, they are more *line-like* and will have a fractal dimension closer to `1`. If fast processes (high frequencies) dominate the signal, they are more *plane-like* and will have a fractal dimension closer to `2`.


```{r fig.height=10}
library(casnet)
library(plyr)

N    <- 512
noises <- round(seq(-3,3,by=.5),1)

yy <- llply(noises, function(a){cbind(noise_powerlaw(alpha = a, N = 512, seed = 1234))})
names(yy) <- noises
tmp<- data.frame(yy,check.names = FALSE)

plotTS_multi(tmp, ylabel = "Scaling exponent alpha")

```


<!-- ## Three time series -->

<!-- We are going to analyse 3 different time series using different techniques. The goal is to describe the dynamics, temporal patterns (if any) and perhaps other interesting characteristics of these time series. -->


<!-- **Importing data** -->

<!-- Two ways: -->

<!-- A. By downloading: -->

<!--   1. Follow the link, e.g. for [`series.xlsx`](https://github.com/complexity-methods/CSA-assignments/blob/master/assignment_data/BasicTSA_arma/series.xlsx). -->
<!--   2. On the Github page, find a button marked **Download** (or **Raw** for text files). -->
<!--   3. Download the file -->
<!--   4. Load it into `R` using the code below        -->

<!-- ```{r, eval=FALSE} -->
<!-- library(rio) -->
<!-- series <- rio::import("series.xlsx") -->
<!-- ``` -->


<!-- B. By importing from Github: -->

<!--    1. Copy the `url` associated with the **Download**  button [on Github](https://github.com/complexity-methods/CSA-assignments/blob/master/assignment_data/BasicTSA_arma/series.csv) (right-click). -->
<!--    2. The copied path should contain the word 'raw' somewhere in the url. -->
<!--    3. Call `rio::import(url)` -->

<!-- ```{r} -->
<!-- library(rio) -->
<!-- series <- rio::import("https://github.com/complexity-methods/CSA-assignments/raw/master/assignment_data/BasicTSA_arma/series.xlsx") -->
<!-- ``` -->



### Relative Roughness   {.tabset .tabset-fade .tabset-pills}

Relative Roughness is calculated using the following formula:    

\begin{equation}
RR = 2\left[1 + \frac{\gamma_1(x_i)}{Var(x_i)}\right]
(\#eq:RR)
\end{equation}

The numerator in the formula stands for the `lag 1` auto-covariance of the time series $x_i$, this is the unstandardised lag1 autocorrelation. Check the function `acf()` to figure out how to get it. The denominator stands for the (global) variance of $x_i$ which all statistics packages can calculate. Another way to describe the variance is: `lag 0` auto-covariance.


#### Questions {-}

* Write down your first guesses for roughness values based on the plots of the time series (i.e., which one looks more like a line than a plane, which one looks more 'smooth' than 'rough'?).
*	Next, explore some measures of central tendency and dispersion (calculate the mean, variance standard deviation)
*	Compute the Relative Roughness for each time series.
    + You can try to figure it out for yourself by using Equation \@ref(eq:RR). Hint: Look at the manual entry for function `acf()`, argument `type`. 
    + There is also a function in `casnet` that will do everything for you: `fd_RR()`


Use Figure \@ref(fig:RRf3) to lookup which value of `RR` corresponds to which type of dynamics.

```{r RRf3, fig.cap="Coloured Noise versus Relative Roughness", fig.align='center',echo=FALSE, include=TRUE}
#knitr::include_graphics('images/RRfig3.png')
N    <- 2048
noises <- round(seq(-2.5,2.5,by=.1),1)
yy <- llply(noises, function(a){cbind(noise_powerlaw(alpha = a, N = N, seed = 1234))})
names(yy) <- noises
out <- ldply(yy, function(t) fd_RR(y = t))
out$.id <- as.numeric(out$.id)
out$FD <- sa2fd_psd(as.numeric(out$.id))
out$Label <-NA
out$Label[out$.id==-2] <-"Brownian (red) noise"
out$Label[out$.id==-1] <- "Pink noise"
out$Label[out$.id==0] <- "White noise"
out$Label[out$.id==1] <- "Blue noise"
out$Label[out$.id==2] <- "Violet (black) noise"
  
ggplot(out, aes(x=FD,y=V1)) +
         geom_line() +
         geom_point() +
         geom_point(aes(fill = Label, colour = Label), shape = 21,size=4) +
  scale_x_continuous("Fractal Dimension", breaks = seq(1,2,by=.1)) +
  scale_y_continuous("Relative Roughness", breaks = seq(0,4,by=.1)) +
  scale_color_manual("Noise",na.translate=FALSE, values = rep("black",5)) +
  scale_fill_manual("Noise",values=c("Brownian (red) noise"="red3","Pink noise"="pink","White noise"="snow","Blue noise"="skyblue","Violet (black) noise"="purple3"), na.translate = FALSE) +
  theme_bw()
```


#### Answers {-}


* Write down your first guesses for roughness values based on the plots of the time series (i.e., which one looks more like a line than a plane, which one looks more 'smooth' than 'rough'?).     
*	Next, explore some measures of central tendency and dispersion (calculate the mean, variance standard deviation)
*	Compute the Relative Roughness for each time series.    
   - You can try to figure it out for yourself by using Equation \@ref(eq:RR). Hint: Look at the manual entry for function `acf()`, argument `type`. 
   - There is also a function in `casnet` that will do everything for you: `fd_RR()`

----

To see how to calculate RR 'by hand' you could look at the code of the function `fd_RR()`
If you select the function name in R and press F2, you should be able to see the code, or you can just type `fd_RR` (without parentheses)

```{r}
fd_RR
```

----

```{r}
# TS1
fd_RR(series$TS_1)

# TS2
fd_RR(series$TS_2)

# TS3
fd_RR(series$TS_3)
```


### Sample Entropy  {.tabset .tabset-fade .tabset-pills}

Use the `sample_entropy()` function in package `pracma`.

#### Questions {-}

* Calculate the Sample Entropy of the two sets of three time series you now have.
    + Use a segment length `edim` of 3 data points, and a tolerance range `r` equal to the `sd()` of the series (`1*sd(ts)`).
    + Can you change the absolute SampEn outcomes by 'playing' with the `edim` parameter? If so, how does the outcome change, and why?
    + Do changes in the `SampEn` value due to different parameter settings change the relative order of the entropy for the three time series?


Use Figure \@ref(fig:sampEnt) to lookup which value of `SampEn` corresponds to which type of dynamics.

```{r sampEnt, echo=FALSE, fig.align='center', fig.cap="Coloured Noise versus Sample Entropy", message=FALSE, warning=FALSE}
#knitr::include_graphics('images/RRfig3.png')
N    <- 1024
noises <- round(seq(-2.5,2.5,by=.1),1)
yy <- llply(noises, function(a){cbind(noise_powerlaw(alpha = a, N = N, seed = 1234))})
names(yy) <- noises
out <- ldply(yy, function(t) pracma::sample_entropy(ts = t, edim = 3, r = sd(t)))
out$.id <- as.numeric(out$.id)
out$FD <- sa2fd_psd(as.numeric(out$.id))
out$Label <-NA
out$Label[out$.id==-2] <-"Brownian (red) noise"
out$Label[out$.id==-1] <- "Pink noise"
out$Label[out$.id==0] <- "White noise"
out$Label[out$.id==1] <- "Blue noise"
out$Label[out$.id==2] <- "Violet (black) noise"
  
ggplot(out, aes(x=FD,y=V1)) +
         geom_line() +
         geom_point() +
         geom_point(aes(fill = Label, colour = Label), shape = 21,size=4) +
  scale_x_continuous("Fractal Dimension", breaks = seq(1,2,by=.1)) +
  scale_y_continuous("Sample entropy (edim = 3, r = SD)", breaks = seq(0,4,by=.1)) +
  scale_color_manual("Noise",na.translate=FALSE, values = rep("black",5)) +
  scale_fill_manual("Noise",values=c("Brownian (red) noise"="red3","Pink noise"="pink","White noise"="snow","Blue noise"="skyblue","Violet (black) noise"="purple3"), na.translate = FALSE) +
  theme_bw()
```



#### Answers {-}

* Calculate the Sample Entropy of the two sets of three time series you now have.
    + Use a segment length `edim` of 3 data points, and a tolerance range `r` equal to the `sd()` of the series (`1*sd(ts)`).
    + Can you change the absolute SampEn outcomes by 'playing' with the `edim` parameter? If so, how does the outcome change, and why?
    + Do changes in the `SampEn` value due to different parameter settings change the relative order of the entropy for the three time series?

```{r,collapse=TRUE}
library(pracma)

sample_entropy(series$TS_1, edim = 3, r = sd(series$TS_1))
sample_entropy(series$TS_2, edim = 3, r = sd(series$TS_2))
sample_entropy(series$TS_3, edim = 3, r = sd(series$TS_3))

# Change edim parameters.
sample_entropy(series$TS_1, edim = 6, r = sd(series$TS_1))
sample_entropy(series$TS_2, edim = 6, r = sd(series$TS_2))
sample_entropy(series$TS_3, edim = 6, r = sd(series$TS_3))


# Change r parameters.
# Has effect on relative order.
sample_entropy(series$TS_1, edim = 3, r = .2*sd(series$TS_1))
sample_entropy(series$TS_2, edim = 3, r = .2*sd(series$TS_2))
sample_entropy(series$TS_3, edim = 3, r = .2*sd(series$TS_3))

```

The change of `edim` keeps the relative order, change of `r` for the same `edim` does not.



### The return plot {.tabset .tabset-fade .tabset-pills}

On Day 1 we looked at return plots of simulated time series, we discussed what kind of shapes to expect based on the kind of change processes generated the data.


#### Questions {-}

* Based on the:
  + *correlation functions*, what kind of shape or pattern do you expect to see in a lag-1 return plot of each series?
  + *RR*, what kind of shape or pattern do you expect to see in a lag-1 return plot of each series?
  + *SampEn*, what kind of shape or pattern do you expect to see in a lag-1 return plot of each series?
* Create the return plots!



#### Answers{-}

* Based on the:
  + *correlation functions*, what kind of shape or pattern do you expect to see in a lag-1 return plot of each series?
  + *RR*, what kind of shape or pattern do you expect to see in a lag-1 return plot of each series?
  + *SampEn*, what kind of shape or pattern do you expect to see in a lag-1 return plot of each series?
* Create the return plots!

```{r}
library(dplyr)

plot(dplyr::lag(series$TS_1), series$TS_1, pch = ".")

plot(dplyr::lag(series$TS_2), series$TS_2, pch = ".")

plot(dplyr::lag(series$TS_3), series$TS_3, pch = ".")

```

<!-- Use Figure \@ref(fig:RRf3) to lookup which value of $RR$ corresponds to which type of dynamics: -->

<!-- **TS1**: Pink noise -->
<!-- **TS2**: Brownian noise -->
<!-- **TS3**: White noise -->



<!-- **Randomize** -->

<!-- To randomize the data you may use the function `sample` (which is easier than `randperm`) -->

<!-- ```{r} -->
<!-- library(pracma) -->
<!-- # randperm() -->
<!-- TS1Random <- TS1$TS1[randperm(length(TS1$TS1))] -->

<!-- # sample() -->
<!-- TS1Random <- sample(TS1$TS1, length(TS1$TS1)) -->
<!-- TS2Random <- sample(TS2$TS2, length(TS2$TS2)) -->
<!-- TS3Random <- sample(TS3$TS3, length(TS3$TS3)) -->


<!-- plot.ts(TS1Random) -->
<!-- lines(ts(TS1$TS1),col="red3") -->
<!-- ``` -->

<!-- If you repeat this for TS2 and TS3 and compute the Relative Roughness of each randomized time series, the outcomes should be around 2, white noise! This makes sense, you destroyed all the correlations in the data by removing the temporal order with which values were observed. -->

<!-- ```{r} -->
<!-- library(casnet) -->
<!-- cat("TS1random\n") -->
<!-- cat(fd_RR(TS1Random)) -->
<!-- cat("\nTS2random\n") -->
<!-- cat(fd_RR(TS2Random)) -->
<!-- cat("\nTS3random\n") -->
<!-- cat(fd_RR(TS3Random)) -->
<!-- ``` -->

<!-- **Integrate** -->

<!-- Normalize the white noise time series -->
<!-- ```{r} -->
<!-- TS3Norm <- scale(TS3$TS3) -->
<!-- ``` -->

<!-- Now integrate it, which just means, 'take the cumulative sum'. -->
<!-- ```{r} -->
<!-- TS3Int <- cumsum(TS3Norm) -->
<!-- plot.ts(TS3Int) -->
<!-- lines(ts(TS3Norm),col="red3") -->
<!-- ``` -->

<!-- If you compute the Relative Roughness of the integrated time series, the outcome should be close to 0, Brownian noise. -->
<!-- ```{r} -->
<!-- cat("\nTS3Int\n") -->
<!-- cat(RR(TS3Int)) -->
<!-- ``` -->


<!-- ### Sample Entropy  {.tabset .tabset-fade .tabset-pills} -->

<!-- Use the `sample_entropy()` function in package `pracma`. -->

<!-- #### Questions {-} -->

<!-- * Calculate the Sample Entropy of the two sets of three time series you now have. -->
<!--     + Use your favourite function to estimate the sample entropy of the three time series. Use for instance a segment length `edim` of 3 data points, and a tolerance range `r` of 1 * the standard deviation of the series. What values do you observe? -->
<!--     + Can you change the absolute SampEn outcomes by 'playing' with the m parameter? If so, how does the outcome change, and why? -->
<!--     + Can you change the absolute SampEn outcomes by 'playing' with the r parameter If so, how does the outcome change, and why? -->
<!--     + Do changes in the relative SampEn outcome change the outcomes for the three time series relative to each other? -->

<!-- *	Extra: Go back to the assignment where you generated simulated time series from the logistic map. -->


<!-- #### Answers {-} -->

<!-- Change some of the parameters. -->

<!-- ```{r,collapse=TRUE} -->
<!-- library(rio) -->
<!-- library(pracma) -->

<!-- # ACF assignment data `series` -->
<!-- cat(paste0("\nseries.TS1 m=3\n",sample_entropy(series$TS_1, edim = 3, r = sd(series$TS_1)))) -->
<!-- cat(paste0("\nseries.TS2 m=3\n",sample_entropy(series$TS_2, edim = 3, r = sd(series$TS_2)))) -->
<!-- cat(paste0("\nseries.TS3 m=3\n",sample_entropy(series$TS_3, edim = 3, r = sd(series$TS_3)))) -->


<!-- # ACF assignment data `series` -->
<!-- cat(paste0("\nseries.TS1 m=6\n",sample_entropy(series$TS_1, edim = 6, r = sd(series$TS_1)))) -->
<!-- cat(paste0("\nseries.TS2 m=6\n",sample_entropy(series$TS_2, edim = 6, r = sd(series$TS_2)))) -->
<!-- cat(paste0("\nseries.TS3 m=6\n",sample_entropy(series$TS_3, edim = 6, r = sd(series$TS_3)))) -->


<!-- # ACF assignment data `series` -->
<!-- cat(paste0("\nseries.TS1 m=6, r=.5\n",sample_entropy(series$TS_1, edim = 3, r = .5*sd(series$TS_1)))) -->
<!-- cat(paste0("\nseries.TS2 m=6, r=.5\n",sample_entropy(series$TS_2, edim = 3, r = .5*sd(series$TS_2)))) -->
<!-- cat(paste0("\nseries.TS3 m=6, r=.5\n",sample_entropy(series$TS_3, edim = 3, r = .5*sd(series$TS_3)))) -->

<!-- ``` -->

<!-- The change of `m` keeps the relative order, change of `r` for the same `m` does not. -->

<!-- **Values of other time series** -->

<!-- ```{r message=FALSE, warning=FALSE, collapse=TRUE} -->

<!-- # RR assignment data `TS1,TS2,TS3` -->
<!-- cat(paste0("\nTS1\n",sample_entropy(TS1$TS1, edim = 3, r = sd(TS1$TS1)))) -->
<!-- cat(paste0("TS1Random\n",sample_entropy(TS1Random, edim = 3, r = sd(TS1Random)))) -->

<!-- cat(paste0("\nTS2\n",sample_entropy(TS2$TS2, edim = 3, r = sd(TS2$TS2)))) -->
<!-- cat(paste0("TS2Random\n",sample_entropy(TS2Random, edim = 3, r = sd(TS2Random)))) -->

<!-- cat(paste0("\nTS3\n",sample_entropy(TS3$TS3, edim = 3, r = sd(TS3$TS3)))) -->
<!-- cat(paste0("TS3Int\n",sample_entropy(TS3Norm, edim = 3, r = sd(TS3Norm)))) -->


<!-- # Logistic map -->
<!-- library(casnet) -->
<!-- cat("\nLogistic map\nr=2.9\n") -->
<!-- y1<-growth_ac(r = 2.9,type="logistic") -->
<!-- sample_entropy(y1, edim = 3, r = sd(y1)) -->
<!-- cat("\nLogistic map\nr=4\n") -->
<!-- y2<-growth_ac(r = 4,type="logistic") -->
<!-- sample_entropy(y2, edim = 3, r = sd(y2)) -->


<!-- ``` -->



<!--chapter:end:ASSIGNMENTS_D2_basicTSA.Rmd-->

---
title: "Fluctuation and Dispersion analyses"
author: "Fred Hasselman & Maarten Wijnants"
date: "1/14/2018"
output: 
  bookdown::html_document2: 
    variant: markdown+hard_line_breaks
    fig_caption: yes
    highlight: pygments
    keep_md: yes
    self_contained: yes
    number_sections: yes
    theme: spacelab
    toc: yes
    toc_float: true
    collapsed: false
    smooth_scroll: true
    code_folding: show
    bibliography: [refs.bib, packages.bib]
    biblio-style: apalike
    csl: apa.csl
    includes:
        before_body: assignmentstyle.html
    pandoc_args: ["--number-offset","2"]
editor_options: 
  chunk_output_type: console

---


# **Fluctuation and Disperion analyses** {-}

    
     
     
**Importing data**

Two ways:

A. By downloading:

  1. Follow the link, e.g. for [`noises.xlsx`](https://github.com/complexity-methods/CSA-assignments/blob/master/assignment_data/BasicTSA_arma/noises.xlsx).
  2. On the Github page, find a button marked **Download** (or **Raw** for textfiles).
  3. Download the file
  4. Load it into `R` using the code below       

```{r, eval=FALSE}
library(rio)
noises <- rio::import("noises.xlsx")
```
     
      
B. By importing from Github:

   1. Copy the `url` associated with the **Download**  button [on Github](https://github.com/complexity-methods/CSA-assignments/blob/master/assignment_data/BasicTSA_arma/noises.xlsx) (right-clik).
   2. The copied path should contain the word 'raw' somewhere in the url.
   3. Call `rio::import(url)`     
   
        
```{r,eval=TRUE}
library(rio)
noises <- rio::import("https://github.com/complexity-methods/CSA-assignments/raw/master/assignment_data/BasicTSA_arma/noises.xlsx")
```
       
        
        
      
###  Standardised Dispersion Analysis {.tabset .tabset-fade .tabset-pills}


#### Questions {-}

* Common data preparation before running fluctuation analyses like `SDA`:
    + Normalize the time series (using `sd` based on `N`, not `N-1`, e.g. by using `ts_standardise()`, or use function arguments)
    + Check whether time series length is a power of `2`. If you use `N <- log2(length(y))`, the number you get is `2^N`. You need an integer power for the length in order to create equal bin sizes. You can pad the series with 0s, or make it shorter before analysis.
* Perform `sda` on the 3 time series, or any of the other series you already analysed.
* Compare to what you find for `fd_sda` to the other techniques (`fd_RR`,`SampEn`, `acf`, `pacf`)


#### Answers {-}


```{r collapse=TRUE, message=FALSE, warning=FALSE}
library(casnet)

fdS1 <- fd_sda(noises$TS_1, returnPLAW = TRUE)
fdS2 <- fd_sda(noises$TS_2, returnPLAW = TRUE)
fdS3 <- fd_sda(noises$TS_3, returnPLAW = TRUE)

# If you asked to return the powerlaw (returnPLAW = TRUE) the function plotFD_loglog() can make a plot
plotFD_loglog(fdS1,title = "SDA", subtitle = "Series 1")
plotFD_loglog(fdS2,title = "SDA", subtitle = "Series 2")
plotFD_loglog(fdS3,title = "SDA", subtitle = "Series 3")
```



<!-- **Values of other time series** -->

<!-- ```{r message=FALSE, warning=FALSE, collapse=TRUE, cache=TRUE} -->

<!-- # RR assignment data TS1, TS2, TS3 and shuffled versions -->
<!-- # This is just a check so you can see which bin sizes to expect -->
<!-- # It is also possible to force calclulation of specific bin sizes, see the manual pages for more info ?fd_sda -->
<!-- L1 <- floor(log2(length(TS1$TS1))) -->
<!-- L2 <- floor(log2(length(TS2$TS2))) -->
<!-- L3 <- floor(log2(length(TS3$TS3))) -->

<!-- # You can also ask for a plot directly by passing doPlot = TRUE -->
<!-- fdTS1  <- fd_sda(TS1$TS1[1:2^L1], doPlot = TRUE, tsName = "TS1") -->

<!-- fdTS1r <- fd_sda(TS1Random[1:2^L1], doPlot = TRUE, tsName = "TS1 shuffled") -->


<!-- fdTS2 <- fd_sda(TS2$TS2[1:2^L2], returnPLAW = TRUE) -->
<!-- plotFD_loglog(fdTS2,subtitle = "TS2") -->

<!-- fdTS2r <- fd_sda(TS2Random[1:2^L2], returnPLAW = TRUE) -->
<!-- plotFD_loglog(fdTS2r,subtitle = "TS2 shuffled") -->

<!-- fdTS3 <- fd_sda(TS3$TS3[1:2^L3], returnPLAW = TRUE) -->
<!-- plotFD_loglog(fdTS3,subtitle = "TS3") -->

<!-- fdTS3r <- fd_sda(TS3Random[1:2^L3], returnPLAW = TRUE) -->
<!-- plotFD_loglog(fdTS3r,subtitle = "TS3 shuffled") -->

<!-- fdTS3n <- fd_sda(TS3Norm[1:2^L3], returnPLAW = TRUE) -->
<!-- plotFD_loglog(fdTS3n,subtitle = "TS3 standardised") -->


<!-- # Logistic map -->
<!-- y1 <- growth_ac(r = 2.9, type="logistic",N = 1024) -->
<!-- fdL29 <- fd_sda(y1, returnPLAW = TRUE) -->
<!-- plotFD_loglog(fdL29, subtitle = "Logistic Map: r =  2.9") -->

<!-- y2 <- growth_ac(r = 4, type="logistic", N = 1024) -->
<!-- fdL4 <- fd_sda(y2, returnPLAW = TRUE) -->
<!-- plotFD_loglog(fdL4, subtitle = "Logistic Map: r =  4") -->

<!-- ``` -->


###  Spectral Slope {.tabset .tabset-fade .tabset-pills}


#### Questions {-}

* Common data preparation before running spectral analyses:
    + Normalize the time series (using `sd` based on `N`, not `N-1`)
    + Check whether time series length is a power of `2`. If you use `N <- log2(length(y))`, the number you get is `2^N`. You need an integer power for the length in order to create equal bin sizes. You can pad the series with 0s, or make it shorter before analysis.
* Perform `fd_psd()` on the 3 time series, or any of the other series you already analysed.
* Compare to what you find for `fd_psd` to the other measures (`fd_RR`,`SampEn`, `acf`, `pacf`)


#### Answers {-}


```{r collapse=TRUE,cache=TRUE, message=FALSE, warning=FALSE}

fdS1 <- fd_psd(noises$TS_1, doPlot = TRUE)
fdS2 <- fd_psd(noises$TS_2, returnPLAW = TRUE)
plotFD_loglog(fdS2,subtitle = "Series 2")
fdS3 <- fd_psd(noises$TS_3, returnPLAW = TRUE)
plotFD_loglog(fdS3,subtitle = "Series 3")

```



<!-- **Values of other time series** -->

<!-- ```{r message=FALSE, warning=FALSE, collapse=TRUE, cache=TRUE} -->

<!-- # This is just a check so you can see which frequency range to expect -->
<!-- # It is also possible to force calclulation of specific bin sizes, see the manual pages for more info ?fd_psd -->
<!-- L1 <- floor(log2(length(TS1$TS1))) -->
<!-- L2 <- floor(log2(length(TS2$TS2))) -->
<!-- L3 <- floor(log2(length(TS3$TS3))) -->

<!-- # RR assignment data TS1, TS2, TS3 and shuffled versions -->

<!-- fdTS1 <- fd_psd(TS1$TS1[1:2^L1], doPlot = TRUE, tsName = "TS1") -->

<!-- fdTS1r <- fd_psd(TS1Random[1:2^L1], returnPLAW = TRUE) -->
<!-- plotFD_loglog(fdTS1r,subtitle = "TS1 shuffled") -->

<!-- fdTS2 <- fd_psd(TS2$TS2[1:2^L2], returnPLAW = TRUE) -->
<!-- plotFD_loglog(fdTS2,subtitle = "TS2") -->

<!-- fdTS2r <- fd_psd(TS2Random[1:2^L2], returnPLAW = TRUE) -->
<!-- plotFD_loglog(fdTS2r,subtitle = "TS2 shuffled") -->

<!-- fdTS3 <- fd_psd(TS3$TS3[1:2^L3], returnPLAW = TRUE) -->
<!-- plotFD_loglog(fdTS3,subtitle = "TS3") -->

<!-- fdTS3r <- fd_psd(TS3Random[1:2^L3], returnPLAW = TRUE) -->
<!-- plotFD_loglog(fdTS3r,subtitle = "TS3 shuffled") -->

<!-- fdTS3n <- fd_psd(TS3Norm[1:2^L3], returnPLAW = TRUE) -->
<!-- plotFD_loglog(fdTS3n,subtitle = "TS3 standardised") -->


<!-- # Logistic map -->
<!-- y1 <- growth_ac(r = 2.9, type="logistic",N = 1024) -->
<!-- fdL29 <- fd_psd(as.numeric(y1), returnPLAW = TRUE) -->
<!-- plotFD_loglog(fdL29, subtitle = "Logistic Map: r =  2.9") -->

<!-- y2 <- growth_ac(r = 4, type="logistic", N = 1024) -->
<!-- fdL4 <- fd_psd(as.numeric(y2), returnPLAW = TRUE) -->
<!-- plotFD_loglog(fdL4, subtitle = "Logistic Map: r =  4") -->

<!-- ``` -->


###  Detrended Fluctuation Analysis {.tabset .tabset-fade .tabset-pills}


#### Questions {-}

* Data preparation before running `DFA` analyses are not really necessary, except perhaps:
    + Check whether time series length is a power of `2`. If you use `N <- log2(length(y))`, the number you get is `2^N`. You need an integer power for the length in order to create equal bin sizes. You can pad the series with 0s, or make it shorter before analysis.
* Perform `fd_dfa()` on the 3 time series, or any of the other series you already analysed.
* Compare to what you find for `fd_dfa` to the other measures (`fd_RR`,`SampEn`, `acf`, `pacf`)


#### Answers {-}


```{r collapse=TRUE,cache=TRUE, message=FALSE, warning=FALSE}

fdS1 <- fd_dfa(noises$TS_1, doPlot = TRUE)
fdS2 <- fd_dfa(noises$TS_2, returnPLAW = TRUE)
plotFD_loglog(fdS2,subtitle = "Series 2")
fdS3 <- fd_dfa(noises$TS_3, returnPLAW = TRUE)
plotFD_loglog(fdS3,subtitle = "Series 3")

```


<!-- **Values of other time series** -->

<!-- ```{r message=FALSE, warning=FALSE, collapse=TRUE,cache=TRUE} -->

<!-- # This is just a check so you can see which bin sizes to expect -->
<!-- # It is also possible to force calclulation of specific bin sizes, see the manual pages for more info ?fd_dfa -->
<!-- L1 <- floor(log2(length(TS1$TS1))) -->
<!-- L2 <- floor(log2(length(TS2$TS2))) -->
<!-- L3 <- floor(log2(length(TS3$TS3))) -->

<!-- # RR assignment data TS1, TS2, TS3 and shuffled versions -->

<!-- fdTS1 <- fd_dfa(TS1$TS1[1:2^L1], doPlot = TRUE, tsName = "TS1") -->

<!-- fdTS1r <- fd_dfa(TS1Random[1:2^L1], returnPLAW = TRUE) -->
<!-- plotFD_loglog(fdTS1r,subtitle = "TS1 shuffled") -->

<!-- fdTS2 <- fd_dfa(TS2$TS2[1:2^L2], returnPLAW = TRUE) -->
<!-- plotFD_loglog(fdTS2,subtitle = "TS2") -->

<!-- fdTS2r <- fd_dfa(TS2Random[1:2^L2], returnPLAW = TRUE) -->
<!-- plotFD_loglog(fdTS2r,subtitle = "TS2 shuffled") -->

<!-- fdTS3 <- fd_dfa(TS3$TS3[1:2^L3], returnPLAW = TRUE) -->
<!-- plotFD_loglog(fdTS3,subtitle = "TS3") -->

<!-- fdTS3r <- fd_dfa(TS3Random[1:2^L3], returnPLAW = TRUE) -->
<!-- plotFD_loglog(fdTS3r,subtitle = "TS3 shuffled") -->

<!-- fdTS3n <- fd_dfa(TS3Norm[1:2^L3], returnPLAW = TRUE) -->
<!-- plotFD_loglog(fdTS3n,subtitle = "TS3 standardised") -->


<!-- # Logistic map -->
<!-- y1 <- growth_ac(r = 2.9, type="logistic",N = 1024) -->
<!-- fdL29 <- fd_dfa(y1, returnPLAW = TRUE) -->
<!-- plotFD_loglog(fdL29, subtitle = "Logistic Map: r =  2.9") -->

<!-- y2 <- growth_ac(r = 4, type="logistic", N = 1024) -->
<!-- fdL4 <- fd_dfa(y2, returnPLAW = TRUE) -->
<!-- plotFD_loglog(fdL4, subtitle = "Logistic Map: r =  4") -->

<!-- ``` -->


## **Surrogate testing: Beyond the straw-man null-hypothesis** 

Look at the explanation of surrogate analysis on the [TiSEAN website](https://www.pks.mpg.de/~tisean/Tisean_3.0.1/)
(Here is [direct link](https://www.pks.mpg.de/~tisean/Tisean_3.0.1/docs/surropaper/node5.html) to the relevant sections)


### Constrained realisations of complex signals {.tabset .tabset-fade .tabset-pills}

We'll look mostly at three different kinds of surrogates:

* *Randomly shuffled*: $H_0:$ The time series data are independent random numbers drawn from some probability distribution.
* *Phase Randomised*: $H_0:$ The time series data were generated by a stationary linear stochastic process
* *Amplitude Adjusted ,Phase Randomised*: $H_0:$ The time series data were generated by a rescaled linear stochastic process.


#### Questions {-}

* Figure out how many surrogates you minimally need for a 2-sided test.
* Package `nonlinearTseries` has a function `FFTsurrogate` and package `fractal` `surrogate`. Look them up and try to create a surrogate test for some of the time series of the previous assignments.
    + If you use `fractal::surrogate()`, choose either `aaft` (amplitude adjusted Fourier transform) or `phase` (phase randomisation)
    + `nonlinearTseries::FFTsurrogates` will calculate phase randomised surrogates.
* In package `casnet` there's a function that will calculate and display a point-probability based on a distribution of surrogate measures and one observed measure: `plotSUR_hist`



#### Answers {-}


Create a number of surrogates, calulate the slopes and plot the results.

* First let's get the values we found earlier (we'll use the FD from `sda`, then we do the same for `psd`,but you can take any of the measures as well)


```{r message=FALSE, warning=FALSE, cache=TRUE}
library(fractal)

# RR assignment data `TS1,TS2,TS3`
# Now we'll trim the time noises length to a power of 2 to get the cleanest results, this is not strictly necessary 
L1 <- floor(log2(length(noises$TS_1)))
L2 <- floor(log2(length(noises$TS_2)))
L3 <- floor(log2(length(noises$TS_3)))

TS1_FDsda <- fd_sda(noises$TS_1[1:2^L1])$fitRange$FD
TS2_FDsda <- fd_sda(noises$TS_2[1:2^L2])$fitRange$FD
TS3_FDsda <- fd_sda(noises$TS_3[1:2^L3])$fitRange$FD

# y1<-growth_ac(r = 2.9,type="logistic",N = 2^L1)
# TSlogMapr29 <- fd_sda(y1)$fitRange$FD
# 
# y2<-growth_ac(r = 4,type="logistic", N = 2^L1)
# TSlogMapr4 <- fd_sda(y2)$fitRange$FD
# 
# # These were rendomised
# TS1R_FDsda <- fd_sda(TS1Random[1:2^L1])$fitRange$FD
# TS2R_FDsda <- fd_sda(TS1Random[1:2^L2])$fitRange$FD
# TS3R_FDsda <- fd_sda(TS1Random[1:2^L3])$fitRange$FD
```


* To create the surrogates using `fractal::surrogate()`
    + You'll need to repeatedly call the function and store the result
    + To get into a dataframe we have to use `unclass()` otherwise `R` doesn't recognise the object
    + Function `t()` trnasposes the data from 39 rows to 39 columns
* Once you have the surrogates in a datafram, repeatedly calculate the measure you want to compare, in this case `fd_sda()`.


```{r echo=TRUE, fig.height=12, message=FALSE, warning=FALSE, cache=TRUE, collapse=TRUE}
# NOW CREATE SURROGATES
library(plyr)
library(tidyverse)
library(fractal)

# For a two-sided test at alpha = .05 we need N=39
Nsurrogates <- 39

TS1surrogates <- as.data.frame(t(ldply(1:Nsurrogates, function(s) unclass(surrogate(x=noises$TS_1[1:2^L1] ,method="aaft")))))
colnames(TS1surrogates) <- paste0("S",1:NCOL(TS1surrogates))

# Add the observed data
TS1surrogates$Obs <- noises$TS_1[1:2^L1]
plotTS_multi(TS1surrogates)
````


```{r}
library(fractal)
# Now we calculate FD for each series
TS1surrogates_FD <- plyr::laply(1:Nsurrogates,function(s) fd_sda(y=TS1surrogates[,s], silent = TRUE)$fitRange$FD)

TS2surrogates <- t(plyr::ldply(1:Nsurrogates, function(s) unclass(surrogate(x=noises$TS_2[1:2^L2] ,method="aaft"))))
colnames(TS2surrogates) <- paste0("aaftSurr",1:NCOL(TS2surrogates))

TS2surrogates_FD <- plyr::laply(1:Nsurrogates,function(s) fd_sda(y=TS2surrogates[,s], silent = TRUE)$fitRange$FD)


TS3surrogates <- t(plyr::ldply(1:Nsurrogates, function(s) unclass(surrogate(x=noises$TS_3[1:2^L3] ,method="aaft"))))
colnames(TS3surrogates) <- paste0("aaftSurr",1:NCOL(TS3surrogates))

TS3surrogates_FD <- plyr::laply(1:Nsurrogates,function(s) fd_sda(y=TS3surrogates[,s], silent = TRUE)$fitRange$FD)

# 
# TSr29surrogates <- t(ldply(1:Nsurrogates, function(s) unclass(surrogate(x=y1, method="aaft"))))
# colnames(TSr29surrogates) <- paste0("aaftSurr",1:NCOL(TSr29surrogates))
# 
# TSr29surrogates_FD <- laply(1:Nsurrogates,function(s) fd_sda(y=TSr29surrogates[,s], silent = TRUE)$fitRange$FD)
# 
# 
# TSr4surrogates <- t(ldply(1:Nsurrogates, function(s) unclass(surrogate(x=y2, method="aaft"))))
# colnames(TSr4surrogates) <- paste0("aaftSurr",1:NCOL(TSr4surrogates))
# 
# TSr4surrogates_FD <- laply(1:Nsurrogates,function(s) fd_sda(y=TSr4surrogates[,s], silent = TRUE)$fitRange$FD)
```

* Collect the results in a dataframe, so we can compare.

```{r echo=TRUE, message=FALSE, warning=FALSE}
x <- data.frame(Source = c("TS1", "TS2", "TS3"), # "TSlogMapr29", "TSlogMapr4"), 
                FDsda = c(TS1_FDsda, TS2_FDsda, TS3_FDsda), # TSlogMapr29, TSlogMapr4), 
                #FDsdaRandom =  c(TS1R_FDsda, TS2R_FDsda, TS3R_FDsda,NA,NA), 
                FDsdaAAFT.median= c(median(TS1surrogates_FD),median(TS2surrogates_FD),median(TS3surrogates_FD))) #,median(TSr29surrogates_FD),median(TSr4surrogates_FD)))

```


* Call function `plotSUR_hist()` to see the results of the test.

```{r echo=TRUE, message=FALSE, warning=FALSE, cache=TRUE}
plotSUR_hist(surrogateValues = TS1surrogates_FD, observedValue = TS1_FDsda, sides = "two.sided", doPlot = TRUE, measureName = "sda FD TS1")

plotSUR_hist(surrogateValues = TS2surrogates_FD, observedValue = TS2_FDsda, sides = "two.sided", doPlot = TRUE, measureName = "sda FD TS2")

plotSUR_hist(surrogateValues = TS3surrogates_FD, observedValue = TS3_FDsda,sides = "two.sided", doPlot = TRUE, measureName = "sda FD TS3")
# 
# plotSUR_hist(surrogateValues = TSr29surrogates_FD, observedValue = TSlogMapr29,sides = "two.sided", doPlot = TRUE, measureName = "sda FD logMap r=2.9")
# 
# plotSUR_hist(surrogateValues = TSr4surrogates_FD, observedValue = TSlogMapr4, sides = "two.sided", doPlot = TRUE, measureName = "sda FD logMap r=4")
```

**FD: SPECTRAL SLOPE**

```{r echo=TRUE, message=FALSE, warning=FALSE, cache=TRUE}

# RR assignment data `TS1,TS2,TS3`
L1 <- floor(log2(length(noises$TS_1)))
L2 <- floor(log2(length(noises$TS_2)))
L3 <- floor(log2(length(noises$TS_2)))

TS1_FDpsd <- fd_psd(noises$TS_1[1:2^L1])$fitRange$FD
TS2_FDpsd <- fd_psd(noises$TS_2[1:2^L2])$fitRange$FD
TS3_FDpsd <- fd_psd(noises$TS_3[1:2^L3])$fitRange$FD

# # Logistic map - make it the length of L1
# y1<-growth_ac(r = 2.9,type="logistic",N = 2^L1)
# # Turn standardisation and detrending off!
# TSlogMapr29 <- fd_psd(y1, standardise = FALSE, detrend = FALSE)$fitRange$FD
# y2<-growth_ac(r = 4,type="logistic", N = 2^L1)
# TSlogMapr4 <- fd_psd(y2, standardise = FALSE, detrend = FALSE)$fitRange$FD
# 
# TS1R_FDpsd <- fd_psd(TS1Random[1:2^L1])$fitRange$FD
# TS2R_FDpsd <- fd_psd(TS1Random[1:2^L2])$fitRange$FD
# TS3R_FDpsd <- fd_psd(TS1Random[1:2^L3])$fitRange$FD

```


* To create the surrogates using `fractal::surrogate()`
    + You'll need to repeatedly call the function and store the result
    + To get into a dataframe we have to use `unclass()` otherwise `R` doesn't recognise the object
    + Function `t()` trnasposes the data from 39 rows to 39 columns
* Once you have the surrogates in a dataframe, repeatedly calculate the measure you want to compare, in this case `fd_sda()`.

```{r echo=TRUE, message=FALSE, warning=FALSE, cache=TRUE}
# NOW CREATE SURROGATES
library(dplyr)

# For a two-sided test at alpha = .05 we need N=39
Nsurrogates <- 39

TS1surrogates <- t(ldply(1:Nsurrogates, function(s) unclass(fractal::surrogate(x=noises$TS_1[1:2^L1] ,method="aaft"))))
colnames(TS1surrogates) <- paste0("aaftSurr",1:NCOL(TS1surrogates))

TS1surrogates_FD <- laply(1:Nsurrogates,function(s){fd_psd(y=TS1surrogates[,s], silent = TRUE)$fitRange$FD})


TS2surrogates <- t(ldply(1:Nsurrogates, function(s) unclass(surrogate(x=noises$TS_2[1:2^L2] ,method="aaft"))))
colnames(TS2surrogates) <- paste0("aaftSurr",1:NCOL(TS2surrogates))

TS2surrogates_FD <- laply(1:Nsurrogates,function(s) fd_psd(y=TS2surrogates[,s], silent = TRUE)$fitRange$FD)


TS3surrogates <- t(ldply(1:Nsurrogates, function(s) unclass(surrogate(x=noises$TS_3[1:2^L3] ,method="aaft"))))
colnames(TS3surrogates) <- paste0("aaftSurr",1:NCOL(TS3surrogates))

TS3surrogates_FD <- laply(1:Nsurrogates,function(s) fd_psd(y=TS3surrogates[,s], silent = TRUE)$fitRange$FD)


# TSr29surrogates <- t(ldply(1:Nsurrogates, function(s) unclass(surrogate(x=y1, method="aaft"))))
# colnames(TSr29surrogates) <- paste0("aaftSurr",1:NCOL(TSr29surrogates))
# 
# TSr29surrogates_FD <- laply(1:Nsurrogates,function(s) fd_psd(y=TSr29surrogates[,s], silent = TRUE)$fitRange$FD)
# 
# 
# TSr4surrogates <- t(ldply(1:Nsurrogates, function(s) unclass(surrogate(x=y2, method="aaft"))))
# colnames(TSr4surrogates) <- paste0("aaftSurr",1:NCOL(TSr4surrogates))
# 
# TSr4surrogates_FD <- laply(1:Nsurrogates,function(s) fd_psd(y=TSr4surrogates[,s], silent = TRUE)$fitRange$FD)


```


* Call function `plotSUR_hist()` to get the results of the test.

```{r message=FALSE, warning=FALSE,cache=TRUE}
plotSUR_hist(surrogateValues = TS1surrogates_FD, observedValue = TS1_FDpsd, sides = "two.sided", doPlot = TRUE, measureName = "psd FD TS1")

plotSUR_hist(surrogateValues = TS2surrogates_FD, observedValue = TS2_FDpsd,sides = "two.sided", doPlot = TRUE, measureName = "psd FD TS2")

plotSUR_hist(surrogateValues = TS3surrogates_FD, observedValue = TS3_FDpsd,sides = "two.sided", doPlot = TRUE, measureName = "psd FD TS3")

# plotSUR_hist(surrogateValues = TSr29surrogates_FD, observedValue = TSlogMapr29,sides = "two.sided", doPlot = TRUE, measureName = "psd FD logMap r=2.9")
# 
# plotSUR_hist(surrogateValues = TSr4surrogates_FD, observedValue = TSlogMapr4, sides = "two.sided", doPlot = TRUE, measureName = "psd FD logMap r=4")

```


 * Let's print the results into a table
 

```{r message=FALSE, paged.print=TRUE,cache=TRUE}
x$FDpsd <- c(TS1_FDpsd,TS2_FDpsd,TS3_FDpsd) #TSlogMapr29,TSlogMapr4)
#x$FDpsdRandom <- c(TS1R_FDpsd, TS2R_FDpsd, TS3R_FDpsd,NA,NA)
x$FDpsdAAFT.median <- c(median(TS1surrogates_FD),median(TS2surrogates_FD),median(TS3surrogates_FD)) #,median(TSr29surrogates_FD),median(TSr4surrogates_FD))
knitr::kable(x, digits = 2, booktabs=TRUE,formt="html")
```






<!-- ## The Spectral Slope {#psd} -->

<!-- We can use the power spectrum to estimate a **self-affinity parameter**, or scaling exponent. -->

<!-- * Download `ts1.txt`, `ts2.txt`, `ts3.txt` [here](https://github.com/FredHasselman/DCS/tree/master/assignmentData/Fluctuation_PSDslope). If you use `R` and have package `rio` installed you can run this code. It loads the data into a `data.frame` object directly from `Github`.     -->


<!-- ```{r, echo=TRUE, eval=FALSE, include=TRUE} -->
<!-- library(rio) -->
<!-- TS1 <- rio::import("https://raw.githubusercontent.com/FredHasselman/DCS/master/assignmentData/Fluctuation_PSDslope/ts1.txt") -->
<!-- TS2 <- rio::import("https://raw.githubusercontent.com/FredHasselman/DCS/master/assignmentData/Fluctuation_PSDslope/ts2.txt") -->
<!-- TS3 <- rio::import("https://raw.githubusercontent.com/FredHasselman/DCS/master/assignmentData/Fluctuation_PSDslope/ts3.txt") -->

<!-- # These objects are now data.frames with one column named V1. -->
<!-- # If you want to change the column names -->
<!-- colnames(TS1) <- "TS1" -->
<!-- colnames(TS2) <- "TS2" -->
<!-- colnames(TS3) <- "TS3" -->
<!-- ``` -->

<!-- * Plot the three 'raw' time series. -->

<!-- ### Basic data checks and preparations -->

<!-- For spectral analysis we need to check some data assumptions (see [notes on data preparation, Lecture 4](#data-considerations)). -->

<!-- #### Normalize {-}   -->

<!-- 1. Are the lengths of the time series a power of 2? (Use `log2(length of var)` ) -->
<!--   + Computation of the frequency domain is greatly enhanced if data length is a power (of 2). -->
<!-- 2. Are the data normalized? (we will *not* remove datapoints outside 3SD) -->
<!--     + To normalize we have to subtract the mean from each value in the time series and divide it by the standard deviation, the function `scale()` can do this for you, but you could also use `mean()` and `sd()` to construct your own function. -->
<!-- 3. Plot the normalized time series. -->

<!-- #### Detrend {-}    -->

<!-- Before a spectral analysis you should remove any linear trends (it cannot deal with nonstationary signals!) -->

<!-- 1. Detrend the normalized data (just the linear trend). -->
<!--     + This can be done using the function `pracma::detrend()`. -->
<!--     + Extra: Try to figure out how to detrend the data using `stats::lm()` or `stats::poly()` -->
<!-- 2. Plot the detrended data. -->

<!-- #### Get the log-log slope in Power Spectral Density {-} -->
<!-- The function `fd.psd()` will perform the spectral slope fitting procedure. -->

<!-- 1. Look at the manual pages to figure out how to call the function. The manual is on blackboard and [Github](https://github.com/FredHasselman/DCS/blob/master/functionLib/) -->
<!--     + Remember, we have already normalized and detrended the data. -->
<!--     + You can also look at the code itself by selecting the function name in`R` and pressing `F2` -->
<!-- 2. Calculate the spectral slopes for the three normalized and detrended time series. -->
<!--     + Call with `plot = TRUE` -->
<!--     + Compare the results... What is your conclusion? -->


<!-- ## DFA and SDA {#dfa} -->

<!-- * Use the functions `fd.dfa()` and `fd.sda()` to estimate the self-affinity parameter and Dimension of the series. -->
<!--     + Check what kind of data preparation is required for SDA and DFA in [notes on data preparation, Lecture 4](#data-considerations). -->
<!--     + Compare the results between the three different methods. -->

<!-- [| jump to solution |](#dfasol) -->





<!-- ## Analysis of Deterministic Chaos {#chaos} -->

<!-- * Generate a chaotic timeseries (e.g. $r = 4$ ) of equal length as the time series used above (use the function `growth.ac( ..., type = "logistic")` in `nlRtsa_SOURCE`, see the [solutions of Lecture 1 and 2](#linear-and-logistic-growth)) -->
<!--     + This is in fact one of the series you analysed in a [previous assignment](#pacf). If you still have the results use them for the next part. -->
<!-- * Get all the scaling quantities for this series as well as the ACF and PACF and [some return plots](#the-return-plot) just like in the previous assignments. -->
<!--     + Compare the results to e.g. the heartbeat series. -->

<!--chapter:end:ASSIGNMENTS_D2_scaling.Rmd-->

---
title: "Categorical Auto-RQA"
author: "Fred Hasselman"
date: "1/14/2019"
output: 
  bookdown::html_document2: 
    variant: markdown+hard_line_breaks
    fig_caption: yes
    highlight: pygments
    keep_md: yes
    number_sections: yes
    theme: spacelab
    toc: yes
    toc_float: true
    collapsed: false
    smooth_scroll: true
    code_folding: show
    bibliography: [refs.bib, packages.bib]
    biblio-style: apalike
    csl: apa.csl
    includes:
        before_body: assignmentstyle.html
    pandoc_args: ["--number-offset","2"]
editor_options: 
  chunk_output_type: console
---


```{r, include=FALSE}
knitr::opts_chunk$set(include=TRUE, tidy = FALSE)
```


# (Cross-) Recurrence Quantification Analysis Software

There are several packages in `R` you could use to do `(C)RQA` analyses, but you can also find a `Matlab` toolbox for `(C)RQA` here: [CRP toolbox](http://tocsy.pik-potsdam.de/CRPtoolbox/), there are `Python` libraries as well, see [this page](http://www.recurrence-plot.tk/programmes.php) for an overview of software. We will use the functions in package `casnet`.

```{block2, crqa, type='rmdimportant'}
 The package `crqa()` was mainly designed to run categorical Cross-Recurrence Quantification Analysis (see [Coco & Dale (2014)](http://journal.frontiersin.org/Journal/10.3389/fpsyg.2014.00510/abstract) and for R code see appendices in [Coco & Dale (2013)](http://arxiv.org/abs/1310.0201)). The article is a good reference work for details on how to conduct RQA analysis.
```


You'll need the latest version of `casnet`:

```{r, eval=FALSE}
# Install casnet if necessary: https://github.com/FredHasselman/casnet
# !!Warning!! Very Beta...
# library(devtools)
# devtools::install_github("FredHasselman/invctr")
# devtools::install_github("FredHasselman/casnet")
library(invctr)
library(casnet)
library(rio)
```


Package [`casnet`](https://fredhasselman.github.io/casnet) has 2 functions that will perform auto- and cross-recurrence quantification analyses:         

* Function `crqa_cl()` uses a precompiled program that is run from the command line (see [command line recurrence plot tools](http://tocsy.pik-potsdam.de/commandline-rp.php)). It will take as input one or two timeseries and construct a recurrence matrix and is very fast compared to other methods. The first time you run the command `crqa_cl()` the `casnet` package will try to download the correct executables for your operating system, look for messages in the console to find out if the script succeeded. The output of `crqa_cl()` will be similar to the [CRP toolbox](http://tocsy.pik-potsdam.de/CRPtoolbox/) and [`Python` libraries](http://www.recurrence-plot.tk/programmes.php) by Norbert Marwan.
  - If downloading or running `crqa_cl()` fails, this is usually because you do not have enough rights to execute a command line program on your machine. In that case, figure out how to get those rights, or use `crqa_rp()`.
* Function `crqa_rp()` assumes you have already contructed a recurrence matrix using function `rp()`. The output of `crqa_rp()` contains more measures than `crqa_cl()` some of which are still experimental.


## **Categorical Auto-RQA** 


### RQA by 'hand' {.tabset .tabset-fade .tabset-pills}

Let's start with a small time series... open and/or download this Google sheet: https://docs.google.com/spreadsheets/d/1k4FYDbszfCReCM5_wcw6NIEgSFtTPIy3qmf-kFiZjy8/edit?usp=sharing 

The spreadsheet displays a short categorical time series of 5 observed categories labelled `A` through `E`.
Below the time series a matrix is displayed with zeroes in the upper triangle. The columns and rows of the matrix are labelled according to the categorical time series **y** in `B2:B13`, starting in the lower left corner. The *Line Of Incidence* (LOI) is the diagonal of the matrix going from the lower left corner to the upper right corner. Like the diagonal of a correlation matrix, the LOI in Auto-RQA will contain only recurrent points (`1`), because that's where the column labels will be identical to the row labels.


#### Questions {-}

Because this is Auto-RQA and we are comparing the time series to itself, the recurrence matrix will be symmetrical, so we only have to look at the upper triangle.

* Fill the upper triangle with recurrent points by changing a `0` into a `1`
  - Start with the label in cell `C29` (**A**) and look upwards (`C27` to `C17`). If you see the value **A** recurring, change the `0` into a `1`
   - Proceed to `D29` and find recurrences of the value **B** in the future (in `D26` to `D17`)
   - Continue to fill the upper triangle
* Count the number of `1`s, the recurrent points (`RN`) and put the value in cell `B32` (*Total*)
* Count how many recurrent points are part of a vertical line, and how many are on a diagonal line, put the values in cell `B33` and `B34` respectively.
* Diagonal line lengths:
  - Determine the frequency of occurrence of specific lengths of diagonal lines, record the values in cells `Q17` to `Q20`
  - Record the maximal diagonal line length in `V22`
  - The mean of the lengths of diagonal lines (*L*) will be calculated in cell `V21`
* Vertical line lengths: 
  - Determine the frequency of occurrence of specific lengths of vertical lines, record the values in cells `Q24` to `Q27`
  - Record the maximal vertical line length in `V27`
  - The mean of the lengths of vertical lines (*Trapping Time*) will be calculated in cell `V21`
* Several values are auto-calculated, if you click on the cell you can see how this is done, by looking at the formula bar.
  - Can you understand the calculation of the number of possible recurrent points in `X17`?
* Suppose you randomise the time series... which of the following values do you think would change, and in what way (larger/smaller)
  - Recurrence Rate (`RR`)
  - Mean diagonal line length (`L`, or `MEAN_dl`)
  - Maximum diagonal line length (`L_max`, or `MAX_dl`)
  - Entropy of distribution of diagonal line lengths (`L_entr`, or `ENT_dl`)

#### Answers {-}

* The correct matrix is on the second tab of the spreadsheet.
* The calculation of the number of possible recurrent points in cell `X17` is the size of the matrix (12*12) divided by 2 (because we are only looking at the upper triangle), minus the length of the diagonal, because these aren't actually recurrent points.
* Which of the following values do you think would change if `y` is randomised?
  - Recurrence Rate (`RR`) - **No change**, none of the values are changed, so the number of values that will recur is the unchanged.
  - Mean diagonal line length (`L`, or `MEAN_dl`) - **Likely to be lower**, shuffling will likely break up line structures, but if there are just a few categories and/or the time series is short, it could be the same or higher.
  - Maximum diagonal line length (`L_max`, or `MAX_dl`) - **Very likely to be lower**, shuffling will likely break up line structures, but if there are just a few categories and/or the time series is short, it could be the same or higher.
  - Entropy of distribution of diagonal line lengths (`L_entr`, or `ENT_dl`) - - **Difficult to predict**, shuffling will break up line structures and could increase entropy (more different line lengths), but it is also possible that ther will be only lines of length 2 or 3 left after randomisation, this will be a very homogeneous distribution which has a high entropy. This is more likely if there are just a few categories and/or the time series is short.


### RQA and executive control

`casnet` comes with a vignette demonstrating the usage of `crqa_cl()`, see https://fredhasselman.github.io/casnet/articles/cl_RQA.html, or the manual pages. 

* Study the vignette and try to understand the conclusion based on the surrogate tests.




<!--chapter:end:ASSIGNMENTS_D3_catARQA.Rmd-->

---
title: "Continuous Auto-RQA"
author: "Fred Hasselman"
date: "1/14/2019"
output: 
  bookdown::html_document2: 
    variant: markdown+hard_line_breaks
    fig_caption: yes
    highlight: pygments
    keep_md: yes
    number_sections: yes
    theme: spacelab
    toc: yes
    toc_float: true
    collapsed: false
    smooth_scroll: true
    code_folding: show
    bibliography: [refs.bib, packages.bib]
    biblio-style: apalike
    csl: apa.csl
    includes:
        before_body: assignmentstyle.html
    pandoc_args: ["--number-offset","2"]
editor_options: 
  chunk_output_type: console
---


```{r, include=FALSE}
knitr::opts_chunk$set(include=TRUE, tidy = FALSE)
```


# Phase Space Reconstruction and Continuous Auto-RQA


## **Phase Space Reconstruction** 


Here is a great video summary of the phase space reconstruction technique: https://youtu.be/6i57udsPKms


```{block2, type='rmdnote'}
**R-packages for Phase Space Reconstruction**

Package `casnet` relies on packages `fractal` and `rgl` to reconstruct phase space. Package `rgl` is used for plotting and can cause problems on some windows machines. On windows You'll need to check if you have the [X Window System](http://www.x.org/wiki/) for interactive 3D plotting. This Linux desktop system comes installed in some way or form on most Mac and Windows systems. You can test if it is present by running `rgl::open3d()` in `R`, which will try to open an interactive plotting device.
```


### Reconstruct the Lorenz attractor {.tabset .tabset-fade .tabset-pills}

Package `fractal` includes the 3 dimensions of the Lorenz system in the chaotic regime. Run this code `rgl::plot3d(lorenz,type="l")` with all three packages loaded to get an interactive 3D plot of this strange attractor.

We'll reconstruct the attractor based on just dimension `X` of the system using the data from package `fractal`, and functions from package `casnet`. Don't forget: *always look at the manual pages of a function* you are running.

```{block2, samepack, type='rmdimportant'}
Package `fractal` and package `nonlinearTseries` use functions with similar names, perhapp better to not load them together, or always use `fractal::` and `nonlinearTseries::` if you want to call the functions direclty.
```


#### Questions {-}

* Use `lx <- lorenz[1:2048,1]` to reconstruct the phase space based on `lx`. 
    + Find an optimal embedding lag and dimension by calling `crqa_parameters()`.
    + Use `nnThres = .01`. This is the proportion of remaining nearest neighbours below which we consider the number of dimensions optimal. Because this time series was generated by a simulation of a system, we can use a much lower threshold. For real data, which is much noisier, you can use `0.1` or `0.2`.
    + **NOTE:** If you have a slow computer don't use `2048`, but use e.g. `512`.
    
This `crqa_parameters()` function will call several other functions (e.g. `fractal::timeLag()` with `method = "mutual"` and `nonlinearTseries::findAllNeighbours()` for the false nearest neighbourhood search). Look at the manual page, you can run the `crqa_parameters()` function with the defaults settings, it will show a diagnostic plot. Because we have to search over a range of parameters it can take a while to get the results...

* Based on the output:
    + Choose an embedding lag $\tau$ (in `casnet` functions this is the argument `emLag`). Remember, this is just an optimisation choice. Sometimes there is no first minimum, or no global minimum.
    + Find an appropriate embedding dimension by looking when the false nearest neighbours drop below 1\%. 
    
* Now you can use this information to reconstruct the phase space: 
    + Embed the time series using `ts_embed()` (you can also use `fractal::embedSeries()`).
    + Inspect the object that was returned
    + Use  `rgl::plot3d()` to plot the reconstructed space. Plot the reconstructed phase space. (You'll need to use `as.matrix()` on the object created by `fractal::embedSeries()`)

#### Answers {-}

* Use `lx <- lorenz[1:2048,1]` to reconstruct the phase space based on `lx`. 
    + Find an optimal embedding lag and dimension by calling `crqa_parameters()`.

```{r}
library(rgl)
library(ggplot2)
library(fractal)
library(invctr)
library(casnet)

# The X data
lx <- lorenz[1:2048,1]

# Search for the parameters,
params <- crqa_parameters(lx)

# Assign the optimal parameters
emLag <- params$optimLag
emDim <- params$optimDim
```

* Now you can use this information to reconstruct the phase space

```{r, eval = TRUE}
# For X
lx_emb <- ts_embed(y=lx,emLag = emLag, emDim = emDim)

rgl::plot3d(lx_emb, type="l")

# For Y
ly <- lorenz[1:2048,2]

# Search for the parameters,
params <- crqa_parameters(ly)

# Assign the optimal parameters
emLag <- params$optimLag
emDim <- params$optimDim

# Embed the time series
ly_emb <- ts_embed(y=ly,emLag = emLag, emDim = emDim)
rgl::plot3d(ly_emb,type="l")

# For Z
lz <- lorenz[1:2048,3]

# Search for the parameters,
params <- crqa_parameters(lz)

# Assign the optimal parameters
emLag <- params$optimLag
emDim <- params$optimDim

# Embed the time series
lz_emb <- ts_embed(y=lz,emLag = emLag, emDim = emDim)
rgl::plot3d(lz_emb,type="l")
```


## Continuous Auto-RQA


### RQA of the Lorenz attractor {.tabset .tabset-fade .tabset-pills}

Perform an RQA on the reconstructed state space of the Lorenz system. You'll need a radius (also called: threshold, or $\epsilon$) in order to decide which points are close together (recurrent). You have already seen `casnet` provides a function which will automatically select the best parameter settings for phase space reconstruction: `crqa_parameters()`. 


#### Questions {-}

* First get a radius that gives you a fixed recurrence rate use `crqa_radius()` look at the manual pages.

* Best way to ensure you are using the same parameters in each function is to create some lists with parameter settings (check the `crqa_cl()` manual to figure out what these parameters mean)


#### Answers{-}

```{r}
library(fractal)
library(casnet)

# Lorenz X
lx <- lorenz[1:2048,1]
emLag = 17
emDim = 3

(emRad <- crqa_radius(y1 = lx, emLag = emLag, emDim = emDim)$Radius)

# RQA analysis
(out <- crqa_cl(lx,emDim = emDim, emLag = emLag, emRad = emRad))
```


We can plot the recurrence matrix...

```{r}
library(casnet)

# Unthresholded matrix (no radius applied)
RM <- rp(y1 = lx, emDim = emDim, emLag = emLag)

# plot it
rp_plot(RM)


# Thresholded by the radius
RMth <- di2bi(RM,emRad = emRad)

rp_plot(RMth, plotDimensions = TRUE, plotMeasures = TRUE)


```


### Distinguish between Logistic Map and White noise? {.tabset .tabset-fade .tabset-pills}

RQA promises to be able to distinguish between different dynamical regimes, for example dynamics due to different parameter settings of the logistic map, see e.g. https://en.wikipedia.org/wiki/Recurrence_quantification_analysis#Time-dependent_RQA 


#### Questions {-}

Use RQA to compare white noise to the deterministic chaos series of previous assignments.

```{r eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE, include=TRUE}
library(rio)
library(pracma)

# Reload the data
series <- rio::import("https://github.com/complexity-methods/CSA-assignments/blob/master/assignment_data/BasicTSA_arma/series.xlsx", setclass = "tbl_df")

```

#### Answers {-}


https://en.wikipedia.org/wiki/Recurrence_quantification_analysis#Example




### RQA of circle-tracing data {.tabset .tabset-fade .tabset-pills}

We'll analyse a time series recorded from a subject who is tracing a circle using a computer mouse:   [circle tracing data](https://github.com/FredHasselman/The-Complex-Systems-Approach-Book/tree/master/assignments/assignment_data/RQA_circletrace).


#### Questions {-}

* Perform an RQA on one of the coordinate series (x or y)
* Study what happens to the RQA measures if you shuffle the temporal order.
* Package `fractal` contains a function `surrogate`. This will create so-called *constrained* realisations of the time series. You can look at the vignette in package `casnet` `cl_CRQA` ([also available on Github](https://fredhasselman.github.io/casnet/)) Look at the help pages of the function, or study the *Surrogates Manual* of the [TISEAN software](http://www.mpipks-dresden.mpg.de/~tisean/Tisean_3.0.1/index.html) and create two surrogate series, one based on `phase` and one on `aaft`.
     + Look at the RQA measures and think about which $H_0$ should probably be rejected.
     + If you want to be more certain, you'll have to create a test (more surrogates). The [TISEAN manual](http://www.mpipks-dresden.mpg.de/~tisean/Tisean_3.0.1/docs/surropaper/node5.html#SECTION00030000000000000000) provides all the info you need to construct such a test:
     
     > "For a minimal significance requirement of 95\% , we thus need at least 19 or 39 surrogate time series for one- and two-sided tests, respectively. The conditions for rank based tests with more samples can be easily worked out. Using more surrogates can increase the discrimination power."
  


### EXTRA: Convergent cross mapping

Have a look at the package [`rEDM`, you can use it to do a convergent cross mapping analysis](https://cran.r-project.org/web/packages/rEDM/vignettes/rEDM-tutorial.html)



[Sugihara, G., May, R., Ye, H., Hsieh, C. H., Deyle, E., Fogarty, M., & Munch, S. (2012). Detecting causality in complex ecosystems. science, 338(6106), 496-500.](https://science.sciencemag.org/content/338/6106/496)

<!--chapter:end:ASSIGNMENTS_D3_contARQA.Rmd-->

---
title: "Cross-RQA"
author: "Fred Hasselman"
date: "1/14/2019"
output: 
  bookdown::html_document2: 
    variant: markdown+hard_line_breaks
    fig_caption: yes
    highlight: pygments
    keep_md: yes
    number_sections: yes
    theme: spacelab
    toc: yes
    toc_float: true
    collapsed: false
    smooth_scroll: true
    code_folding: show
    bibliography: [refs.bib, packages.bib]
    biblio-style: apalike
    csl: apa.csl
    includes:
        before_body: assignmentstyle.html
    pandoc_args: ["--number-offset","3"]
editor_options: 
  chunk_output_type: console
---


```{r, include=FALSE}
knitr::opts_chunk$set(include=TRUE, tidy = FALSE)
```

# **Cross-Recurrence Analysis**




## Categorical Cross-RQA and the Diagonal Recurrence Profile {.tabset .tabset-fade .tabset-pills}


In 2008 there was some commotion about a speech held by Barack Obama in Milwaukee ([news item](https://www.cbsnews.com/news/obama-accused-of-plagiarism-in-speech/)). According to the media the speech was very similar to a speech held the gouverneur of Massachusetts (Deval Patrick) some years earlier. Here are some fragments of the speeches,

Obama (2008):

> Don't tell me words don't matter. "I have a dream." Just words? "We hold these truths to be self-evident, that all men are created equal." Just words? "We have nothing to fear but fear itself"just words? Just speeches?

Patrick (2006): 

> "We hold these truths to be self-evident, that all men are created equal." Just wordsjust words! "We have nothing to fear but fear itself." Just words! "Ask not what your country can do for you, ask what you can do for your country." Just words! "I have a dream." Just words!


```{r}
# Load the speeches as timeseries
library(casnet)
obama2008 <- strsplit(x = "dont tell me words dont matter I have a dream just words we hold these truths to be selfevident that all men are created equal just words We have nothing to fear but fear itself just words just speeches", split = " ")[[1]]

patrick2006 <- strsplit(x = "we hold these truths to be selfevident that all men are created equal just words just words we have nothing to fear but fear itself just words ask not what your country can do for you ask what you can do for your country just words I have a dream just words", split = " ")[[1]]

uniqueID <- as.numeric_discrete(c(obama2008,patrick2006))

speeches <- as.data.frame(ts_trimfill(x=uniqueID[1:length(obama2008)], y=uniqueID[(length(obama2008)+1):length(uniqueID)]))
colnames(speeches) <- c("Obama","Patrick")

```


### Questions {-}

* Plot both time series. 

* Because both fragments weren't of equal size, one series is "filled" at the end with the value `0`
    - Do you think this has an effect on the CRQA measures? 
    - What & why?

* Perform a categorical CRQA:
   - First create a cross recurrence matrix using function `rp()`. 
   - Look at the manual page what the values mean, look at last the categorical RQA assignment for the values of `emLag`, `emDim` and `emRad`
   - Get recurrence measures using `crqa_rp()` (ignore the warning),
   - Plot the recurrence plot using `rp_plot()`
   - Also look at the diagonal recurrence profile using `crqa_diagProfile()`
   
* What is your opinion about the similarity of these speeches?


### Answers {-}

* Adding a value which occurs in just 1 series (whether it is `-2`, or `0`), but not the other makes sure that value will not be recurring in a Cross Recurrence Plot.


```{r}
# Plot the speech series
plot(ts(speeches))

# Cross recurrence matrix
RM <- rp(y1 = as.numeric(speeches$Obama), y2 = as.numeric(speeches$Patrick), emDim = 1, emLag = 1, emRad = 0)

# Measures
crqa_out <- crqa_rp(RM)

# Plot
rp_plot(RM, plotDimensions = TRUE, plotMeasures = TRUE)

# Diagonal profile
drp <- crqa_diagProfile(RM, diagWin = 40,doShuffle = FALSE, doPlot = FALSE)
plot(drp$Diagonal,drp$RR,type="l")

```


## Continuous CRQA and the Diagonal Profile {.tabset .tabset-fade .tabset-pills}


### Questions {-}

* Create two sinewave variables for CRQA analysis and use the $x$ and $y$ coordinates of the circletracing data:

```{r}
library(rio)
y1 <- sin(1:500*2*pi/67)
y2 <- sin(.01*(1:500*2*pi/67)^2)

# Here are the circle trace data
xy <- import("https://raw.githubusercontent.com/FredHasselman/The-Complex-Systems-Approach-Book/master/assignments/assignment_data/RQA_circletrace/mouse_circle_xy.csv") 

circle_x <- xy$x[1:500]
circle_y <- xy$y[1:500]

```

* You have just created two sine(-like) waves. Well examine if and how they are coupled in a shared phase space. 
   + As a first step plot them.
   + Also plot the cirlce trace state space

* Find a common embedding delay and an embedding dimension (if you calculate an embedding dimension for each signal separately, as a rule of thumb use the highest embedding dimension you find in further analyses).

* We can now create an unthresholded cross recurrence matrix. 
    + use function `rp()`
    + Fill in the values you decided on for embedding delay and embedding dimension
    
* Run the CRQA using `crqa_cl()` using th time series as input, or `crqa_rp()` using a thresholded matrix as input.
    + You can find a radius automatically, look in the `casnet` manual for function `crqa_radius()`. 
    + Most functions, like `rp_plot()` will call `crqa_radius()` if no radius is provided in the argume t `emRad`.
    + Request a radius which will give us about 5\% recurrent points (default setting).
    + You can create a thresholded matrix by calling function `di2bi()`,
* Produce a plot of the recurrence matrix using `rp_plot()`. Look at the manual pages.

* Can you understand what is going on? 
  + For the simulated data: Explain the the lack of recurrent points at the beginning of the time series.
  + For the circle trace: How could one see these are not deterministic sine waves?

* Examine the synchronisation under the diagonal LOS. Look in the manual of `crqa_diagProfile()`. More elaborate explanation can be found in [Coco & Dale (2014)](http://journal.frontiersin.org/Journal/10.3389/fpsyg.2014.00510/abstract). 
  + To get the diagonal profile from a recurrence matrix `crqa_diagProfile(RM, winDiag = , ...)`. Where `winDiag` is will indicate the window size `-winDiag ... 0 ... +winDiag`.
  + How far is the peak in RR removed from 0 (Line of Synchronisation)? 

* To check whether the patterns are real by conducting a Cross-RQA with a shuffled/surrogate version of e.g. time series $y2$.
   + Function `crqa_diagProfile()` can do a shuffled analysis, check the manual pages. 
   + Use `Nshuffle = 1` (or wait a long time)

> NOTE: If you generate surrogate timeseries, make sure the RR is the same for all surrogates. Try to keep the RR in the same range by using.


### Answers {-}


* You have just created two sine(-like) waves. Well examine if and how they are coupled in a shared phase space. 
   + As a first step plot them.
   + Also plot the cirlce trace state space

```{r}
library(rio)
y1 <- sin(1:1000*2*pi/67)
y2 <- sin(.01*(1:1000*2*pi/67)^2)

# Here are the circle trace data
xy <- import("https://raw.githubusercontent.com/FredHasselman/The-Complex-Systems-Approach-Book/master/assignments/assignment_data/RQA_circletrace/mouse_circle_xy.csv") 

circle_x <- xy$x[1:1000]
circle_y <- xy$y[1:1000]

# Time series
plot(cbind(ts(y1),ts(y2)))
plot(cbind(ts(circle_x),ts(circle_y)))

# State space
plot(y1,y2,type="l", pty="s",main = "Sine waves")
plot(circle_x,circle_y,type="l", pty="s",main = "Mouse Circle Trace")
```

* Find a common embedding delay and an embedding dimension (if you calculate an embedding dimension for each signal separately, as a rule of thumb use the highest embedding dimension you find in further analyses).

```{r}
params_y1 <- crqa_parameters(y1)
params_y2 <- crqa_parameters(y2)
```

> The diagnostic plot of the sine wave `y1` looks a bit odd. This is due to the fact that mutual information will discretize the time series, and the sine will  fill all bins with the same values, because it is perfectly repeating itself. We'll use emDim = 2 and lag = 1

```{r, eval=FALSE}
emLag <- 1
emDim <- 2

RM <- rp(y1=y1, y2=y2,emDim = emDim, emLag = emLag)

# Unthresholded Matrix
rp_plot(RM, plotDimensions = TRUE)

# Thresholded Matrix, based on 5% RR
emRad <- crqa_radius(RM = RM, targetValue = .05)$Radius
RP <- di2bi(RM, emRad = emRad)
rp_plot(RP, plotDimensions = TRUE)

# The diagonal profile
drp <- crqa_diagProfile(RP, diagWin = 50, doPlot = FALSE)

# Compare CRQA with one randomly shuffled series
drpRND <- crqa_diagProfile(RP, diagWin = 50, doShuffle = TRUE, y1 = y1, y2 = y2, Nshuffle = 19, doPlot = FALSE)

```


```{r}
params_cx <- crqa_parameters(circle_x)
params_cy <- crqa_parameters(circle_y)
```

> The circle trace data can be embedded in 3 dimensions. This is not so strange, even though the circle was drawn in 2 dimensions, there are also speed/acceleration and accuracy constraints.


```{r, eval=FALSE}
emLag <- median(params_cx$optimLag, params_cy$optimLag)
emDim <- max(params_cx$optimDim, params_cy$optimDim)

RM <- rp(y1=circle_x, y2=circle_y,emDim = emDim, emLag = emLag)

# Unthresholded Matrix
rp_plot(RM, plotDimensions = TRUE)

# Thresholded Matrix, based on 5% RR
emRad <- crqa_radius(RM = RM, targetValue = .05)$Radius
RP <- di2bi(RM, emRad = emRad)
rp_plot(RP, plotDimensions = TRUE)

# The diagonal profile
drp <- crqa_diagProfile(RP, diagWin = 100, doPlot = FALSE)

```


### RQA of circle-tracing data {.tabset .tabset-fade .tabset-pills}


The data from the c ircle trracing experiment are available below, scrtuinise those data, compare the mouse to the centre of pressure.


```{r}
library(rio)
series <- rio::import("https://github.com/complexity-methods/CSA-assignments/raw/master/assignment_data/BasicTSA_arma/series.xlsx")
```


  


<!--chapter:end:ASSIGNMENTS_D4_CRQA.Rmd-->

---
title: "DCS Assignments Day 4"
author: "Fred Hasselman"
date: "1/14/2019"
output: 
  bookdown::html_document2: 
    variant: markdown+hard_line_breaks
    fig_caption: yes
    highlight: pygments
    keep_md: yes
    self_contained: yes
    number_sections: yes
    theme: spacelab
    toc: yes
    toc_float: true
    collapsed: false
    smooth_scroll: true
    code_folding: show
    bibliography: [refs.bib, packages.bib]
    biblio-style: apalike
    csl: apa.csl
    includes:
        before_body: assignmentstyle.html
    pandoc_args: ["--number-offset","3"]
    
---


```{r, include=FALSE}
knitr::opts_chunk$set(include=TRUE, tidy = FALSE)
```



# **Complex Networks**

To complete these assignments you need these packages:
```{r, message=FALSE, warning=FALSE}
library(igraph)
library(qgraph)
library(casnet) 
```


* There are many frameworks for analysing and plotting graphs and they are implemented on different platforms often in more-or-less the same way. We'll use the `igraph` framework: https://igraph.org for `R`, but there many more options, see e.g. https://github.com/briatte/awesome-network-analysis#r
* The manual pages of package `igraph` in `R` provide a lot of information, but not everything the package can do. One thing that is missing are  the parameters available to customise the graphs for plotting. If you can't find something in the `R` manual, have a look at the online documentation: https://igraph.org/r/doc/plot.common.html
* Package `qgraph` was developped to fit and analyse the so-called `Gaussian Graph Model`, e.g in the symptom network studies. It is built on the `igraph` framework and you can transform `qgraph` output to an `igraph` object using the function `qgraph::as.igraph.qgraph()`
* The functions in package `casnet` developped to create, analyse and plot **recurrencde networks** are also based on the `igraph` framework. 


## Creating, plotting and describing simple graphs {.tabset .tabset-fade .tabset-pills}

We'll start using `igraph` and create some simple graphs. 

To create a small ring graph and plot it run the code below.

```{r}
gR <- graph.ring(20)

# This will display the igraph structure
gR

# To plot, simply call:
plot(gR)
```

You can request to represent the graph as a so-called *edge-list* which just lists the edges in a graph by reporting the nodes the edge connects *from* - *to*

```{r}
get.edgelist(gR)

```

Other useful functions are `E()` and `V()` for manipulating attributes of *Edges* (connections) and *Vertices* (nodes). See  https://igraph.org/r/doc/plot.common.html for all available attributes.

```{r}
# List vertices
V(gR) 
# List edges
E(gR)

# Set vertex color
V(gR)$color <- "red3"
# set vertex label
V(gR)$label.color <- "white"
# Set vertex shape
V(gR)$shape <- "square"
# Set edge color
E(gR)$color <- "steelblue"
# Set efge size
E(gR)$width <- 4

# Plot
plot(gR)
```


### Questions {-}

* What will be the degree of each node in the ring graph?
    - To check your answer, look in the manual pages of the `igraph` package for a function that will get you the degree of each node
    - Also, get the average path length.
* Create a "small world" graph like the one descibed by [Watts & Strogatz (1998)](https://www.nature.com/articles/30918) and plot it, using the `igraph` function `watts.strogatz.game()`. 
     - Use dimension `dim = 1`, number of nodes `size = 20`, neighbourhood `nei = 5`, and connection probability `p = .05`
     - Get the degree, average path length and transitivity of this graph.
     - `qgraph` has a function called `smallworldness()` that allows you to calculate the so-called *Small Worls Index* [(Humphries & Gurney, 2008)](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0002051). Calulate the smallwordlness of the graph, a value `> 1` implies a small world network. 
     - 
* Create a directed scale-free graph using `igraph` function `barabasi.game()` with 100 nodes `n=100`.
     - Get the degree, average path length and transitivity
     - There should be a power-law relation between the number og nodes that have a specific degree... (but this is a very small network)


### Answers {-}


```{r}
# Ring graph degree
degree(gR)

# avergage path length
average.path.length(gR)
```



```{r}
# Small world network
gSW <- igraph::watts.strogatz.game(dim = 1, size = 20, nei = 5, p =0.05)

# Plot
plot(gSW)

# Degree
cbind(node = V(gSW), connections = degree(gSW))

# Average path length
igraph::average.path.length(gSW)

# Transitivity
igraph::transitivity(gSW)

# Complute the Small World Index, aka 'smallworldness'
qgraph::smallworldness(gSW)
```


```{r}
# Scale-free network
gSF <- barabasi.game(100)

## Change some aesthetics
# No numbers in nodes
V(gSF)$name <- ""
# Change node size
V(gSF)$size = 5
# Change Edge size
E(gSF)$arrow.size = .5
E(gSF)$arrow.width = .5


# Plot
plot(gSF)

# Degree
rbind(node = V(gSF), connections = degree(gSF))

# Average path length
average.path.length(gSF)

# Transitivity
transitivity(gSF)

## Power-law?
# Get degree distribution
dist <- degree_distribution(gSF)
# Note nonzero elements for log trasnsform
ID <- dist>0
# Get an index for node groups
nodes <- seq_along(dist)

# Fit a line in log-log space
lf  <- lm(log(dist[ID])~log(nodes[ID]))
lfit <- predict(lf)

# Plot it!
plot(nodes,dist,pch=16,log = "xy")
lines(x = nodes[ID],y = exp(lfit), col="red")
```



## Social Networks 

The package `igraph` contains data on a Social network of friendships between 34 members of a karate club at a US university in the 1970s.

> See W. W. Zachary, An information flow model for conflict and fission in small groups, *Journal of Anthropological Research 33*, 452-473 (1977).


* Get a graph of the **community structure** within the social network. 
    - There are many procedures for finding communities, or, subgraphs, in graphs, try to understand how the`walktrap` procedure finds them.
    - Check the manual pages to figure out what kind of information the functions `sizes`, `communities`, `membership` and `modularity` provide
    

```{r}
# Community membership
karate <- graph.famous("Zachary")
wc     <- cluster_walktrap(karate)
plot(wc, karate)

# Measures
modularity(wc)

sizes(wc)
communities(wc)
cbind(node = V(karate) , community = membership(wc))

# Is the structure a hierarchy?
if(is_hierarchical(wc)){
  plot(as.dendrogram(wc))
}
```


* It is also possible to get the adjacency matrix of a graph
    - What do the columns, rows and 0s and 1s stand for?
    - Looks a bit like a recurrence matrix, except, the rows and columns don't represent an (embedded) time series 

```{r}
(am <-get.adjacency(karate))


rp_plot(am)
```


## Recurrence Networks {.tabset .tabset-fade .tabset-pills}

Recurrence networks are graphs created from a recurrence matrix. This means the nodes of the graph represent time points and the connections between nodes represent a recurrence relation betwen the values observed at those time points. That is, often the matrix represents recurrences in a reconstructed state space, the values are coordinates and therefore we would say the edges of a recurrence network represent a temporal relation between recurring states. The ultimate reference for learning everything about recurrence networks is:

> [Zou, Y., Donner, R. V., Marwan, N., Donges, J. F., & Kurths, J. (2018). Complex network approaches to nonlinear time series analysis. Physics Reports](https://www.sciencedirect.com/science/article/abs/pii/S037015731830276X?via%3Dihub)


Package `casnet` has some functions to create recurrence networks, they are similar to the functions used for CRQA:
* `rn()` is very similar to `rp()`, it will create a matrix based on embedding parameters. One difference is the option to create a weighted matrix. This is a matrix in which non-recurring values are set to 0, but the recurring values are not replaced by a 1, the distance value is retained and acts as an edge-weight
* `rn_plot()` will produce the same as `rp_plot()`

We can turn the recurrence matrix into an adjecency matrix, an `igraph` object. This means we can use all the `igraph` functions to calculate network measures.

```{r}
# Reload the data we used earlier
series <- rio::import("https://github.com/complexity-methods/CSA-assignments/raw/master/assignment_data/BasicTSA_arma/series.xlsx")

# Lets use a shorter dataset to speed things up
series <- series[1:500,]
```

We'll analyse the three time series as a recurrence network:
* Compare the results, look at the SMall World Index and other measures
   - Remember: TS_1 was white noise, TS_2 was a sine with added noise, TS_3 was the logistic map in the chaotic regime.
* Note that some of the RQA measures can be exactly calculated from the measures of the network representation.
   - Try to understand why the Recurrence is reprented as the *degree centrality() of the network (`igraph::centr_degree()`)


### TS 1 {-}

```{r message=FALSE, warning=FALSE}
#----------------------
# Adjacency matrix TS_1
#----------------------
plot(ts(series$TS_1))

arcs=6

crqa_parameters(series$TS_1)

# By passing emRad = NA, a radius will be calculated
RN1 <- casnet::rn(y1 = series$TS_1, emDim = 7, emLag = 2, emRad = NA, targetValue = .01)
casnet::rn_plot(RN1)

# Get RQA measures
rqa1 <- crqa_rp(RN1)
knitr::kable(rqa1,digits = 2)

g1  <- igraph::graph_from_adjacency_matrix(RN1, mode="undirected", diag = FALSE)
V(g1)$size <- degree(g1)
g1r <- casnet::make_spiral_graph(g1,arcs = arcs,type = "Euler" ,epochColours = getColours(arcs))


# Network measures
igraph::average.path.length(g1)
igraph::transitivity(g1)
qgraph::smallworldness(g1)

recs1 <- igraph::centr_degree(g1)

(RP_N <- sum(recs1$res))
rqa1$RP_N

RP_N / recs1$theoretical_max
rqa1$RR
```

### TS 2 {-}

```{r message=FALSE, warning=FALSE}
#----------------------
# Adjacency matrix TS_2
#----------------------
plot(ts(series$TS_2))

crqa_parameters(series$TS_2)

RN2 <- rn(y1 = series$TS_2, emDim = 7, emLag = 2, emRad = NA,  targetValue = 0.01)
rn_plot(RN2)

# Get RQA measures
rqa2 <- crqa_rp(RN2)
knitr::kable(rqa2,digits = 2)

g2 <- graph_from_adjacency_matrix(RN2, mode="undirected", diag = FALSE)
V(g2)$size <- degree(g2)
g2r <- casnet::make_spiral_graph(g2,arcs = arcs,type = "Euler" ,epochColours = getColours(arcs))

# Network measures
average.path.length(g2)
transitivity(g2)
smallworldness(g2)

recs2 <- centr_degree(g2)

(RP_N <- sum(recs2$res))
rqa2$RP_N

RP_N / recs2$theoretical_max
rqa2$RR
```

### TS 3 {-}

```{r message=FALSE, warning=FALSE, paged.print=TRUE}
#----------------------
# Adjacency matrix TS_3
#----------------------
plot(ts(series$TS_3))

crqa_parameters(series$TS_3)

RN3 <- rn(y1 = series$TS_3, emDim = 7, emLag = 8, emRad = NA, targetValue = 0.01)
rn_plot(RN3)

# Get RQA measures
rqa3 <- crqa_rp(RN3)
knitr::kable(rqa3,digits = 2)

g3 <- graph_from_adjacency_matrix(RN3, mode="undirected", diag = FALSE)
V(g3)$size <- degree(g3)
g3r <- casnet::make_spiral_graph(g3,arcs = arcs,type = "Euler" ,epochColours = getColours(arcs))


average.path.length(g3)
transitivity(g3)
smallworldness(g3)

recs3 <- centr_degree(g3)

(RP_N <- sum(recs3$res))
rqa3$RP_N

RP_N / recs3$theoretical_max
rqa3$RR
```


## Multiplex Recurrence Networks {.tabset .tabset-fade .tabset-pills}


Consider the three time series to be part of a multi-layer recurrence network.
Common properties of the multiplex network are multi-layer mutual information and edge overlap.

```{r}
# Inter-layer Mutual Information
(interlayer_mi12 <- mif_interlayer(g1,g2))
(interlayer_mi13 <- mif_interlayer(g1,g3))
(interlayer_mi23 <- mif_interlayer(g2,g3))

# mean I-L MI  
mean(c(interlayer_mi12,interlayer_mi13,interlayer_mi23))

# Edge Overlap
(edge_overlap12 <- length(E(g1 %s% g2)) / (length(E(g1))+length(E(g2))))
(edge_overlap13 <- length(E(g1 %s% g3)) / (length(E(g1))+length(E(g3))))
(edge_overlap23 <- length(E(g2 %s% g3)) / (length(E(g2))+length(E(g3))))

# mean EO 
mean(c(edge_overlap12,interlayer_mi13,interlayer_mi23))

# Overall Edge Overlap
(eo_all  <- length(E(intersection(g1,g2,g3))))
(eo_mean <- eo_all / (edge_overlap12+edge_overlap13+edge_overlap23))
```
  
  
  



## Symptom Networks: Package `qgraph`

Learn about the functionality of the `qgraph` frm its author Sacha Epskamp.

Try running the examples, e.g. of the `Big 5`: 

[http://sachaepskamp.com/qgraph/examples](http://sachaepskamp.com/qgraph/examples)


### `qgraph` tutorials / blog

Great site by Eiko Fried:
[http://psych-networks.com ](http://psych-networks.com )




<!-- ## Early Warnings -->

<!-- Have a look at the site [Early Warning Systems](http://www.early-warning-signals.org/home/) -->

<!-- There is an accompanying [`R` package `earlywarnings`](https://cran.r-project.org/web/packages/earlywarnings/index.html) -->



<!--chapter:end:ASSIGNMENTS_D4_Networks.Rmd-->

---
title: "Dynamic Complexity as an Early-Warning Signal"
author: "Merlijn Olthof & Fred Hasselman"
date: "7/8/2019"
output: 
  bookdown::html_document2: 
    variant: markdown+hard_line_breaks
    fig_caption: yes
    highlight: pygments
    keep_md: yes
    number_sections: yes
    theme: spacelab
    toc: yes
    toc_float: true
    collapsed: false
    smooth_scroll: true
    code_folding: show
    bibliography: [refs.bib, packages.bib]
    biblio-style: apalike
    csl: apa.csl
    includes:
        before_body: assignmentstyle.html
    pandoc_args: ["--number-offset","4"]
    
---


```{r, include=FALSE}
knitr::opts_chunk$set(include=TRUE, tidy = FALSE)
```
## **Set up**

Make sure that the following packages are installed and loaded.The OSFR package should be installed from GitHub. (you may wish/need to use more packages of course)
```{r notrun}
library(invctr)
library(casnet)
library(plyr)
library(tidyverse)
library(osfr)

devtools::install_github('CenterForOpenScience/osfr')
```

## **Early Warning Signals in a Clinical Case Study**

Dynamic assessment (e.g. ecological momentary assesment) is increasingly used in the mental health field. Process monitoring with dynamic assessment can empower patients and benefit self-insight (van Os et al., 2017). Additionally, ecological momentary assesment (EMA) time series can be analyzed and the results can be used to inform the treatment process. 

In this assignment, you'll use complex systems theory and methodology developed to analyse fluctuations in rating scale time series to perform a new quantitative case study for real patient data. You'll analyze publicly available EMA data from 1 patient that was drawn from a personalized psychotherapy study (cf. Fisher & Boswell, 2016) and was analysed by several different teams in a 'Many Analysists' study: [*Time to get personal? The impact of researchers choices on the selection of treatment targets using the experience sampling methodology*](https://doi.org/10.31234/osf.io/c8vp7) by Bastiaansen et al. (2019). 


### First Steps {-}
Load the data from this study as follows:
```{r firststeps}
# Load data from OSF https://osf.io/tcnpd/
df <- rio::import(osfr::osf_download(osfr::osf_retrieve_file("tcnpd") , overwrite = TRUE)$local_path)
```

Take a look at the dataset. You can learn the meaning of the variable names by looking at the open science framework project page: https://osf.io/h3djy/ 
Also, take a look at the paper by Bastiaansen et al. (2019).

### Data inspection {-}
Let's inspect the data. Run this code, which results in plots of the data with indicated missing values. 
```{r,fig.height=10}
library(imputeTS)

vars <- c(seq("energetic"%ci%df,"procrast"%ci%df),"avoid_people"%ci%df)


op <- par(mfrow = c(ceiling(length(vars)/4),4),mar =c(2,2,2,2))
l_ply(vars, function(c) imputeTS::plotNA.distribution(x = df[,c],main=colnames(df)[c],xlab = "",ylab = ""))
par(op)
```


## New dataset {.tabset .tabset-fade .tabset-pills}
As you might have concluded from inspecting the data, it might make sense to split the items 'difficult', 'unsatisfy' and 'hours' from the other items since these three items are measured daily, instead of four times a day. 

### Questions {-}

Create a new dataset (called df.ema) with all EMA items expect the three items named above.  
Omit all missing values, as these will lead to errors in the use of ts_levels and dyn_comp. (Of course, you could and maybe should do time series imputation as well, but we won't do that now). 

### Answers {-}

```{r}
df.ema <- df[, 4:25]
df.ema$avoid_people <- df$avoid_people
df.ema <- df.ema[!is.na(df.ema$energetic),]
```

## Phase Transitions and Early-Warning Signals {.tabset .tabset-fade .tabset-pills}

Before we continue, read this short summary of what you've learned about phase transitions and early-warning signals: 

> Change from one attractor to another is called a phase transition. Phase transitions are preceded by a destabilization period in which the stability of the existing state decreases (as a consequence of increasing control parameters). During destabilization, the system loses its resilience to external influences leading to increased fluctuations and disorder in the systems behaviour (critical fluctuations) and an increased return time to the existing state after perturbation (critical slowing down). Destabilization ends abruptly when the system makes a phase transition towards a new stable state. Critical fluctuations and critical slowing down can therefore serve as early-warning signals (EWS) for imminent phase transitions. 
Phase transitions are identified as qualitative changes in the macro-level behaviour of the system by looking at so-called 'collective variables' (Thelen & Ullrich, 1991). As was already explained in the opening lecture of this summerschool, finding the right collective variable to look at for identification of phase transitions can be difficult. In clinical data, a symptom severity measure might be the most sensible collective variable. 
This dataset has no symptom severity measure, but let's use the item 'feeling positive' as collective variable. 'Feeling positive' is of course cleary related to both diagnosis of this patient (major depressive disorder and generalized anxiety disorder) and it provides a nice illustration of a transition, as we will see next...


### Questions {-}

Search for mean-shifts in the item 'feeling positive' using the function ts_levels()

### Answers {-}

```{r}
lvl <- ts_levels(df.ema$positive, minDataSplit = 12, minLevelDuration = 7, changeSensitivity = 0.05, maxLevels=30, method="anova")

lvl_long <- lvl$pred %>% 
  gather(key=variable, value=value, -x)
  
ggplot(lvl_long, aes(x=x, y=value, colour=variable)) + geom_line() + theme_bw()
```

## Dynamic Complexity {.tabset .tabset-fade .tabset-pills}

Now it is time to analyze the dynamic complexity of the ESM time series and see if we can find early-warning signals. 

Use `dc_win()` to calculate Dynamic Complexity. The function can also call `plotDC_res()` to produce a *Complexity Resonance Diagram*. Also, the resonance diagrams for only measure D or F can simply be obtained by adding `doPlotF = TRUE` and `doPlotD = TRUE`.

We need to make sure all variables are on the same scale, for this dataset there's no need to transform, all data are on a scale of `0-100`. The arguments `scale_min` and `scale_max` have to be given manually as they refer to the theoretical limits of the scale. If the data contain a mix of known scale ranges, transformation should be done manually.


### Questions {-}

a) Calculate the dynamic complexity of the EMA items using the function `dc_win()` and ask for a plot, set the window size to 16 measurement points (4 days)
b) Plot the dynamic complexity resonance diagram with variables ordered on mean dynamic complexity
c) Calculate cumulative complexity peaks with the function `dc_ccp()` 
d) Make a cumulative complexity peak plot using the function `plotDC_ccp()`
e) Plot the 'dynamic complexity profile', i.e. the time series of the average dynamic complexity of all items 


### Answers {-}

```{r}
win <- 7 

dc.case <- dc_win(df.ema, win = win, scale_min=0, scale_max=100, doPlot = TRUE, colOrder = TRUE, doPlotF = TRUE, doPlotD = TRUE)

plotDC_res(df_win = dc.case, win = win, doPlot = TRUE, colOrder = NA)

ccp.case <- dc_ccp(df_win = dc.case)
plotDC_ccp(df_ccp = ccp.case, win = win, colOrder = NA, title = "Cumulative Complexity Peak Plot")

dc.mean <- rowMeans(as.matrix(dc.case))
plot(dc.mean, type='l')
```


## Dynamic Complexity as an Early-Warning Signal? {.tabset .tabset-fade .tabset-pills}

We can visualize the relation between dynamic complexity and the transition in 'feeling positive' in multiple ways.
   
### Questions {-}

a) Plot the dynamic complexity profile and `feeling positive` (both raw data and levels) in one figure. You can use the code in the answer. 
b) Plot the relation again using the funciton `plotDC_lvl()` which takes the results of `ts_levels()`, `dc_win()` and `dc_ccp()` as an input

### Answers {-}

a) Plot the dynamic complexity profile and `feeling positive` (both raw data and levels) in one figure. You can use the code in the answer. 

```{r}
df.ema$measure <- (1:nrow(df.ema)) #as a time index variable
df.ema$lvl <- lvl$pred$p

df.ema %>% 
  mutate(Level=ts_standardise(lvl),
         CollectiveVariable=ts_standardise(df.ema$positive),
         MeanDC=ts_standardise(dc.mean)) %>% 
  select(measure,CollectiveVariable,MeanDC, Level) %>% 
  gather(key=variable,value=value,-measure) %>%
  ggplot(aes(x=measure, y=value, colour=variable)) + geom_line() + theme_bw()
```

b) Plot the relation again using the funciton `plotDC_lvl()` which takes the results of `ts_levels()`, `dc_win()` and `dc_ccp()` as an input

```{r}
plotDC_lvl(df_ccp = ccp.case, df_win = dc.case, df_lvl = lvl, win = win, levelName = "Feeling Positive")
```



<!--chapter:end:ASSIGNMENTS_D5_DC.Rmd-->

---
title: "Time Series Analysis: Temporal Correlations and Fractal Scaling"
author: "Fred Hasselman & Maarten Wijnants"
date: "1/14/2018"
output: 
  html_document: 
    fig_caption: yes
    highlight: pygments
    keep_md: yes
    number_sections: no
    theme: spacelab
    toc: yes
    toc_float: true
    collapsed: false
    smooth_scroll: true
    code_folding: show

---



# **Basic Timeseries Analysis** 

In this course we will not discuss the type of linear time series models known as Autoregressive Models (e.g. AR, ARMA, ARiMA, ARfiMA) summarised on [this Wikipedia page on timeseries](https://en.wikipedia.org/wiki/Time_series#Models). We will in fact be discussing a lot of methods in a book the Wiki page refers to for *'Further references on nonlinear time series analysis'*: [**Nonlinear Time Series Analysis** by Kantz & Schreiber](https://www.cambridge.org/core/books/nonlinear-time-series-analysis/519783E4E8A2C3DCD4641E42765309C7). You do not need to buy the book, but it can be a helpful reference if you want to go beyond the formal level (= mathematics) used in this course. Some of the packages we use are based on the acompanying software [**TiSEAN**](https://www.pks.mpg.de/~tisean/Tisean_3.0.1/index.html) which is written in `C` and `Fortran` and can be called from the commandline (Windows / Linux).


## **Correlation functions**  

Correlation functions are intuitive tools for quantifiying the temporal structure in a time series. As you know, correlation can only quantify linear regularities between variables, which is why we discuss them here as `basic` tools for time series analysis. So what are the variables? In the simplest case, the variables between which we calculate a correlation are between a datapoint at time *t* and a data point that is seperated in time by some *lag*, for example, if you would calculate the correlation in a lag-1 return plot, you would have calculated the 1st value of the correlation function (actually, it is 2nd value, the 1st value is the correlation of time series with itself, the lag-0 correlation, which is of course $r = 1$)  

### ACF and PCF {.tabset .tabset-fade .tabset-pills}


You can do the analyses in SPSS or in `R`, but this analysis is very common so you'll find functions called `acf`, `pacf`and `ccf` in many other statistical software packages,


#### Questions (SPSS) {-}

* Download the file [`series.sav`](https://github.com/FredHasselman/The-Complex-Systems-Approach-Book/blob/master/assignments/assignment_data/BasicTSA_arma/series.sav) from Github. 

It contains three time series `TS_1`, `TS_2` and `TS_3`. As a first step look at the mean and the standard deviation (`Analyze` >> `Descriptives`).  Suppose these were time series from three subjects in an experiment, what would you conclude based on the means and SD???s?  

* Let???s visualize these data. Go to `Forecasting` >> `Time Series` >> `Sequence Charts`. Check the box One chart per variable and move all the variables to Variables. Are they really the same?  

* Let???s look at the `ACF` and `PCF`
    + Go to `Analyze` >> `Forecasting` >> `Autocorrelations`. 
    + Enter all the variables and make sure both *Autocorrelations* (ACF) and *Partial autocorrelations* (PACF) boxes are checked. Click `Options`, and change the `Maximum Number of Lags` to `30`. 
    + Use the table to characterize the time series:  


|                    SHAPE                | INDICATED MODEL |
|-----------------------------------------|-------------------------------------------------------------------------------------------------|
|       Exponential, decaying to zero     | Autoregressive model. Use the partial autocorrelation plot to identify the order of the autoregressive model|
| Alternating positive and negative, decaying to zero  | Autoregressive model. Use the partial autocorrelation plot to help identify the order.|
| One or more spikes, rest are essentially zero | Moving average model, order identified by where plot becomes zero. |
| Decay, starting after a few lags | Mixed autoregressive and moving average model.|
All zero or close to zero  | Data is essentially random.|
| High values at fixed intervals | Include seasonal autoregressive term. |
| No decay to zero  | Series is not stationary. |


<!-- 4. You should have identified just one time series with autocorrelations: `TS_2`. Try to fit an `ARIMA(p,0,q)` model on this time series.  -->
<!--     - Go to `Analyze` >> `Forecasting` >> `Create Model`, and at `Method` (Expert modeler) choose `ARIMA`.  -->
<!--     - Look back at the `PACF` to identify which order (`p`) you need (last lag value at which the correlation is still significant). This lag value should go in the Autocorrelation p box.  -->
<!--     - Start with a Moving Average `q` of one. The time series variable `TS_2` is the `Dependent`.  -->
<!--     - You can check the statistical significance of the parameters in the output under `Statistics`, by checking the box `Parameter Estimates`.  -->
<!--     - This value for `p` is probably too high, because not all AR parameters are significant.  -->
<!--     - Run ARIMA again and decrease the number of AR parameters by leaving out the non-significant ones.   -->

<!-- 5. By default `SPSS` saves the predicted values and 95% confidence limits (check the data file). We can now check how well the prediction is: Go to `Graphs` >> `Legacy Dialogs` >> `Line.` Select `Multiple` and `Summaries of Separate Variables`. Now enter `TS_2`, `Fit_X`, `LCL_X` and `UCL_X` in `Lines Represent`. `X` should be the number of the last (best) model you fitted, probably 2. Enter `TIME` as the `Category Axis`.   -->

* In the simulation part of this course we have learned a very simple way to explore the dynamics of a system: The return plot. The time series is plotted against itself shifted by 1 step in time. 
    * Create return plots (use a Scatterplot) for the three time series. Tip: You can easily create a `t+1` version of the time series by using the LAG function in a `COMPUTE` statement. For instance: 
    
```
COMPUTE TS_1_lag1 = LAG(TS_1)
``` 
    * Are your conclusions about the time series the same as in 3. after interpreting these return plots? 

#### Answers (SPSS) {-}




#### Notes for `R` {-}

**Importing data in `R`**

By downloading:

1. Follow the link, e.g. for [`series.sav`](https://github.com/FredHasselman/The-Complex-Systems-Approach-Book/blob/master/assignments/assignment_data/BasicTSA_arma/series.sav).
2. On the Github page, find a button marked **Download** (or **Raw** for textfiles).
3. Download the file
4. Load it into `R`

```{r, eval=FALSE}
library(rio)
series <- import("series.sav", setclass = "tbl_df")
```


By importing from Github:

1. Copy the `url` associated with the **Download**  button on Github (right-clik).
2. The copied path should contain the word 'raw' somewhere in the url.
3. Call `rio::import(url)`

```{r}
library(rio)
series <- import("https://github.com/FredHasselman/The-Complex-Systems-Approach-Book/raw/master/assignments/assignment_data/BasicTSA_arma/series.sav", setclass = "tbl_df")
```

You can use the functions in the `stats` package: `arima()`, `acf()` and `pacf()` (`Matlab` has functions that go by slightly different names, check the [Matlab Help pages](https://nl.mathworks.com/help/econ/autocorr.html)). 

There are many extensions to these linear models, check the [`CRAN Task View` on `Time Series Analysis`](https://cran.r-project.org/web/views/TimeSeries.html) to learn more (e.g. about package `zoo` and `forecast`).



### Relative Roughness of the Heart

> "We can take it to the end of the line
Your love is like a shadow on me all of the time (all of the time)
I don't know what to do and I'm always in the dark
We're living in a powder keg and giving off sparks"
>
> --- Bonnie Tyler/James R. Steinman, Total Eclipse of the Heart


Download three different time series of heartbeat intervals (HBI) [here](https://github.com/FredHasselman/The-Complex-Systems-Approach-Book/tree/master/assignments/assignment_data/RelativeRoughness). If you use `R` and have package `rio` installed you can run this code and the load the data into a `data.frame` directly from `Github`.
```{r, echo=TRUE, eval=FALSE, include=TRUE}
library(rio)
TS1 <- rio::import("https://github.com/FredHasselman/The-Complex-Systems-Approach-Book/raw/master/assignments/assignment_data/RelativeRoughness/TS1.xlsx", col_names=FALSE)
TS2 <- rio::import("https://github.com/FredHasselman/The-Complex-Systems-Approach-Book/raw/master/assignments/assignment_data/RelativeRoughness/TS2.xlsx", col_names=FALSE)
TS3 <- rio::import("https://github.com/FredHasselman/The-Complex-Systems-Approach-Book/raw/master/assignments/assignment_data/RelativeRoughness/TS3.xlsx", col_names=FALSE)
```

The Excel files did not have any column names, so let's create them in the `data.frame`
```{r, eval=FALSE, include=TRUE}
colnames(TS1) <- "TS1"
colnames(TS2) <- "TS2"
colnames(TS3) <- "TS3"
```

### The recordings
These HBI???s were constructed from the R-R intervals in electrocardiogram (ECG) recordings, as defined in Figure \@ref(fig:RRf1).

```{r RRf1, fig.cap="Definition of Heart Beat Periods.", fig.align='center',echo=FALSE, include=TRUE}
knitr::include_graphics('images/RRfig1.png')
```


 * One HBI series is a sample from a male adult, 62 years old (called *Jimmy*). Approximately two years before the recording, the subject has had a coronary artery bypass, as advised by his physician following a diagnosis of congestive heart failure. *Jimmy* used antiarrhythmic medicines at the time of measurement.

 * Another HBI series is a sample from a healthy male adult, 21 years old (called *Tommy*). This subject never reported any cardiac complaint. Tommy was playing the piano during the recording.

 * A third supposed HBI series is fictitious, and was never recorded from a human subject (let???s call this counterfeit *Dummy*).
Your challenge

The assignment is to scrutinise the data and find out which time series belongs to *Jimmy*, *Tommy*, and *Dummy* respectively. ^[The HBI intervals were truncated (not rounded) to a multiple of 10 ms (e.g., an interval of 0.457s is represented as 0.450s), and to 750 data points each. The means and standard deviations among the HBI series are approximately equidistant, which might complicate your challenge.]


### First inspection
The chances that you are an experienced cardiologist are slim. We therefore suggest you proceed your detective work as follows:

*	Construct a graphical representation of the time series, and inspect their dynamics visually ( plot your time series).
* Write down your first guesses about which time series belongs to which subject. Take your time for this visual inspection (i.e., which one looks more like a line than a plane, which one looks more 'smooth' than 'rough').
*	Next, explore some measures of central tendency and dispersion, etc.
*	Third, compute the Relative Roughness for each time series, use Equation \@ref(eq:RR)

\begin{equation}
RR = 2\left[1???\frac{\gamma_1(x_i)}{Var(x_i)}\right]
(\#eq:RR)
\end{equation}

The numerator in the formula stands for the `lag 1` autocovariance of the HBI time series $x_i$. The denominator stands for the (global) variance of $x_i$. Most statistics packages can calculate these variances, `R` and `Matlab` have built in functions. Alternatively, you can create the formula yourself.

*	Compare your (intuitive) visual inspection with these preliminary dynamic quantifications, and find out where each of the HIB series are positions on the ???colorful spectrum of noises??? (i.e., line them up with Figure \@ref(fig:RRf3)).

```{r RRf3, fig.cap="Coloured Noise versus Relative Roughness", fig.align='center',echo=FALSE, include=TRUE}
knitr::include_graphics('images/RRfig3.png')
```


### What do we know now, that we didn???t knew before?
Any updates on Jimmy???s, Tommy???s and Dummy???s health? You may start to wonder about the 'meaning' of these dynamics, and not find immediate answers.

Don???t worry; we???ll cover the interpretation over the next two weeks in further depth. Let???s focus the dynamics just a little further for now. It might give you some clues.

* Use the `randperm` function (in `Matlab` or in package  [`pracma`](http://www.inside-r.org/packages/cran/pracma) in `R`) to randomize the temporal ordering of the HBI series.
* Visualize the resulting time series to check whether they were randomized successfully
* Next estimate the Relative Roughness of the randomized series. Did the estimates change compared to your previous outcomes (if so, why)?
* Now suppose you would repeat what you did the previous, but instead of using shuffle you would integrate the fictitious HBI series (i.e., normalize, then use `x=cumsum(x))`. You can look up `cumsum` in `R` or `Matlab`???s Help documentation). Would you get an estimate of Relative Roughness that is approximately comparable with what you got in another HBI series? If so, why?


### Sample Entropy


* Calculate the Sample Entropy
    + Use your favourite function to estimate the sample entropy of the three time series. Use for instance a segment length m of 3 datapoints, and a tolerance range r of 1 standard deviation. What values do you observe?
    +Can you change the absolute SampEn outcomes by 'playing' with the m parameter? If so, how does the outcome change, and why?
( Can you change the absolute SampEn outcomes by 'playing' with the r parameter If so, how does the outcome change, and why?
    + Do changes in the relative SampEn outcome change the outcomes for the three time series relative to each other?

*	Extra: Go back to the assignment where you generated simulated time series from the logistic map.





<!-- # **Fluctuation and Disperion analyses I** {#fda1} -->

<!-- ```{block2, L5, type='rmdimportant'} -->
<!-- Before you begin, look at the notes for [Lecture 4](#lecture-4). -->
<!-- ``` -->

<!-- ## The Spectral Slope {#psd} -->

<!-- We can use the power spectrum to estimate a **self-affinity parameter**, or scaling exponent. -->

<!-- * Download `ts1.txt`, `ts2.txt`, `ts3.txt` [here](https://github.com/FredHasselman/DCS/tree/master/assignmentData/Fluctuation_PSDslope). If you use `R` and have package `rio` installed you can run this code. It loads the data into a `data.frame` object directly from `Github`. -->
<!-- ```{r, echo=TRUE, eval=FALSE, include=TRUE} -->
<!-- library(rio) -->
<!-- TS1 <- rio::import("https://raw.githubusercontent.com/FredHasselman/DCS/master/assignmentData/Fluctuation_PSDslope/ts1.txt") -->
<!-- TS2 <- rio::import("https://raw.githubusercontent.com/FredHasselman/DCS/master/assignmentData/Fluctuation_PSDslope/ts2.txt") -->
<!-- TS3 <- rio::import("https://raw.githubusercontent.com/FredHasselman/DCS/master/assignmentData/Fluctuation_PSDslope/ts3.txt") -->

<!-- # These objects are now data.frames with one column named V1. -->
<!-- # If you want to change the column names -->
<!-- colnames(TS1) <- "TS1" -->
<!-- colnames(TS2) <- "TS2" -->
<!-- colnames(TS3) <- "TS3" -->
<!-- ``` -->

<!-- * Plot the three 'raw' time series. -->

<!-- ### Basic data checks and preparations -->

<!-- For spectral analysis we need to check some data assumptions (see [notes on data preparation, Lecture 4](#data-considerations)). -->

<!-- #### Normalize {-} -->
<!-- 1. Are the lengths of the time series a power of 2? (Use `log2(length of var)` ) -->
<!--   + Computation of the frequency domain is greatly enhanced if data length is a power (of 2). -->
<!-- 2. Are the data normalized? (we will *not* remove datapoints outside 3SD) -->
<!--     + To normalize we have to subtract the mean from each value in the time series and divide it by the standard deviation, the function `scale()` can do this for you, but you could also use `mean()` and `sd()` to construct your own function. -->
<!-- 3. Plot the normalized time series. -->

<!-- #### Detrend {-} -->
<!-- Before a spectral analysis you should remove any linear trends (it cannot deal with nonstationary signals!) -->

<!-- 1. Detrend the normalized data (just the linear trend). -->
<!--     + This can be done using the function `pracma::detrend()`. -->
<!--     + Extra: Try to figure out how to detrend the data using `stats::lm()` or `stats::poly()` -->
<!-- 2. Plot the detrended data. -->

<!-- #### Get the log-log slope in Power Spectral Density {-} -->
<!-- The function `fd.psd()` will perform the spectral slope fitting procedure. -->

<!-- 1. Look at the manual pages to figure out how to call the function. The manual is on blackboard and [Github](https://github.com/FredHasselman/DCS/blob/master/functionLib/) -->
<!--     + Remember, we have already normalized and detrended the data. -->
<!--     + You can also look at the code itself by selecting the function name in`R` and pressing `F2` -->
<!-- 2. Calculate the spectral slopes for the three normalized and detrended time series. -->
<!--     + Call with `plot = TRUE` -->
<!--     + Compare the results... What is your conclusion? -->


<!-- ## DFA and SDA {#dfa} -->

<!-- * Use the functions `fd.dfa()` and `fd.sda()` to estimate the self-affinity parameter and Dimension of the series. -->
<!--     + Check what kind of data preparation is required for SDA and DFA in [notes on data preparation, Lecture 4](#data-considerations). -->
<!--     + Compare the results between the three different methods. -->

<!-- [| jump to solution |](#dfasol) -->

<!-- ## Heartbeat dynamics II {#hrv2} -->
<!-- In the [previous assignment](#relR), you were presented with three different time series of heartbeat intervals (HBI), and you analyzed them using a measure of Relative Roughness (RR; cf. Marmelat & Deligni??res, 2012). -->

<!-- A logical step is to unleash the full force of your new analytic toolbox onto the HBI series. -->

<!-- * Keep track of the outcomes of each time series for 4 different analyses (RR, PSD, SDA, DFA). -->
<!--     + Do the outcomes of the different methods converge on the continuum of blue, white and pink, to Brownian and black noise? That is, do they indicate the same type of temporal structure? -->
<!-- * As a final step, construct return plots for each time series and try to interpret what you observe, given the outcomes of the scaling parameter estimates. -->


<!-- ## Analysis of Deterministic Chaos {#chaos} -->

<!-- * Generate a chaotic timeseries (e.g. $r = 4$ ) of equal length as the time series used above (use the function `growth.ac( ..., type = "logistic")` in `nlRtsa_SOURCE`, see the [solutions of Lecture 1 and 2](#linear-and-logistic-growth)) -->
<!--     + This is in fact one of the series you analysed in a [previous assignment](#pacf). If you still have the results use them for the next part. -->
<!-- * Get all the scaling quantities for this series as well as the ACF and PACF and [some return plots](#the-return-plot) just like in the previous assignments. -->
<!--     + Compare the results to e.g. the heartbeat series. -->

<!-- [| jump to solution |](#chaossol) -->

<!-- # Fluctuation and Disperion analyses II {#fda2} -->

<!-- There were no assignments for this Lecture. -->


<!--chapter:end:ASSIGNMENTS_P2_t.Rmd-->

---
title: "DCS Assignments Session 3"
author: "Fred Hasselman & Maarten Wijnants"
date: "1/14/2019"
output: 
  bookdown::html_document2: 
    variant: markdown+hard_line_breaks
    fig_caption: yes
    highlight: pygments
    keep_md: yes
    self_contained: yes
    number_sections: yes
    theme: spacelab
    toc: yes
    toc_float: true
    collapsed: false
    smooth_scroll: true
    code_folding: show
    bibliography: [refs.bib, packages.bib]
    biblio-style: apalike
    csl: apa.csl
    includes:
        before_body: assignmentstyle.html
    pandoc_args: ["--number-offset","1"]
editor_options: 
  chunk_output_type: console
---


```{r, include=FALSE}
knitr::opts_chunk$set(include=TRUE, tidy = FALSE)
```


# **Quick Links** {-}

* [Main Assignments Page](https://darwin.pwo.ru.nl/skunkworks/courseware/1819_DCS/assignments/)

<!-- * [Assignments Part 1A: Introduction to the mathematics of change](https://darwin.pwo.ru.nl/skunkworks/courseware/1718_DCS/assignments/ASSIGNMENTS_P1A.html) -->
<!-- * [Assignments Part 2: Time Series Analysis: Temporal Correlations and Fractal Scaling](https://darwin.pwo.ru.nl/skunkworks/courseware/1718_DCS/assignments/ASSIGNMENTS_P2.html) -->
<!-- * [Assignments Part 3: Quantifying Recurrences in State Space](https://darwin.pwo.ru.nl/skunkworks/courseware/1718_DCS/assignments/ASSIGNMENTS_P3.html) -->
<!-- * [Assignments Part 4: Complex Networks](https://darwin.pwo.ru.nl/skunkworks/courseware/1718_DCS/assignments/ASSIGNMENTS_P4.html) -->

<!-- </br> -->
<!-- </br> -->


# **Fitting Parameters of Analytic Solutions [EXTRA]**


## **Nonlinear Growth curves**


### Fitting the Logistic Growth model in SPSS {.tabset .tabset-fade .tabset-pills}

#### Questions {-}

Open the file [Growthregression.sav](https://github.com/FredHasselman/The-Complex-Systems-Approach-Book/blob/master/assignments/assignment_data/BasicTSA_nonlinreg/GrowthRegression.sav), it contains two variables: `Time` and `Y(t)`. 

This is data from an iteration of the logistic growth differential equation you are familiar with by now, but lets pretend its data from one subject measured on 100 occasions.

* Plot Y(t) against Time Recognize the shape?
* To get the growth parameter well try to fit the solution of the logistic flow with SPSS non-linear regression
     + Select non-linear from the `Analysis` >> `Regression` menu.
     + Here we can build the solution equation. We need three parameters:
            a. **Yzero**, the initial condition.
            b. *K*, the carrying capacity.
            c. *r*, the growth rate.
     + Fill these in where it says `parameters` give all parameters a starting value of  $0.01$

Take a good look at the analytic solution of the (stilized) logistic flow:

$$
Y(t)  =  \frac{K * Y_0}{Y_0 + \left(K-Y_{0}\right) * e^{-r*t} }
$$

Try to build this equation, the function for $e$ is called `EXP` in `SPSS` (`Function Group` >> `Arithmetic`)
Group terms by using parentheses as shown in the equation.

* If you think you have built the model correctly, click on `Save` choose `predicted values`. Then paste your syntax and run it!
    + Check the estimated parameter values.
    + Check $R^2$!!!

* Plot a line graph of both the original data and the predicted values. (Smile)

You can try do the same using a polynomial approach.

* A polynomial fishing expedition:
    + Create time-varying 'covariates' of $Y(t)$:
```
COMPUTE T1=Yt * Time.
COMPUTE T2=Yt * (Time ** 2). 
COMPUTE T3=Yt * (Time ** 3). 
COMPUTE T4=Yt * (Time ** 4). 
EXECUTE.
```
    + Use these variables as predictors of $Y(t)$ in a regular linear regression analysis. This is called a *polynomial regression*: Fitting combinations of curves of different shapes on the data.
    + Before you run the analysis: Click `Save` Choose `Predicted Values: Unstandardized`

```{block2, type = "rmdselfthink"}
Look at $R^2$. This is also almost 1!

Which model is better? 

Think about this: Based on the results of the linear regression what can you tell about the *growth rate*, the *carrying capacity* or the *initial condition*?
```

*	Create a line graph of $Y(t)$, plot the predicted values of the non-linear regression and the unstandardised predicted values of the linear polynomial regression against `time` in one figure.
    + Now you can see that the shape is approximated by the polynomials, but it is not quite the same. Is this really a model of a growth process as we could encounter it in nature?


#### Answers {-}

The solutions are also provided as an `SPSS` [syntax file](https://github.com/FredHasselman/The-Complex-Systems-Approach-Book/blob/master/assignments/assignment_data/BasicTSA_nonlinreg/GrowthRegression_sol.sps) file.

Or copy the block below:
```
GRAPH
  /LINE(SIMPLE)=VALUE(Yt) BY Time.

* NonLinear Regression.
MODEL PROGRAM  Yzero=0.01 r=0.01 K=0.01.
COMPUTE  PRED_=K*Yzero/(Yzero + (K-Yzero) * EXP(-1*(r * Time))).
NLR Yt
  /PRED PRED_
  /SAVE PRED
  /CRITERIA SSCONVERGENCE 1E-8 PCON 1E-8.

GRAPH
  /LINE(MULTIPLE)=VALUE(Yt PRED_) BY Time.

COMPUTE T1=Yt * Time.
COMPUTE T2=Yt * (Time ** 2).
COMPUTE T3=Yt * (Time ** 3).
COMPUTE T4=Yt * (Time ** 4).
EXECUTE.

REGRESSION
  /MISSING LISTWISE
  /STATISTICS COEFF OUTS R ANOVA
  /CRITERIA=PIN(.05) POUT(.10)
  /NOORIGIN 
  /DEPENDENT Yt
  /METHOD=ENTER T1 T2 T3 T4
  /SAVE PRED.


GRAPH
  /LINE(MULTIPLE)=VALUE(Yt PRED_ PRE_1) BY Time.
```

* The point here is that the polynomial regression approach is "just" curve fitting ... adding components until a nice fit is found ... but what does component $Y_t^4$ represent? A quartic sub-process?
* Fitting the solution of the the logistic function will give us parameters we can interpret unambiguously: Carrying capacity, growth rate, starting value.


### Fitting the Logistic Growth model in R {.tabset .tabset-fade .tabset-pills}

There are several packages that can perform non-linear regression analysis, the function most resembling the approach used by `SPSS` is `nls` in the default `stats` package.  

The easiest way to do this is to first define your function (i.e., the solution) and then fit it using starting values for the parameters.

```{r, cache=FALSE}
library(rio)
df <- import("https://github.com/FredHasselman/The-Complex-Systems-Approach-Book/raw/master/assignments/assignment_data/BasicTSA_nonlinreg/GrowthRegression.sav", setclass = "tbl_df")

# Create the function for the analytic solution
# Same as SPSS syntax: PRED_=K*Yzero/(Yzero + (K-Yzero) * EXP(-1*(r * Time))).
log.eq <- function(Yzero, r, K, Time) {
    K*Yzero/(Yzero + (K-Yzero) * exp(-1*(r * Time)))
}
```

There is one drawback and you can read about in the help pages:

> Warning. Do not use nls on artificial "zero-residual" data.

This means, "do not use it on data generated by a deterministic model which has no residual error", which is exactly what the time series in this assignment is, it is the output of the logistic flow.

So, this will give an error:
```{r, echo=TRUE, message=TRUE, warning=TRUE, cache=FALSE, eval=FALSE}
# Fit this function ... gives an error
# The list after 'start' provides the initial values
m.log <- nls(Yt ~ log.eq(Yzero, r, K, Time), data = df, start = list(Yzero=.01, r=.01, K=1), trace = TRUE)
```



We'll provide you the answers right away, but be sure to figure out what is going on.



#### Answers{-}

It is possible to fit these ideal data using package `minpack.lm`, which contains function `nlsM`.

```{r}
library(minpack.lm)

m.log <- nlsLM(Yt ~ log.eq(Yzero, r, K, Time), data = df, start = list(Yzero = .01, r=.01, K=0.1))

summary(m.log)
```

In order to look at the model prediction, we use `predict()` which is defined for almost all model fitting functions in `R`
```{r}
Ypred <- predict(m.log)

plot(ts(df$Yt), col="gray40", lwd=5, ylab = ("Yt | Ypred"))
lines(Ypred, col="gray80", lwd=2, lty=2)
```

Then we do a polynomial regression using `lm`:
```{r, echo=TRUE, message=FALSE, warning=FALSE}
# Mimic the SPSS syntax
attach(df)
  df$T1 <- Yt * Time
  df$T2 <- Yt * (Time^2) 
  df$T3 <- Yt * (Time^3) 
  df$T4 <- Yt * (Time^4)
detach(df)

m.poly <- lm(Yt ~ T1 + T2 + T3 + T4, data=df)
summary(m.poly)
```

Then, predict and plot!
```{r}
Ypoly <- predict(m.poly)

plot(ts(Ypoly), col="blue1", lwd=2, ylab = ("Ypoly (blue) | Ypred (red)"))
lines(Ypred, col="red1", lwd=2)
```

SPSS computes an $r^2$ value for non-linear regression models, which doesn't make a lot of sense if you think about it. Here we van just compare the residual errors:

* Polynomial regression: $0.005506$
* Analytic solution: $0.002865$

Slightly less residual error for the analytic solution, using less parameters to fit the model (3 vs. 5). **More important:**, the parameters of the analytic solution have a direct interpretation in terms of growth processes.



# **Basic Timeseries Analysis** 

In this course we will not discuss the type of linear time series models known as Autoregressive Models (e.g. AR, ARMA, ARiMA, ARfiMA) summarised on [this Wikipedia page on timeseries](https://en.wikipedia.org/wiki/Time_series#Models). We will in fact be discussing a lot of methods in a book the Wiki page refers to for *'Further references on nonlinear time series analysis'*: [**Nonlinear Time Series Analysis** by Kantz & Schreiber](https://www.cambridge.org/core/books/nonlinear-time-series-analysis/519783E4E8A2C3DCD4641E42765309C7). You do not need to buy the book, but it can be a helpful reference if you want to go beyond the formal level (= mathematics) used in this course. Some of the packages we use are based on the companying software [**TiSEAN**](https://www.pks.mpg.de/~tisean/Tisean_3.0.1/index.html) which is written in `C` and `Fortran` and can be called from the command line (Windows / Linux).


## **Correlation functions**  

Correlation functions are intuitive tools for quantifying the temporal structure in a time series. As you know, correlation can only quantify linear regularities between variables, which is why we discuss them here as `basic` tools for time series analysis. So what are the variables? In the simplest case, the variables between which we calculate a correlation are between a data point at time *t* and a data point that is separated in time by some *lag*, for example, if you would calculate the correlation in a lag-1 return plot, you would have calculated the 1st value of the correlation function (actually, it is 2nd value, the 1st value is the correlation of time series with itself, the lag-0 correlation, which is of course $r = 1$)  

### ACF and PCF {.tabset .tabset-fade .tabset-pills}


You can do the analyses in SPSS or in `R`, but this analysis is very common so you'll find functions called `acf`, `pacf`and `ccf` in many other statistical software packages, In package `casnet` you can use the function `plotRED_acf()` (plot REDundancies).


#### Questions (SPSS/R) {-}

* Download the file [`series.sav`](https://github.com/FredHasselman/The-Complex-Systems-Approach-Book/blob/master/assignments/assignment_data/BasicTSA_arma/series.sav) from Github. 

It contains three time series `TS_1`, `TS_2` and `TS_3`. As a first step look at the mean and the standard deviation (`Analyze` >> `Descriptives`).  Suppose these were time series from three subjects in an experiment, what would you conclude based on the means and SDs?  

* Lets visualize these data. Go to `Forecasting` >> `Time Series` >> `Sequence Charts`. Check the box One chart per variable and move all the variables to Variables. Are they really the same?  

* Lets look at the `ACF` and `PCF`
    + Go to `Analyze` >> `Forecasting` >> `Autocorrelations`. 
    + Enter all the variables and make sure both *Autocorrelations* (ACF) and *Partial autocorrelations* (PACF) boxes are checked. Click `Options`, and change the `Maximum Number of Lags` to `30`. 
    + Use the table to characterize the time series:  


|                    SHAPE                | INDICATED MODEL |
|-----------------------------------------|-------------------------------------------------------------------------------------------------|
|       Exponential, decaying to zero     | Autoregressive model. Use the partial autocorrelation plot to identify the order of the autoregressive model|
| Alternating positive and negative, decaying to zero  | Autoregressive model. Use the partial autocorrelation plot to help identify the order.|
| One or more spikes, rest are essentially zero | Moving average model, order identified by where plot becomes zero. |
| Decay, starting after a few lags | Mixed autoregressive and moving average model.|
All zero or close to zero  | Data is essentially random.|
| High values at fixed intervals | Include seasonal autoregressive term. |
| No decay to zero  | Series is not stationary. |


* In the simulation part of this course we have learned a very simple way to explore the dynamics of a system: The return plot. The time series is plotted against itself shifted by 1 step in time. 

* Create return plots (use a Scatterplot) for the three time series. Tip: You can easily create a `t+1` version of the time series by using the LAG function in a `COMPUTE` statement. For instance:

```
COMPUTE TS_1_lag1 = LAG(TS_1)
```
    
* Are your conclusions about the time series based on interpreting these return plots the same as based on the `acf` and `pacf`?
     
     
#### Answers (SPSS) {-}

If you run this syntax in `SPSS` you'll get the correct output.

```
DESCRIPTIVES
  VARIABLES=TS_1 TS_2 TS_3
  /STATISTICS=MEAN STDDEV MIN MAX .

*Sequence Charts .
TSPLOT VARIABLES= TS_1
  /NOLOG
  /FORMAT NOFILL REFERENCE.
TSPLOT VARIABLES= TS_2
  /NOLOG
  /FORMAT NOFILL REFERENCE.
TSPLOT VARIABLES= TS_3
  /NOLOG
  /FORMAT NOFILL REFERENCE.

*ACF and PCF.

ACF
  VARIABLES= TS_1 TS_2 TS_3
  /NOLOG
  /MXAUTO 30
  /SERROR=IND
  /PACF.


*Return plots.

COMPUTE TS_1_lag1 = LAG(TS_1) .
COMPUTE TS_2_lag1 = LAG(TS_2) .
COMPUTE TS_3_lag1 = LAG(TS_3) .
EXECUTE .


IGRAPH /VIEWNAME='Scatterplot' /X1 = VAR(TS_1_lag1) TYPE = SCALE /Y =
  VAR(TS_1) TYPE = SCALE /COORDINATE = VERTICAL  /X1LENGTH=3.0 /YLENGTH=3.0
  /X2LENGTH=3.0 /CHARTLOOK='NONE' /SCATTER COINCIDENT = NONE.
EXE.

IGRAPH /VIEWNAME='Scatterplot' /X1 = VAR(TS_2_lag1) TYPE = SCALE /Y =
  VAR(TS_2) TYPE = SCALE /COORDINATE = VERTICAL  /X1LENGTH=3.0 /YLENGTH=3.0
  /X2LENGTH=3.0 /CHARTLOOK='NONE' /SCATTER COINCIDENT = NONE.
EXE.

IGRAPH /VIEWNAME='Scatterplot' /X1 = VAR(TS_3_lag1) TYPE = SCALE /Y =
  VAR(TS_3) TYPE = SCALE /COORDINATE = VERTICAL  /X1LENGTH=3.0 /YLENGTH=3.0
  /X2LENGTH=3.0 /CHARTLOOK='NONE' /SCATTER COINCIDENT = NONE.
EXE.

```


#### Notes for `R` {-}

If you want to use `R`, just go through the questions and ignore the `SPSS` specific comments. Here are some tips:

**Importing data in `R`**

By downloading:

1. Follow the link, e.g. for [`series.sav`](https://github.com/FredHasselman/The-Complex-Systems-Approach-Book/blob/master/assignments/assignment_data/BasicTSA_arma/series.sav).
2. On the Github page, find a button marked **Download** (or **Raw** for textfiles).
3. Download the file
4. Load it into `R`

```{r, eval=FALSE}
library(rio)
series <- import("series.sav", setclass = "tbl_df")
```


By importing from Github:

1. Copy the `url` associated with the **Download**  button on Github (right-clink).
2. The copied path should contain the word 'raw' somewhere in the url.
3. Call `rio::import(url)`
```{r}
library(rio)
series <- import("https://github.com/FredHasselman/The-Complex-Systems-Approach-Book/raw/master/assignments/assignment_data/BasicTSA_arma/series.sav", setclass = "tbl_df")
```

You can use the functions in the `stats` package: `arima()`, `acf()` and `pacf()`. In package `casnet` you can use the function `plotRED_acf()` (plot REDundancies). `Matlab` has functions that go by slightly different names, check the [Matlab Help pages](https://nl.mathworks.com/help/econ/autocorr.html). 

There are many extensions to these linear models, check the [`CRAN Task View` on `Time Series Analysis`](https://cran.r-project.org/web/views/TimeSeries.html) to learn more (e.g. about package `zoo` and `forecast`).

```{r}
library(casnet)
library(ggplot2)

plotRED_acf(y = series$TS_1,Lmax = 50, returnCorFun = FALSE)

plotRED_acf(y = series$TS_2,Lmax = 50, returnCorFun = FALSE)

plotRED_acf(y = series$TS_3,Lmax = 50, returnCorFun = FALSE)

```



### Relative Roughness of the Heart {.tabset .tabset-fade .tabset-pills}

> "We can take it to the end of the *line*     
>  Your love is like a shadow on me all of the *time*     
>  I don't know what to do and I'm always in the dark     
>  We're living in a powder keg and giving off *sparks*"     
>
> --- Bonnie Tyler/James R. Steinman, Total Eclipse of the Heart


Download three different time series of heartbeat intervals (HBI) [here](https://github.com/FredHasselman/The-Complex-Systems-Approach-Book/tree/master/assignments/assignment_data/RelativeRoughness). If you use `R` and have package `rio` installed you can run this code and the load the data into a `data.frame` directly from `Github`.

```{r, echo=TRUE, eval=FALSE, include=TRUE}
library(rio)
TS1 <- rio::import("https://github.com/FredHasselman/The-Complex-Systems-Approach-Book/raw/master/assignments/assignment_data/RelativeRoughness/TS1.xlsx", col_names=FALSE)
TS2 <- rio::import("https://github.com/FredHasselman/The-Complex-Systems-Approach-Book/raw/master/assignments/assignment_data/RelativeRoughness/TS2.xlsx", col_names=FALSE)
TS3 <- rio::import("https://github.com/FredHasselman/The-Complex-Systems-Approach-Book/raw/master/assignments/assignment_data/RelativeRoughness/TS3.xlsx", col_names=FALSE)
```

The Excel files did not have any column names, so let's create them in the `data.frame`
```{r, eval=FALSE, include=TRUE}
colnames(TS1) <- "TS1"
colnames(TS2) <- "TS2"
colnames(TS3) <- "TS3"
```


**The recordings**
These HBIs were constructed from the[] R-R inter-beat intervals](https://www.physionet.org/tutorials/hrv/) in electrocardiogram (ECG) recordings, as defined in Figure \@ref(fig:RRf1).

```{r RRf1, fig.cap="Definition of Heart Beat Periods.", fig.align='center',echo=FALSE, include=TRUE}
knitr::include_graphics('images/RRfig1.png')
```


 * One HBI series is a sample from a male adult, 62 years old (called *Jimmy*). Approximately two years before the recording, the subject has had a coronary artery bypass, as advised by his physician following a diagnosis of congestive heart failure. *Jimmy* used anti-arrhythmic medicines at the time of measurement.

 * Another HBI series is a sample from a healthy male adult, 21 years old (called *Tommy*). This subject never reported any cardiac complaint. Tommy was playing the piano during the recording.

 * A third supposed HBI series is fictitious, and was never recorded from a human subject (lets call this counterfeit *Dummy*).


#### Questions {-}

The assignment is to scrutinise the data and find out which time series belongs to *Jimmy*, *Tommy*, and *Dummy* respectively. ^[The HBI intervals were truncated (not rounded) to a multiple of 10 ms (e.g., an interval of 0.457s is represented as 0.450s), and to 750 data points each. The means and standard deviations among the HBI series are approximately equidistant, which might complicate your challenge.]

The chances that you are an experienced cardiologist are slim. We therefore suggest you proceed your detective work as follows:

*	Construct a graphical representation of the time series, and inspect their dynamics visually ( plot your time series).
* Write down your first guesses about which time series belongs to which subject. Take your time for this visual inspection (i.e., which one looks more like a line than a plane, which one looks more 'smooth' than 'rough').
*	Next, explore some measures of central tendency and dispersion, etc.
*	Third, compute the Relative Roughness for each time series, use Equation \@ref(eq:RR)

\begin{equation}
RR = 2\left[1\frac{\gamma_1(x_i)}{Var(x_i)}\right]
(\#eq:RR)
\end{equation}

The numerator in the formula stands for the `lag 1` auto-covariance of the HBI time series $x_i$. The denominator stands for the (global) variance of $x_i$. Most statistics packages can calculate these variances, `R` and `Matlab` have built in functions. Alternatively, you can create the formula yourself.

*	Compare your (intuitive) visual inspection with these preliminary dynamic quantifications, and find out where each of the HIB series are positions on the colourful spectrum of noises (i.e., line them up with Figure \@ref(fig:RRf3)).

```{r RRf3, fig.cap="Coloured Noise versus Relative Roughness", fig.align='center',echo=FALSE, include=TRUE}
knitr::include_graphics('images/RRfig3.png')
```


**What do we know now, that we didnt knew before?**
Any updates on Jimmys, Tommys and Dummys health? You may start to wonder about the 'meaning' of these dynamics, and not find immediate answers.

Dont worry; well cover the interpretation over the next two weeks in further depth. Lets focus the dynamics just a little further for now. It might give you some clues.

* Use the `randperm` function (in `Matlab` or in package  [`pracma`](http://www.inside-r.org/packages/cran/pracma) in `R`) to randomize the temporal ordering of the HBI series. Or try to use the function `sample()`
* Visualize the resulting time series to check whether they were randomized successfully
* Next estimate the Relative Roughness of the randomized series. Did the estimates change compared to your previous outcomes (if so, why)?
* Now suppose you would repeat what you did the previous, but instead of using shuffle you would integrate the fictitious HBI series (i.e., normalize, then use `x=cumsum(x))`. You can look up `cumsum` in `R` or `Matlab`s Help documentation). Would you get an estimate of Relative Roughness that is approximately comparable with what you got in another HBI series? If so, why?


#### Answers {-}

```{r, echo=TRUE, eval=TRUE, include=TRUE}
library(rio)
TS1 <- rio::import("https://github.com/FredHasselman/The-Complex-Systems-Approach-Book/raw/master/assignments/assignment_data/RelativeRoughness/TS1.xlsx", col_names=FALSE)
TS2 <- rio::import("https://github.com/FredHasselman/The-Complex-Systems-Approach-Book/raw/master/assignments/assignment_data/RelativeRoughness/TS2.xlsx", col_names=FALSE)
TS3 <- rio::import("https://github.com/FredHasselman/The-Complex-Systems-Approach-Book/raw/master/assignments/assignment_data/RelativeRoughness/TS3.xlsx", col_names=FALSE)
```

The Excel files did not have any column names, so let's create them in the `data.frame`
```{r, eval=TRUE, include=TRUE}
colnames(TS1) <- "TS1"
colnames(TS2) <- "TS2"
colnames(TS3) <- "TS3"
```

```{r, echo=TRUE, include=TRUE, eval=TRUE}
# Create a function for RR
RR <- function(ts){
  # lag.max = n gives autocovariance of lags 0 ... n,
  VAR  <- acf(ts, lag.max = 1, type = 'covariance', plot=FALSE)
  # RR formula
  RelR   <- 2*(1-VAR$acf[2] / VAR$acf[1])
  # Add some attributes to the output
  attributes(RelR) <- list(localAutoCoVariance = VAR$acf[2], globalAutoCoVariance = VAR$acf[1])
  return(RelR)
}

# Look at the results
for(ts in list(TS1,TS2,TS3)){
  relR <- RR(ts[,1])
  cat(paste0(colnames(ts),": RR = ",round(relR,digits=3), " = 2*(1-",
         round(attributes(relR)$localAutoCoVariance, digits = 4),"/",
         round(attributes(relR)$globalAutoCoVariance,digits = 4),")\n"))
  }

# You can also use function fd_RR() from casnet
fd_RR

```

Use Figure \@ref(fig:RRf3) to lookup which value of $RR$ corresponds to which type of dynamics:

**TS1**: Pink noise
**TS2**: Brownian noise
**TS3**: White noise



**Randomize**

To randomize the data you may use the function `sample` (which is easier than `randperm`)

```{r}
library(pracma)
# randperm()
TS1Random <- TS1$TS1[randperm(length(TS1$TS1))]

# sample()
TS1Random <- sample(TS1$TS1, length(TS1$TS1))
TS2Random <- sample(TS2$TS2, length(TS2$TS2))
TS3Random <- sample(TS3$TS3, length(TS3$TS3))


plot.ts(TS1Random)
lines(ts(TS1$TS1),col="red3")
```

If you repeat this for TS2 and TS3 and compute the Relative Roughness of each randomized time series, the outcomes should be around 2, white noise! This makes sense, you destroyed all the correlations in the data by removing the temporal order with which values were observed.

```{r}
library(casnet)
cat("TS1random\n")
cat(fd_RR(TS1Random))
cat("\nTS2random\n")
cat(fd_RR(TS2Random))
cat("\nTS3random\n")
cat(fd_RR(TS3Random))
```

**Integrate**

Normalize the white noise time series
```{r}
TS3Norm <- scale(TS3$TS3)
```

Now integrate it, which just means, 'take the cumulative sum'.
```{r}
TS3Int <- cumsum(TS3Norm)
plot.ts(TS3Int)
lines(ts(TS3Norm),col="red3")
```

If you compute the Relative Roughness of the integrated time series, the outcome should be close to 0, Brownian noise.
```{r}
cat("\nTS3Int\n")
cat(RR(TS3Int))
```


### Sample Entropy  {.tabset .tabset-fade .tabset-pills}

Use the `sample_entropy()` function in package `pracma`.

#### Questions {-}

* Calculate the Sample Entropy of the two sets of three time series you now have.
    + Use your favourite function to estimate the sample entropy of the three time series. Use for instance a segment length `edim` of 3 data points, and a tolerance range `r` of 1 * the standard deviation of the series. What values do you observe?
    + Can you change the absolute SampEn outcomes by 'playing' with the m parameter? If so, how does the outcome change, and why?
    + Can you change the absolute SampEn outcomes by 'playing' with the r parameter If so, how does the outcome change, and why?
    + Do changes in the relative SampEn outcome change the outcomes for the three time series relative to each other?

*	Extra: Go back to the assignment where you generated simulated time series from the logistic map.


#### Answers {-}

Change some of the parameters.

```{r,collapse=TRUE}
library(rio)
library(pracma)

# ACF assignment data `series`
cat(paste0("\nseries.TS1 m=3\n",sample_entropy(series$TS_1, edim = 3, r = sd(series$TS_1))))
cat(paste0("\nseries.TS2 m=3\n",sample_entropy(series$TS_2, edim = 3, r = sd(series$TS_2))))
cat(paste0("\nseries.TS3 m=3\n",sample_entropy(series$TS_3, edim = 3, r = sd(series$TS_3))))


# ACF assignment data `series`
cat(paste0("\nseries.TS1 m=6\n",sample_entropy(series$TS_1, edim = 6, r = sd(series$TS_1))))
cat(paste0("\nseries.TS2 m=6\n",sample_entropy(series$TS_2, edim = 6, r = sd(series$TS_2))))
cat(paste0("\nseries.TS3 m=6\n",sample_entropy(series$TS_3, edim = 6, r = sd(series$TS_3))))


# ACF assignment data `series`
cat(paste0("\nseries.TS1 m=6, r=.5\n",sample_entropy(series$TS_1, edim = 3, r = .5*sd(series$TS_1))))
cat(paste0("\nseries.TS2 m=6, r=.5\n",sample_entropy(series$TS_2, edim = 3, r = .5*sd(series$TS_2))))
cat(paste0("\nseries.TS3 m=6, r=.5\n",sample_entropy(series$TS_3, edim = 3, r = .5*sd(series$TS_3))))

```

The change of `m` keeps the relative order, change of `r` for the same `m` does not.

**Values of other time series**

```{r message=FALSE, warning=FALSE, collapse=TRUE}

# RR assignment data `TS1,TS2,TS3`
cat(paste0("\nTS1\n",sample_entropy(TS1$TS1, edim = 3, r = sd(TS1$TS1))))
cat(paste0("TS1Random\n",sample_entropy(TS1Random, edim = 3, r = sd(TS1Random))))

cat(paste0("\nTS2\n",sample_entropy(TS2$TS2, edim = 3, r = sd(TS2$TS2))))
cat(paste0("TS2Random\n",sample_entropy(TS2Random, edim = 3, r = sd(TS2Random))))

cat(paste0("\nTS3\n",sample_entropy(TS3$TS3, edim = 3, r = sd(TS3$TS3))))
cat(paste0("TS3Int\n",sample_entropy(TS3Norm, edim = 3, r = sd(TS3Norm))))


# Logistic map
library(casnet)
cat("\nLogistic map\nr=2.9\n")
y1<-growth_ac(r = 2.9,type="logistic")
sample_entropy(y1, edim = 3, r = sd(y1))
cat("\nLogistic map\nr=4\n")
y2<-growth_ac(r = 4,type="logistic")
sample_entropy(y2, edim = 3, r = sd(y2))


```



<!--chapter:end:ASSIGNMENTS_P2.Rmd-->

---
title: "DCS Assignments Session 4"
author: "Fred Hasselman & Maarten Wijnants"
date: "1/14/2019"
output: 
  bookdown::html_document2: 
    variant: markdown+hard_line_breaks
    fig_caption: yes
    highlight: pygments
    keep_md: yes
    self_contained: yes
    number_sections: yes
    theme: spacelab
    toc: yes
    toc_float: true
    collapsed: false
    smooth_scroll: true
    code_folding: show
    bibliography: [refs.bib, packages.bib]
    biblio-style: apalike
    csl: apa.csl
    includes:
        before_body: assignmentstyle.html
    pandoc_args: ["--number-offset","4"]
editor_options: 
  chunk_output_type: console
---


```{r, include=FALSE}
knitr::opts_chunk$set(include=TRUE, tidy = FALSE)
```


# **Quick Links** {-}

* [Main Assignments Page](https://darwin.pwo.ru.nl/skunkworks/courseware/1718_DCS/assignments/)


## **Fluctuation analyses**

Install package `casnet` to use the functions `fd_sda()`, `fd_psd()` and `fd_dfa()` and `plotFD_loglog()`.
Check the manual pages for these functions.

```{r,eval=TRUE, message=FALSE, warning=FALSE, eval=TRUE}
# Install casnet if necessary: https://github.com/FredHasselman/casnet
# !!Warning!! Beta version...
# library(devtools)
# devtools::install_github("FredHasselman/invctr")
# devtools::install_github("FredHasselman/casnet")
library(invctr)
library(casnet)
library(rio)
library(plyr)
library(dplyr)
library(ggplot2)
```


We'll use the data from last week:

```{r eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE, include=TRUE}
library(rio)
library(pracma)

# Reload the data
series <- rio::import("https://github.com/FredHasselman/The-Complex-Systems-Approach-Book/raw/master/assignments/assignment_data/BasicTSA_arma/series.sav", setclass = "tbl_df")

TS1 <- rio::import("https://github.com/FredHasselman/The-Complex-Systems-Approach-Book/raw/master/assignments/assignment_data/RelativeRoughness/TS1.xlsx", col_names=FALSE)
TS2 <- rio::import("https://github.com/FredHasselman/The-Complex-Systems-Approach-Book/raw/master/assignments/assignment_data/RelativeRoughness/TS2.xlsx", col_names=FALSE)
TS3 <- rio::import("https://github.com/FredHasselman/The-Complex-Systems-Approach-Book/raw/master/assignments/assignment_data/RelativeRoughness/TS3.xlsx", col_names=FALSE)


# The Excel files did not have any column names, so let's create them in the `data.frame`
colnames(TS1) <- "TS1"
colnames(TS2) <- "TS2"
colnames(TS3) <- "TS3"

# randperm()
TS1Random <- TS1$TS1[randperm(length(TS1$TS1))]

# sample()
TS1Random <- sample(TS1$TS1, length(TS1$TS1))
TS2Random <- sample(TS2$TS2, length(TS2$TS2))
TS3Random <- sample(TS3$TS3, length(TS3$TS3))

TS3Norm <- scale(TS3$TS3)

```



###  Standardised Dispersion Analysis {.tabset .tabset-fade .tabset-pills}


#### Questions {-}

* Common data preparation before running fluctuation analyses like `SDA`:
    + Normalize the time series (using `sd` based on `N`, not `N-1`, e.g. by using `ts_standardise()`, or use function arguments)
    + Check whether time series length is a power of `2`. If you use `N <- log2(length(y))`, the number you get is `2^N`. You need an integer power for the length in order to create equal bin sizes. You can pad the series with 0s, or make it shorter before analysis.
* Perform `sda` on the 3 HRV time series, or any of the other series you already analysed.
* Compare to what you find for `fd_sda` to the other techniques (`fd_RR`,`SampEn`, `acf`, `pacf`)



#### Answers {-}


```{r collapse=TRUE, message=FALSE, warning=FALSE}

# ACF assignment data `series`
fdS1 <- fd_sda(series$TS_1, returnPLAW = TRUE)
fdS2 <- fd_sda(series$TS_2, returnPLAW = TRUE)
fdS3 <- fd_sda(series$TS_3, returnPLAW = TRUE)

# If you asked to return the powerlaw (returnPLAW = TRUE) the function plotFD_loglog() can make a plot
plotFD_loglog(fdS1,title = "SDA", subtitle = "Series 1")
plotFD_loglog(fdS2,title = "SDA", subtitle = "Series 2")
plotFD_loglog(fdS3,title = "SDA", subtitle = "Series 3")

```



**Values of other time series**

```{r message=FALSE, warning=FALSE, collapse=TRUE, cache=TRUE}

# RR assignment data TS1, TS2, TS3 and shuffled versions
# This is just a check so you can see which bin sizes to expect
# It is also possible to force calclulation of specific bin sizes, see the manual pages for more info ?fd_sda
L1 <- floor(log2(length(TS1$TS1)))
L2 <- floor(log2(length(TS2$TS2)))
L3 <- floor(log2(length(TS3$TS3)))

# You can also ask for a plot directly by passing doPlot = TRUE
fdTS1  <- fd_sda(TS1$TS1[1:2^L1], doPlot = TRUE, tsName = "TS1")

fdTS1r <- fd_sda(TS1Random[1:2^L1], doPlot = TRUE, tsName = "TS1 shuffled")


fdTS2 <- fd_sda(TS2$TS2[1:2^L2], returnPLAW = TRUE)
plotFD_loglog(fdTS2,subtitle = "TS2")

fdTS2r <- fd_sda(TS2Random[1:2^L2], returnPLAW = TRUE)
plotFD_loglog(fdTS2r,subtitle = "TS2 shuffled")

fdTS3 <- fd_sda(TS3$TS3[1:2^L3], returnPLAW = TRUE)
plotFD_loglog(fdTS3,subtitle = "TS3")

fdTS3r <- fd_sda(TS3Random[1:2^L3], returnPLAW = TRUE)
plotFD_loglog(fdTS3r,subtitle = "TS3 shuffled")

fdTS3n <- fd_sda(TS3Norm[1:2^L3], returnPLAW = TRUE)
plotFD_loglog(fdTS3n,subtitle = "TS3 standardised")


# Logistic map
y1 <- growth_ac(r = 2.9, type="logistic",N = 1024)
fdL29 <- fd_sda(y1, returnPLAW = TRUE)
plotFD_loglog(fdL29, subtitle = "Logistic Map: r =  2.9")

y2 <- growth_ac(r = 4, type="logistic", N = 1024)
fdL4 <- fd_sda(y2, returnPLAW = TRUE)
plotFD_loglog(fdL4, subtitle = "Logistic Map: r =  4")

```



###  Spectral Slope {.tabset .tabset-fade .tabset-pills}


#### Questions {-}

* Common data preparation before running spectral analyses:
    + Normalize the time series (using `sd` based on `N`, not `N-1`)
    + Check whether time series length is a power of `2`. If you use `N <- log2(length(y))`, the number you get is `2^N`. You need an integer power for the length in order to create equal bin sizes. You can pad the series with 0s, or make it shorter before analysis.
* Perform `fd_psd()` on the 3 HRV time series, or any of the other series you already analysed.
* Compare to what you find for `fd_psd` to the other measures (`fd_RR`,`SampEn`, `acf`, `pacf`)


#### Answers {-}


```{r collapse=TRUE,cache=TRUE, message=FALSE, warning=FALSE}

# ACF assignment data `series`
fdS1 <- fd_psd(series$TS_1, doPlot = TRUE)
fdS2 <- fd_psd(series$TS_2, returnPLAW = TRUE)
plotFD_loglog(fdS2,subtitle = "Series 2")
fdS3 <- fd_psd(series$TS_3, returnPLAW = TRUE)
plotFD_loglog(fdS3,subtitle = "Series 3")

```



**Values of other time series**

```{r message=FALSE, warning=FALSE, collapse=TRUE, cache=TRUE}

# This is just a check so you can see which frequency range to expect
# It is also possible to force calclulation of specific bin sizes, see the manual pages for more info ?fd_psd
L1 <- floor(log2(length(TS1$TS1)))
L2 <- floor(log2(length(TS2$TS2)))
L3 <- floor(log2(length(TS3$TS3)))

# RR assignment data TS1, TS2, TS3 and shuffled versions

fdTS1 <- fd_psd(TS1$TS1[1:2^L1], doPlot = TRUE, tsName = "TS1")

fdTS1r <- fd_psd(TS1Random[1:2^L1], returnPLAW = TRUE)
plotFD_loglog(fdTS1r,subtitle = "TS1 shuffled")

fdTS2 <- fd_psd(TS2$TS2[1:2^L2], returnPLAW = TRUE)
plotFD_loglog(fdTS2,subtitle = "TS2")

fdTS2r <- fd_psd(TS2Random[1:2^L2], returnPLAW = TRUE)
plotFD_loglog(fdTS2r,subtitle = "TS2 shuffled")

fdTS3 <- fd_psd(TS3$TS3[1:2^L3], returnPLAW = TRUE)
plotFD_loglog(fdTS3,subtitle = "TS3")

fdTS3r <- fd_psd(TS3Random[1:2^L3], returnPLAW = TRUE)
plotFD_loglog(fdTS3r,subtitle = "TS3 shuffled")

fdTS3n <- fd_psd(TS3Norm[1:2^L3], returnPLAW = TRUE)
plotFD_loglog(fdTS3n,subtitle = "TS3 standardised")


# Logistic map
y1 <- growth_ac(r = 2.9, type="logistic",N = 1024)
fdL29 <- fd_psd(as.numeric(y1), returnPLAW = TRUE)
plotFD_loglog(fdL29, subtitle = "Logistic Map: r =  2.9")

y2 <- growth_ac(r = 4, type="logistic", N = 1024)
fdL4 <- fd_psd(as.numeric(y2), returnPLAW = TRUE)
plotFD_loglog(fdL4, subtitle = "Logistic Map: r =  4")

```


###  Detrended Fluctuation Analysis {.tabset .tabset-fade .tabset-pills}


#### Questions {-}

* Data preparation before running `DFA` analyses are not really necessary, except perhaps:
    + Check whether time series length is a power of `2`. If you use `N <- log2(length(y))`, the number you get is `2^N`. You need an integer power for the length in order to create equal bin sizes. You can pad the series with 0s, or make it shorter before analysis.
* Perform `fd_dfa()` on the 3 HRV time series, or any of the other series you already analysed.
* Compare to what you find for `fd_dfa` to the other measures (`fd_RR`,`SampEn`, `acf`, `pacf`)


#### Answers {-}



```{r collapse=TRUE,cache=TRUE, message=FALSE, warning=FALSE}

# ACF assignment data `series`
fdS1 <- fd_dfa(series$TS_1, doPlot = TRUE)
fdS2 <- fd_dfa(series$TS_2, returnPLAW = TRUE)
plotFD_loglog(fdS2,subtitle = "Series 2")
fdS3 <- fd_dfa(series$TS_3, returnPLAW = TRUE)
plotFD_loglog(fdS3,subtitle = "Series 3")

```


**Values of other time series**

```{r message=FALSE, warning=FALSE, collapse=TRUE,cache=TRUE}

# This is just a check so you can see which bin sizes to expect
# It is also possible to force calclulation of specific bin sizes, see the manual pages for more info ?fd_dfa
L1 <- floor(log2(length(TS1$TS1)))
L2 <- floor(log2(length(TS2$TS2)))
L3 <- floor(log2(length(TS3$TS3)))

# RR assignment data TS1, TS2, TS3 and shuffled versions

fdTS1 <- fd_dfa(TS1$TS1[1:2^L1], doPlot = TRUE, tsName = "TS1")

fdTS1r <- fd_dfa(TS1Random[1:2^L1], returnPLAW = TRUE)
plotFD_loglog(fdTS1r,subtitle = "TS1 shuffled")

fdTS2 <- fd_dfa(TS2$TS2[1:2^L2], returnPLAW = TRUE)
plotFD_loglog(fdTS2,subtitle = "TS2")

fdTS2r <- fd_dfa(TS2Random[1:2^L2], returnPLAW = TRUE)
plotFD_loglog(fdTS2r,subtitle = "TS2 shuffled")

fdTS3 <- fd_dfa(TS3$TS3[1:2^L3], returnPLAW = TRUE)
plotFD_loglog(fdTS3,subtitle = "TS3")

fdTS3r <- fd_dfa(TS3Random[1:2^L3], returnPLAW = TRUE)
plotFD_loglog(fdTS3r,subtitle = "TS3 shuffled")

fdTS3n <- fd_dfa(TS3Norm[1:2^L3], returnPLAW = TRUE)
plotFD_loglog(fdTS3n,subtitle = "TS3 standardised")


# Logistic map
y1 <- growth_ac(r = 2.9, type="logistic",N = 1024)
fdL29 <- fd_dfa(y1, returnPLAW = TRUE)
plotFD_loglog(fdL29, subtitle = "Logistic Map: r =  2.9")

y2 <- growth_ac(r = 4, type="logistic", N = 1024)
fdL4 <- fd_dfa(y2, returnPLAW = TRUE)
plotFD_loglog(fdL4, subtitle = "Logistic Map: r =  4")

```



## **Surrogate testing: Beyond the straw-man null-hypothesis** 

Look at the explanation of surrogate analysis on the [TiSEAN website](https://www.pks.mpg.de/~tisean/Tisean_3.0.1/)
(Here is [direct link](https://www.pks.mpg.de/~tisean/Tisean_3.0.1/docs/surropaper/node5.html) to the relevant sections)


### Constrained realisations of complex signals {.tabset .tabset-fade .tabset-pills}

We'll look mostly at three different kinds of surrogates:

* *Randomly shuffled*: $H_0:$ The time series data are independent random numbers drawn from some probability distribution.
* *Phase Randomised*: $H_0:$ The time series data were generated by a stationary linear stochastic process
* *Amplitude Adjusted ,Phase Randomised*: $H_0:$ The time series data were generated by a rescaled linear stochastic process.


#### Questions {-}

* Figure out how many surrogates you minimally need for a 2-sided test.
* Package `nonlinearTseries` has a function `FFTsurrogate` and package `fractal` `surrogate`. Look them up and try to create a surrogate test for some of the time series of the previous assignments.
    + If you use `fractal::surrogate()`, choose either `aaft` (amplitude adjusted Fourier transform) or `phase` (phase randomisation)
    + `nonlinearTseries::FFTsurrogates` will calculate phase randomised surrogates.
* In package `casnet` there's a function that will calculate and display a point-probability based on a distribution of surrogate measures and one observed measure: `plotSUR_hist`



#### Answers {-}


Create a number of surrogates, calulate the slopes and plot the results.

* First let's get the values we found earlier (we'll use the FD from `sda`, then we do the same for `psd`,but you can take any of the measures as well)


```{r message=FALSE, warning=FALSE, cache=TRUE}
library(fractal)

# RR assignment data `TS1,TS2,TS3`
# Now we'll trim the time series length to a power of 2 to get the cleanest results, this is not strictly necessary 
L1 <- floor(log2(length(TS1$TS1)))
L2 <- floor(log2(length(TS2$TS2)))
L3 <- floor(log2(length(TS2$TS2)))

TS1_FDsda <- fd_sda(TS1$TS1[1:2^L1])$fitRange$FD
TS2_FDsda <- fd_sda(TS2$TS2[1:2^L2])$fitRange$FD
TS3_FDsda <- fd_sda(TS3$TS3[1:2^L3])$fitRange$FD

y1<-growth_ac(r = 2.9,type="logistic",N = 2^L1)
TSlogMapr29 <- fd_sda(y1)$fitRange$FD

y2<-growth_ac(r = 4,type="logistic", N = 2^L1)
TSlogMapr4 <- fd_sda(y2)$fitRange$FD

# These were rendomised
TS1R_FDsda <- fd_sda(TS1Random[1:2^L1])$fitRange$FD
TS2R_FDsda <- fd_sda(TS1Random[1:2^L2])$fitRange$FD
TS3R_FDsda <- fd_sda(TS1Random[1:2^L3])$fitRange$FD
```


* To create the surrogates using `fractal::surrogate()`
    + You'll need to repeatedly call the function and store the result
    + To get into a dataframe we have to use `unclass()` otherwise `R` doesn't recognise the object
    + Function `t()` trnasposes the data from 39 rows to 39 columns
* Once you have the surrogates in a datafram, repeatedly calculate the measure you want to compare, in this case `fd_sda()`.

```{r echo=TRUE, fig.height=12, message=FALSE, warning=FALSE, cache=TRUE, collapse=TRUE}
# NOW CREATE SURROGATES
library(dplyr)

# For a two-sided test at alpha = .05 we need N=39
Nsurrogates <- 39

TS1surrogates <- as.data.frame(t(ldply(1:Nsurrogates, function(s) unclass(surrogate(x=TS1$TS1[1:2^L1] ,method="aaft")))))
colnames(TS1surrogates) <- paste0("S",1:NCOL(TS1surrogates))

# Let's look at the surrogates and originals series...
# NOTE: This is a bit elaborate, because ggplot2 wants to have data in tidy (= long) format, which means all the time series data have to in 1 column!

# Make data frame to plot:
TS1surrogates$Obs <- TS1$TS1[1:2^L1]
TS1surrogates$Time <- 1:NROW(TS1surrogates)
df <- TS1surrogates %>% tidyr::gather(key=variable, value = Y, -Time)

# This will make the observed series red
eval(parse(text = c("col <- c('Obs' = 'red3',",paste0("'",colnames(TS1surrogates)%[%39,"'"," = 'black'", collapse=","),")")))

ggplot(df,aes(x=Time, y = Y, group = variable, colour = variable)) + 
  geom_path(show.legend = FALSE) +
  facet_grid(variable~.)  +
  scale_colour_manual(values = col) +
  scale_x_continuous(expand = c(0,0)) +
  theme_bw() +
  theme(axis.text = element_blank(), strip.text = element_blank(), strip.background = element_blank())


# Now we calculate FD for each series
TS1surrogates_FD <- laply(1:Nsurrogates,function(s) fd_sda(y=TS1surrogates[,s], silent = TRUE)$fitRange$FD)

TS2surrogates <- t(ldply(1:Nsurrogates, function(s) unclass(surrogate(x=TS2$TS2[1:2^L2] ,method="aaft"))))
colnames(TS2surrogates) <- paste0("aaftSurr",1:NCOL(TS2surrogates))

TS2surrogates_FD <- laply(1:Nsurrogates,function(s) fd_sda(y=TS2surrogates[,s], silent = TRUE)$fitRange$FD)


TS3surrogates <- t(ldply(1:Nsurrogates, function(s) unclass(surrogate(x=TS3$TS3[1:2^L3] ,method="aaft"))))
colnames(TS3surrogates) <- paste0("aaftSurr",1:NCOL(TS3surrogates))

TS3surrogates_FD <- laply(1:Nsurrogates,function(s) fd_sda(y=TS3surrogates[,s], silent = TRUE)$fitRange$FD)


TSr29surrogates <- t(ldply(1:Nsurrogates, function(s) unclass(surrogate(x=y1, method="aaft"))))
colnames(TSr29surrogates) <- paste0("aaftSurr",1:NCOL(TSr29surrogates))

TSr29surrogates_FD <- laply(1:Nsurrogates,function(s) fd_sda(y=TSr29surrogates[,s], silent = TRUE)$fitRange$FD)


TSr4surrogates <- t(ldply(1:Nsurrogates, function(s) unclass(surrogate(x=y2, method="aaft"))))
colnames(TSr4surrogates) <- paste0("aaftSurr",1:NCOL(TSr4surrogates))

TSr4surrogates_FD <- laply(1:Nsurrogates,function(s) fd_sda(y=TSr4surrogates[,s], silent = TRUE)$fitRange$FD)
```

* Collect the results in a dataframe, so we can compare.

```{r echo=TRUE, message=FALSE, warning=FALSE}
x <- data.frame(Source = c("TS1", "TS2", "TS3", "TSlogMapr29", "TSlogMapr4"), 
                FDsda = c(TS1_FDsda, TS2_FDsda, TS3_FDsda, TSlogMapr29, TSlogMapr4), 
                FDsdaRandom =  c(TS1R_FDsda, TS2R_FDsda, TS3R_FDsda,NA,NA), 
                FDsdaAAFT.median= c(median(TS1surrogates_FD),median(TS2surrogates_FD),median(TS3surrogates_FD),median(TSr29surrogates_FD),median(TSr4surrogates_FD)))

```


* Call function `plotSUR_hist()` to see the results of the test.

```{r echo=TRUE, message=FALSE, warning=FALSE, cache=TRUE}
plotSUR_hist(surrogateValues = TS1surrogates_FD, observedValue = TS1_FDsda, sides = "two.sided", doPlot = TRUE, measureName = "sda FD TS1")

plotSUR_hist(surrogateValues = TS2surrogates_FD, observedValue = TS2_FDsda, sides = "two.sided", doPlot = TRUE, measureName = "sda FD TS2")

plotSUR_hist(surrogateValues = TS3surrogates_FD, observedValue = TS3_FDsda,sides = "two.sided", doPlot = TRUE, measureName = "sda FD TS3")

plotSUR_hist(surrogateValues = TSr29surrogates_FD, observedValue = TSlogMapr29,sides = "two.sided", doPlot = TRUE, measureName = "sda FD logMap r=2.9")

plotSUR_hist(surrogateValues = TSr4surrogates_FD, observedValue = TSlogMapr4, sides = "two.sided", doPlot = TRUE, measureName = "sda FD logMap r=4")
```

**FD: SPECTRAL SLOPE**

```{r echo=TRUE, message=FALSE, warning=FALSE, cache=TRUE}

# RR assignment data `TS1,TS2,TS3`
L1 <- floor(log2(length(TS1$TS1)))
L2 <- floor(log2(length(TS2$TS2)))
L3 <- floor(log2(length(TS2$TS2)))

TS1_FDpsd <- fd_psd(TS1$TS1[1:2^L1])$fitRange$FD
TS2_FDpsd <- fd_psd(TS2$TS2[1:2^L2])$fitRange$FD
TS3_FDpsd <- fd_psd(TS3$TS3[1:2^L3])$fitRange$FD

# Logistic map - make it the length of L1
y1<-growth_ac(r = 2.9,type="logistic",N = 2^L1)
# Turn standardisation and detrending off!
TSlogMapr29 <- fd_psd(y1, standardise = FALSE, detrend = FALSE)$fitRange$FD
y2<-growth_ac(r = 4,type="logistic", N = 2^L1)
TSlogMapr4 <- fd_psd(y2, standardise = FALSE, detrend = FALSE)$fitRange$FD

TS1R_FDpsd <- fd_psd(TS1Random[1:2^L1])$fitRange$FD
TS2R_FDpsd <- fd_psd(TS1Random[1:2^L2])$fitRange$FD
TS3R_FDpsd <- fd_psd(TS1Random[1:2^L3])$fitRange$FD

```


* To create the surrogates using `fractal::surrogate()`
    + You'll need to repeatedly call the function and store the result
    + To get into a dataframe we have to use `unclass()` otherwise `R` doesn't recognise the object
    + Function `t()` trnasposes the data from 39 rows to 39 columns
* Once you have the surrogates in a dataframe, repeatedly calculate the measure you want to compare, in this case `fd_sda()`.

```{r echo=TRUE, message=FALSE, warning=FALSE, cache=TRUE}
# NOW CREATE SURROGATES
library(dplyr)

# For a two-sided test at alpha = .05 we need N=39
Nsurrogates <- 39

TS1surrogates <- t(ldply(1:Nsurrogates, function(s) unclass(fractal::surrogate(x=TS1$TS1[1:2^L1] ,method="aaft"))))
colnames(TS1surrogates) <- paste0("aaftSurr",1:NCOL(TS1surrogates))

TS1surrogates_FD <- laply(1:Nsurrogates,function(s){fd_psd(y=TS1surrogates[,s], silent = TRUE)$fitRange$FD})


TS2surrogates <- t(ldply(1:Nsurrogates, function(s) unclass(surrogate(x=TS2$TS2[1:2^L2] ,method="aaft"))))
colnames(TS2surrogates) <- paste0("aaftSurr",1:NCOL(TS2surrogates))

TS2surrogates_FD <- laply(1:Nsurrogates,function(s) fd_psd(y=TS2surrogates[,s], silent = TRUE)$fitRange$FD)


TS3surrogates <- t(ldply(1:Nsurrogates, function(s) unclass(surrogate(x=TS3$TS3[1:2^L3] ,method="aaft"))))
colnames(TS3surrogates) <- paste0("aaftSurr",1:NCOL(TS3surrogates))

TS3surrogates_FD <- laply(1:Nsurrogates,function(s) fd_psd(y=TS3surrogates[,s], silent = TRUE)$fitRange$FD)


TSr29surrogates <- t(ldply(1:Nsurrogates, function(s) unclass(surrogate(x=y1, method="aaft"))))
colnames(TSr29surrogates) <- paste0("aaftSurr",1:NCOL(TSr29surrogates))

TSr29surrogates_FD <- laply(1:Nsurrogates,function(s) fd_psd(y=TSr29surrogates[,s], silent = TRUE)$fitRange$FD)


TSr4surrogates <- t(ldply(1:Nsurrogates, function(s) unclass(surrogate(x=y2, method="aaft"))))
colnames(TSr4surrogates) <- paste0("aaftSurr",1:NCOL(TSr4surrogates))

TSr4surrogates_FD <- laply(1:Nsurrogates,function(s) fd_psd(y=TSr4surrogates[,s], silent = TRUE)$fitRange$FD)


```


* Call function `plotSUR_hist()` to get the results of the test.

```{r message=FALSE, warning=FALSE,cache=TRUE}
plotSUR_hist(surrogateValues = TS1surrogates_FD, observedValue = TS1_FDpsd, sides = "two.sided", doPlot = TRUE, measureName = "psd FD TS1")

plotSUR_hist(surrogateValues = TS2surrogates_FD, observedValue = TS2_FDpsd,sides = "two.sided", doPlot = TRUE, measureName = "psd FD TS2")

plotSUR_hist(surrogateValues = TS3surrogates_FD, observedValue = TS3_FDpsd,sides = "two.sided", doPlot = TRUE, measureName = "psd FD TS3")

plotSUR_hist(surrogateValues = TSr29surrogates_FD, observedValue = TSlogMapr29,sides = "two.sided", doPlot = TRUE, measureName = "psd FD logMap r=2.9")

plotSUR_hist(surrogateValues = TSr4surrogates_FD, observedValue = TSlogMapr4, sides = "two.sided", doPlot = TRUE, measureName = "psd FD logMap r=4")

```


 * Let's print the results into a table
 

```{r message=FALSE, paged.print=TRUE,cache=TRUE}
x$FDpsd <- c(TS1_FDpsd,TS2_FDpsd,TS3_FDpsd,TSlogMapr29,TSlogMapr4)
x$FDpsdRandom <- c(TS1R_FDpsd, TS2R_FDpsd, TS3R_FDpsd,NA,NA)
x$FDpsdAAFT.median <- c(median(TS1surrogates_FD),median(TS2surrogates_FD),median(TS3surrogates_FD),median(TSr29surrogates_FD),median(TSr4surrogates_FD))
knitr::kable(x, digits = 2, booktabs=TRUE,formt="html")
```



<!--chapter:end:ASSIGNMENTS_P2B.Rmd-->

---
title: "DCS Assignments session 5"
author: "Fred Hasselman & Maarten Wijnants"
date: "1/14/2019"
output: 
  bookdown::html_document2: 
    variant: markdown+hard_line_breaks
    fig_caption: yes
    highlight: pygments
    keep_md: yes
    number_sections: yes
    theme: spacelab
    toc: yes
    toc_float: true
    collapsed: false
    smooth_scroll: true
    code_folding: show
    bibliography: [refs.bib, packages.bib]
    biblio-style: apalike
    csl: apa.csl
    includes:
        before_body: assignmentstyle.html
    pandoc_args: ["--number-offset","4"]
editor_options: 
  chunk_output_type: console
---


```{r, include=FALSE}
knitr::opts_chunk$set(include=TRUE, tidy = FALSE)
```


# **Quick Links** {-}

* [Main Assignments Page](https://darwin.pwo.ru.nl/skunkworks/courseware/1819_DCS/assignments/)


# About (Cross-) Recurrence Quantification Analysis in `R`

You can use `R` to run `(C)RQA` analyses and these assignments assume you'll use `R`, but you can also find a `Matlab` toolbox for `(C)RQA` here: [CRP toolbox](http://tocsy.pik-potsdam.de/CRPtoolbox/), there are `Python` libraries as well, see [this page](http://www.recurrence-plot.tk/programmes.php) for an overview of software. 

There are several `R` packages which can perform (C)RQA analysis, we will not use [`crqa`](https://cran.r-project.org/web/packages/crqa/index.html), but the functions in package `casnet`.

```{block2, crqa, type='rmdimportant'}
 The package `crqa()` was mainly designed to run categorical Cross-Recurrence Quantification Analysis (see [Coco & Dale (2014)](http://journal.frontiersin.org/Journal/10.3389/fpsyg.2014.00510/abstract) and for R code see appendices in [Coco & Dale (2013)](http://arxiv.org/abs/1310.0201)). The article is a good reference work for details on how to conduct RQA analysis.
```


You'll need the latest version of `casnet`:

```{r, eval=FALSE}
# Install casnet if necessary: https://github.com/FredHasselman/casnet
# !!Warning!! Very Beta...
# library(devtools)
# devtools::install_github("FredHasselman/invctr")
# devtools::install_github("FredHasselman/casnet")
library(invctr)
library(casnet)
library(rio)
```


Package [`casnet`](https://fredhasselman.github.io/casnet) has 2 functions that will perform auto- and cross-recurrence quantification analyses:         

* Function `crqa_cl()` uses a precompiled program that is run from the command line (see [command line recurrence plot tools](http://tocsy.pik-potsdam.de/commandline-rp.php)). It will take as input one or two timeseries and construct a recurrence matrix and is very fast compared to other methods. The first time you run the command `crqa_cl()` the `casnet` package will try to download the correct executables for your operating system, look for messages in the console to find out if the script succeeded. The output of `crqa_cl()` will be similar to the [CRP toolbox](http://tocsy.pik-potsdam.de/CRPtoolbox/) and [`Python` libraries](http://www.recurrence-plot.tk/programmes.php) by Norbert Marwan.
  - If downloading or running `crqa_cl()` fails, this is usually because you do not have enough rights to execute a command line program on your machine. In that case, figure out how to get those rights, or use `crqa_rp()`.
* Function `crqa_rp()` assumes you have already contructed a recurrence matrix using function `rp()`. The output of `crqa_rp()` contains more measures than `crqa_cl()` some of which are still experimental.


## **Categorical Auto-RQA** 


### RQA by 'hand' {.tabset .tabset-fade .tabset-pills}

Let's start with a small time series... open and/or download this Google sheet: https://docs.google.com/spreadsheets/d/1k4FYDbszfCReCM5_wcw6NIEgSFtTPIy3qmf-kFiZjy8/edit?usp=sharing 

The spreadsheet displays a short categorical time series of 5 observed categories labelled `A` through `E`.
Below the time series a matrix is displayed with zeroes in the upper triangle. The columns and rows of the matrix are labelled according to the categorical time series **y** in `B2:B13`, starting in the lower left corner. The *Line Of Incidence* (LOI) is the diagonal of the matrix going from the lower left corner to the upper right corner. Like the diagonal of a correlation matrix, the LOI in Auto-RQA will contain only recurrent points (`1`), because that's where the column labels will be identical to the row labels.


#### Questions {-}

Because this is Auto-RQA and we are comparing the time series to itself, the recurrence matrix will be symmetrical, so we only have to look at the upper triangle.

* Fill the upper triangle with recurrent points by changing a `0` into a `1`
  - Start with the label in cell `C29` (**A**) and look upwards (`C27` to `C17`). If you see the value **A** recurring, change the `0` into a `1`
   - Proceed to `D29` and find recurrences of the value **B** in the future (in `D26` to `D17`)
   - Continue to fill the upper triangle
* Count the number of `1`s, the recurrent points (`RN`) and put the value in cell `B32` (*Total*)
* Count how many recurrent points are part of a vertical line, and how many are on a diagonal line, put the values in cell `B33` and `B34` respectively.
* Diagonal line lengths:
  - Determine the frequency of occurrence of specific lengths of diagonal lines, record the values in cells `Q17` to `Q20`
  - Record the maximal diagonal line length in `V22`
  - The mean of the lengths of diagonal lines (*L*) will be calculated in cell `V21`
* Vertical line lengths: 
  - Determine the frequency of occurrence of specific lengths of vertical lines, record the values in cells `Q24` to `Q27`
  - Record the maximal vertical line length in `V27`
  - The mean of the lengths of vertical lines (*Trapping Time*) will be calculated in cell `V21`
* Several values are auto-calculated, if you click on the cell you can see how this is done, by looking at the formula bar.
  - Can you understand the calculation of the number of possible recurrent points in `X17`?
* Suppose you randomise the time series... which of the following values do you think would change, and in what way (larger/smaller)
  - Recurrence Rate (`RR`)
  - Mean diagonal line length (`L`, or `MEAN_dl`)
  - Maximum diagonal line length (`L_max`, or `MAX_dl`)
  - Entropy of distribution of diagonal line lengths (`L_entr`, or `ENT_dl`)

#### Answers {-}

* The correct matrix is on the second tab of the spreadsheet.
* The calculation of the number of possible recurrent points in cell `X17` is the size of the matrix (12*12) divided by 2 (because we are only looking at the upper triangle), minus the length of the diagonal, because these aren't actually recurrent points.
* Which of the following values do you think would change if `y` is randomised?
  - Recurrence Rate (`RR`) - **No change**, none of the values are changed, so the number of values that will recur is the unchanged.
  - Mean diagonal line length (`L`, or `MEAN_dl`) - **Likely to be lower**, shuffling will likely break up line structures, but if there are just a few categories and/or the time series is short, it could be the same or higher.
  - Maximum diagonal line length (`L_max`, or `MAX_dl`) - **Very likely to be lower**, shuffling will likely break up line structures, but if there are just a few categories and/or the time series is short, it could be the same or higher.
  - Entropy of distribution of diagonal line lengths (`L_entr`, or `ENT_dl`) - - **Difficult to predict**, shuffling will break up line structures and could increase entropy (more different line lengths), but it is also possible that ther will be only lines of length 2 or 3 left after randomisation, this will be a very homogeneous distribution which has a high entropy. This is more likely if there are just a few categories and/or the time series is short.


### RQA and executive control

`casnet` comes with a vignette demonstrating the usage of `crqa_cl()`, see https://fredhasselman.github.io/casnet/articles/cl_RQA.html, or the manual pages. 

* Study the vignette and try to understand the conclusion based on the surrogate tests.



## **Phase Space Reconstruction** 


Here is a great video summary of the phase space reconstruction technique: https://youtu.be/6i57udsPKms


```{block2, type='rmdnote'}
**R-packages for Phase Space Reconstruction**

Package `casnet` relies on packages `fractal` and `rgl` to reconstruct phase space. Package `rgl` is used for plotting and can cause problems on some windows machines. On windows You'll need to check if you have the [X Window System](http://www.x.org/wiki/) for interactive 3D plotting. This Linux desktop system comes installed in some way or form on most Mac and Windows systems. You can test if it is present by running `rgl::open3d()` in `R`, which will try to open an interactive plotting device.
```


### Reconstruct the Lorenz attractor {.tabset .tabset-fade .tabset-pills}

Package `fractal` includes the 3 dimensions of the Lorenz system in the chaotic regime. Run this code `rgl::plot3d(lorenz,type="l")` with all three packages loaded to get an interactive 3D plot of this strange attractor.

We'll reconstruct the attractor based on just dimension `X` of the system using the data from package `fractal`, and functions from package `casnet`. Don't forget: *always look at the manual pages of a function* you are running.

```{block2, samepack, type='rmdimportant'}
Package `fractal` and package `nonlinearTseries` use functions with similar names, perhapp better to not load them together, or always use `fractal::` and `nonlinearTseries::` if you want to call the functions direclty.
```


#### Questions {-}

* Use `lx <- lorenz[1:2048,1]` to reconstruct the phase space based on `lx`. 
    + Find an optimal embedding lag and dimension by calling `crqa_parameters()`.
    + Use `nnThres = .01`. This is the proportion of remaining nearest neighbours below which we consider the number of dimensions optimal. Because this time series was generated by a simulation of a system, we can use a much lower threshold. For real data, which is much noisier, you can use `0.1` or `0.2`.
    + **NOTE:** If you have a slow computer don't use `2048`, but use e.g. `512`.
    
This `crqa_parameters()` function will call several other functions (e.g. `fractal::timeLag()` with `method = "mutual"` and `nonlinearTseries::findAllNeighbours()` for the false nearest neighbourhood search). Look at the manual page, you can run the `crqa_parameters()` function with the defaults settings, it will show a diagnostic plot. Because we have to search over a range of parameters it can take a while to get the results...

* Based on the output:
    + Choose an embedding lag $\tau$ (in `casnet` functions this is the argument `emLag`). Remember, this is just an optimisation choice. Sometimes there is no first minimum, or no global minimum.
    + Find an appropriate embedding dimension by looking when the false nearest neighbours drop below 1\%. 
    
* Now you can use this information to reconstruct the phase space: 
    + Embed the time series using `ts_embed()` (you can also use `fractal::embedSeries()`).
    + Inspect the object that was returned
    + Use  `rgl::plot3d()` to plot the reconstructed space. Plot the reconstructed phase space. (You'll need to use `as.matrix()` on the object created by `fractal::embedSeries()`)

#### Answers {-}

* Use `lx <- lorenz[1:2048,1]` to reconstruct the phase space based on `lx`. 
    + Find an optimal embedding lag and dimension by calling `crqa_parameters()`.

```{r}
library(rgl)
library(ggplot2)
library(fractal)
library(invctr)
library(casnet)

# The X data
lx <- lorenz[1:2048,1]

# Search for the parameters,
params <- crqa_parameters(lx)

# Assign the optimal parameters
emLag <- params$optimLag
emDim <- params$optimDim
```

* Now you can use this information to reconstruct the phase space

```{r, eval = TRUE}
# For X
lx_emb <- ts_embed(y=lx,emLag = emLag, emDim = emDim)

rgl::plot3d(lx_emb, type="l")

# For Y
ly <- lorenz[1:2048,2]

# Search for the parameters,
params <- crqa_parameters(ly)

# Assign the optimal parameters
emLag <- params$optimLag
emDim <- params$optimDim

# Embed the time series
ly_emb <- ts_embed(y=ly,emLag = emLag, emDim = emDim)
rgl::plot3d(ly_emb,type="l")

# For Z
lz <- lorenz[1:2048,3]

# Search for the parameters,
params <- crqa_parameters(lz)

# Assign the optimal parameters
emLag <- params$optimLag
emDim <- params$optimDim

# Embed the time series
lz_emb <- ts_embed(y=lz,emLag = emLag, emDim = emDim)
rgl::plot3d(lz_emb,type="l")
```


### Reconstruct the Predator-Prey model {.tabset .tabset-fade .tabset-pills}



#### Questions {-}

Use the same procedure as above to reconstruct the state space of the predator-prey system. (Look [at the solutions](https://darwin.pwo.ru.nl/skunkworks/courseware/1819_DCS/assignments/ASSIGNMENTS_P1A.html#221_foxes_and_rabbits_in_r) to get a `Foxes` or `Rabbit` series).

 * You should get a 2D state space, so 3D plotting might be a bit too much for this system.

#### Answers{-}

```{r}
library(plyr)
library(ggplot2)
# Get the time series
# Parameters
N  <- 1000
a  <- d <- 1
b  <- c <- 2 
R0 <- F0 <- 0.1
Ra  <- as.numeric(c(R0, rep(NA,N-1)))
Fx  <- as.numeric(c(F0, rep(NA,N-1)))

# Time constant
delta <- 0.01

# Numerical integration of the predator-prey system
l_ply(seq_along(Ra), function(t){
    Ra[[t+1]] <<- (a - b * Fx[t]) * Ra[t] * delta + Ra[t] 
    Fx[[t+1]] <<- (c * Ra[t] - d) * Fx[t] * delta + Fx[t] 
    })

# lets take Foxes and search
params <- crqa_parameters(Fx)

# Assign the optimal parameters
emLag <- params$optimLag
emDim <- params$optimDim

# Embed the time series
Fx_emb <- ts_embed(y=Fx,emLag = emLag, emDim = emDim)

# Now take Rabbits and search parameters
params <- crqa_parameters(Ra)

# Assign the optimal parameters
emLag <- params$optimLag
emDim <- params$optimDim

# Embed the time series
Ra_emb <- ts_embed(y=Ra,emLag = emLag, emDim = emDim)

# Plot
df.pp <- cbind.data.frame(Foxes = c(Fx, Fx_emb[,1], Ra_emb[,1]), Rabbits = c(Ra, Fx_emb[,2], Ra_emb[,2]), Type = c(rep("Original",N+1), rep("reconstructed Foxes",NROW(Fx_emb)), rep("reconstructed Rabbits",NROW(Ra_emb))))

ggplot(df.pp,aes(x=Foxes, y = Rabbits, group = Type)) +
  geom_path(aes(colour=Type), size=1) +
  theme_bw() +
  coord_equal()
```



## Continuous Auto-RQA


### RQA of the Lorenz attractor {.tabset .tabset-fade .tabset-pills}

Perform an RQA on the reconstructed state space of the Lorenz system. You'll need a radius (also called: threshold, or $\epsilon$) in order to decide which points are close together (recurrent). You have already seen `casnet` provides a function which will automatically select the best parameter settings for phase space reconstruction: `crqa_parameters()`. 


#### Questions {-}

* First get a radius that gives you a fixed recurrence rate use `crqa_radius()` look at the manual pages.

* Best way to ensure you are using the same parameters in each function is to create some lists with parameter settings (check the `crqa_cl()` manual to figure out what these parameters mean)


#### Answers{-}

```{r}
library(fractal)
library(casnet)

# Lorenz X
lx <- lorenz[1:2048,1]
emLag = 17
emDim = 3

(emRad <- crqa_radius(y1 = lx, emLag = emLag, emDim = emDim)$Radius)

# RQA analysis
(out <- crqa_cl(lx,emDim = emDim, emLag = emLag, emRad = emRad))
```


We can plot the recurrence matrix...

```{r}
library(casnet)

# Unthresholded matrix (no radius applied)
RM <- rp(y1 = lx, emDim = emDim, emLag = emLag)

# plot it
rp_plot(RM)


# Thresholded by the radius
RMth <- di2bi(RM,emRad = emRad)

rp_plot(RMth, plotDimensions = TRUE, plotMeasures = TRUE)


```


### Distinguish between Logisitc Map and White noise? {.tabset .tabset-fade .tabset-pills}

RQA promises to be able to distinguish between different dynamical regimes, for example dynamics due to different parameter settings of the logistic map, see e.g. https://en.wikipedia.org/wiki/Recurrence_quantification_analysis#Time-dependent_RQA 


#### Questions {-}

Use RQA to compare white noise to the deterministic chaos series of previous assignments.

```{r eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE, include=TRUE}
library(rio)
library(pracma)

# Reload the data
series <- rio::import("https://github.com/FredHasselman/The-Complex-Systems-Approach-Book/raw/master/assignments/assignment_data/BasicTSA_arma/series.sav", setclass = "tbl_df")

```

#### Answers {-}

Discussed in the next session




### RQA of circle-tracing data {.tabset .tabset-fade .tabset-pills}

We'll analyse a time series recorded from asubject who is tracing a circle using a computer mouse:   [circle tracing data](https://github.com/FredHasselman/The-Complex-Systems-Approach-Book/tree/master/assignments/assignment_data/RQA_circletrace).


#### Questions {-}

* Perform an RQA on one of the coordinate series (x or y)
* Study what happens to the RQA measures if you shuffle the temporal order.
* Package `fractal` contains a function `surrogate`. This will create so-called *constrained* realisations of the time series. You can look at the vignette in package `casnet` `cl_CRQA` ([also available in Github](https://github.com/FredHasselman/casnet/tree/master/inst/doc)) Look at the help pages of the function, or study the *Surrogates Manual* of the [TISEAN software](http://www.mpipks-dresden.mpg.de/~tisean/Tisean_3.0.1/index.html) and create two surrogate series, one based on `phase` and one on `aaft`.
     + Look at the RQA measures and think about which $H_0$ should probably be rejected.
     + If you want to be more certain, you'll have to create a test (more surrogates). The [TISEAN manual](http://www.mpipks-dresden.mpg.de/~tisean/Tisean_3.0.1/docs/surropaper/node5.html#SECTION00030000000000000000) provides all the info you need to construct such a test:
     
     > "For a minimal significance requirement of 95\% , we thus need at least 19 or 39 surrogate time series for one- and two-sided tests, respectively. The conditions for rank based tests with more samples can be easily worked out. Using more surrogates can increase the discrimination power."
  


<!--chapter:end:ASSIGNMENTS_P3.Rmd-->

---
title: "DCS Assignments session 6"
author: "Fred Hasselman & Maarten Wijnants"
date: "1/14/2019"
output: 
  bookdown::html_document2: 
    variant: markdown+hard_line_breaks
    fig_caption: yes
    highlight: pygments
    keep_md: yes
    number_sections: yes
    theme: spacelab
    toc: yes
    toc_float: true
    collapsed: false
    smooth_scroll: true
    code_folding: show
    bibliography: [refs.bib, packages.bib]
    biblio-style: apalike
    csl: apa.csl
    includes:
        before_body: assignmentstyle.html
    pandoc_args: ["--number-offset","5"]
editor_options: 
  chunk_output_type: console
---


```{r, include=FALSE}
knitr::opts_chunk$set(include=TRUE, tidy = FALSE)
```


# **Quick Links** {-}

* [Main Assignments Page](https://darwin.pwo.ru.nl/skunkworks/courseware/1819_DCS/assignments/)


# About (Cross-) Recurrence Quantification Analysis in `R`

You can use `R` to run `(C)RQA` analyses and these assignments assume you'll use `R`, but you can also find a `Matlab` toolbox for `(C)RQA` here: [CRP toolbox](http://tocsy.pik-potsdam.de/CRPtoolbox/), there are `Python` libraries as well, see [this page](http://www.recurrence-plot.tk/programmes.php) for an overview of software. 

There are several `R` packages which can perform (C)RQA analysis, we will not use [`crqa`](https://cran.r-project.org/web/packages/crqa/index.html), but the functions in package [`casnet`](https://fredhasselman.github.io/casnet/).

```{block2, crqa, type='rmdimportant'}
 
The package `crqa()` was mainly designed to run categorical Cross-Recurrence Quantification Analysis (see [Coco \& Dale (2014)](http://journal.frontiersin.org/Journal/10.3389/fpsyg.2014.00510/abstract) and for R code see appendices in [Coco & Dale (2013)](http://arxiv.org/abs/1310.0201)). The article is a good reference for RQA analysis.
```

To do the assignments you'll need tpo install the latest version of [`casnet`](https://fredhasselman.github.io/casnet/) from Github!

**NOTE:** Package *invctr 0.1.0* has been accepted for distribution on CRAN, so you don't  have to install it seperately from Github anymore. See: https://cran.r-project.org/web/packages/invctr/index.html 

```{r}
# Install casnet if necessary: https://github.com/FredHasselman/casnet
# !!Warning!! Very Beta...
# library(devtools)
# devtools::install_github("FredHasselman/casnet")
library(casnet)
library(rio)
```


Package [`casnet`](https://fredhasselman.github.io/casnet) has 2 functions that will perform auto- and cross-recurrence quantification analyses:         

* Function `crqa_cl()` uses a precompiled program that is run from the command line (see [command line recurrence plot tools](http://tocsy.pik-potsdam.de/commandline-rp.php)). It will take as input one or two timeseries and construct a recurrence matrix and is very fast compared to other methods. The first time you run the command `crqa_cl()` the `casnet` package will try to download the correct executables for your operating system, look for messages in the console to find out if the script succeeded. The output of `crqa_cl()` will be similar to the [CRP toolbox](http://tocsy.pik-potsdam.de/CRPtoolbox/) and [`Python` libraries](http://www.recurrence-plot.tk/programmes.php) by Norbert Marwan.
  - If downloading or running `crqa_cl()` fails, this is usually because you do not have enough rights to execute a command line program on your machine. In that case, figure out how to get those rights, or use `crqa_rp()`.
  - `casnet` has a vignette demonstrating usage of `crqa_cl()`, see e.g. https://fredhasselman.github.io/casnet/articles/cl_RQA.html 
* Function `crqa_rp()` assumes you have already contructed a recurrence matrix using function `rp()`. The output of `crqa_rp()` contains more measures than `crqa_cl()` some of which are still experimental.



## Categorical CRQA and the Diagonal Recurrence Profile {.tabset .tabset-fade .tabset-pills}


In 2008 there was some commotion about a speech held by Barack Obama in Milwaukee ([news item](https://www.cbsnews.com/news/obama-accused-of-plagiarism-in-speech/)). According to the media the speech was very similar to a speech held the gouverneur of Massachusetts (Deval Patrick) some years earlier. Here are some fragments of the speeches,

Obama (2008):

> Don't tell me words don't matter. "I have a dream." Just words? "We hold these truths to be self-evident, that all men are created equal." Just words? "We have nothing to fear but fear itself"just words? Just speeches?

Patrick (2006): 

> "We hold these truths to be self-evident, that all men are created equal." Just wordsjust words! "We have nothing to fear but fear itself." Just words! "Ask not what your country can do for you, ask what you can do for your country." Just words! "I have a dream." Just words!


```{r}
# Load the speeches as timeseries

speeches <- rio::import("/Users/fred/ownCloud/Onderwijs/Dynamics of Complex Systems 2018-2019/session 6_RQA and CRQA/Obama.csv")

speeches <- plyr::colwise(as.numeric)(speeches)
speeches$V1[is.na(speeches$V1)] <- -2

colnames(speeches) <- c("Obama","Patrick")

# speeches <- rio::import("https://raw.githubusercontent.com/FredHasselman/The-Complex-Systems-Approach-Book/master/assignments/assignment_data/RQA_circletrace/CRQA_categorical.csv")

```


### Questions {-}


* Plot both time series. 

* The coding assigned a `-2` to words that do not occur in Patrick's speech (instead of assigning them a unique code).
    - Do you think this has an effect on the CRQA measures?
    - What & why?

* Because both fragments weren't of equal size, one series is "filled" at the end with the value `0`
    -  Do you think this has an effect on the CRQA measures? 
    - What & why?

* Perform a categorical CRQA:
   - First create a cross recurrence matrix using function `rp()`. 
   - Look at the manual page what the values mean, look at last the categorical RQA assignment for the values of `emLag`, `emDim` and `emRad`
   - Get recurrence measures using `crqa_rp()` (ignore the warning),
   - Plot the recurrence plot using `rp_plot()`
   - Also look at the diagonal recurrence profile using `crqa_diagProfile()`
   
* What is your opinion about the similarity of these speeches?


### Answers {-}

* Adding a value which occurs in just 1 series (whether it is `-2`, or `0`), but not the other makes sure that value will not be recurring in a Cross Recurrence Plot


```{r}
# Plot the speech series
plot(ts(speeches))

# Cross recurrence matrix
RM <- rp(y1 = speeches$Obama, y2 = speeches$Patrick, emDim = 1, emLag = 1, emRad = 0)

# Measures
crqa_out <- crqa_rp(RM)

# Plot
rp_plot(RM, plotDimensions = TRUE, plotMeasures = TRUE)

# Diagonal profile
crqa_diagProfile(RM,diagWin = 40)

```


## Continuous CRQA and the Diagonal Profile {.tabset .tabset-fade .tabset-pills}


### Questions {-}

* Create two sinewave variables for CRQA analysis and use the $x$ and $y$ coordinates of the circletracing data:

```{r}
y1 <- sin(1:500*2*pi/67)
y2 <- sin(.01*(1:500*2*pi/67)^2)

# Here are the circle trace data
xy <- import("https://raw.githubusercontent.com/FredHasselman/The-Complex-Systems-Approach-Book/master/assignments/assignment_data/RQA_circletrace/mouse_circle_xy.csv") 

circle_x <- xy$x[1:500]
circle_y <- xy$y[1:500]

```

* You have just created two sine(-like) waves. Well examine if and how they are coupled in a shared phase space. 
   + As a first step plot them.
   + Also plot the cirlce trace state space

* Find a common embedding delay and an embedding dimension (if you calculate an embedding dimension for each signal separately, as a rule of thumb use the highest embedding dimension you find in further analyses).

* We can now create an unthresholded cross recurrence matrix. 
    + use function `rp()`
    + Fill in the values you decided on for embedding delay and embedding dimension
    
* Run the CRQA using `crqa_cl()` using th time series as input, or `crqa_rp()` using a thresholded matrix as input.
    + You can find a radius automatically, look in the `casnet` manual for function `crqa_radius()`. 
    + Most functions, like `rp_plot()` will call `crqa_radius()` if no radius is provided in the argume t `emRad`.
    + Request a radius which will give us about 5\% recurrent points (default setting).
    + You can create a thresholded matrix by calling function `di2bi()`,
* Produce a plot of the recurrence matrix using `rp_plot()`. Look at the manual pages.

* Can you understand what is going on? 
  + For the simulated data: Explain the the lack of recurrent points at the beginning of the time series.
  + For the circle trace: How could one see these are not deterministic sine waves?

* Examine the synchronisation under the diagonal LOS. Look in the manual of `crqa_diagProfile()`. More elaborate explanation can be found in [Coco & Dale (2014)](http://journal.frontiersin.org/Journal/10.3389/fpsyg.2014.00510/abstract). 
  + To get the diagonal profile from a recurrence matrix `crqa_diagProfile(RM, winDiag = , ...)`. Where `winDiag` is will indicate the window size `-winDiag ... 0 ... +winDiag`.
  + How far is the peak in RR removed from 0 (Line of Synchronisation)? 

* To check whether the patterns are real by conducting a Cross-RQA with a shuffled/surrogate version of e.g. time series $y2$.
   + Function `crqa_diagProfile()` can do a shuffled analysis, check the manual pages. 
   + Use `Nshuffle = 1` (or wait a long time)

> NOTE: If you generate surrogate timeseries, make sure the RR is the same for all surrogates. Try to keep the RR in the same range by using.


### Answers {-}


* You have just created two sine(-like) waves. Well examine if and how they are coupled in a shared phase space. 
   + As a first step plot them.
   + Also plot the cirlce trace state space

```{r}
library(rio)
y1 <- sin(1:1000*2*pi/67)
y2 <- sin(.01*(1:1000*2*pi/67)^2)

# Here are the circle trace data
xy <- import("https://raw.githubusercontent.com/FredHasselman/The-Complex-Systems-Approach-Book/master/assignments/assignment_data/RQA_circletrace/mouse_circle_xy.csv") 

circle_x <- xy$x[1:1000]
circle_y <- xy$y[1:1000]

# Time series
plot(cbind(ts(y1),ts(y2)))
plot(cbind(ts(circle_x),ts(circle_y)))

# State space
plot(y1,y2,type="l", pty="s",main = "Sine waves")
plot(circle_x,circle_y,type="l", pty="s",main = "Mouse Circle Trace")
```

* Find a common embedding delay and an embedding dimension (if you calculate an embedding dimension for each signal separately, as a rule of thumb use the highest embedding dimension you find in further analyses).

```{r}
params_y1 <- crqa_parameters(y1)
params_y2 <- crqa_parameters(y2)
```

> The diagnostic plot of the sine wave `y1` looks a bit odd. This is due to the fact that mutual information will discretize the time series, and the sine will  fill all bins with the same values, because it is perfectly repeating itself. We'll use emDim = 2 and lag = 1

```{r}
emLag <- 1
emDim <- 2

RM <- rp(y1=y1, y2=y2,emDim = emDim, emLag = emLag)

# Unthresholded Matrix
rp_plot(RM, plotDimensions = TRUE)

# Thresholded Matrix, based on 5% RR
emRad <- crqa_radius(RM = RM, targetValue = .05)$Radius
RP <- di2bi(RM, emRad = emRad)
rp_plot(RP, plotDimensions = TRUE)

# The diagonal profile
drp <- crqa_diagProfile(RP, diagWin = 50)

# Compare CRQA with one randomly shuffled series
drpRND <- crqa_diagProfile(RP, diagWin = 50, doShuffle = TRUE, y1 = y1, y2 = y2, Nshuffle = 19)

```


```{r}
params_cx <- crqa_parameters(circle_x)
params_cy <- crqa_parameters(circle_y)
```

> The circle trace data can be embedded in 3 dimensions. This is not so strange, even though the circle was drawn in 2 dimensions, there are also speed/acceleration and accuracy constraints.


```{r}
emLag <- median(params_cx$optimLag, params_cy$optimLag)
emDim <- max(params_cx$optimDim, params_cy$optimDim)

RM <- rp(y1=circle_x, y2=circle_y,emDim = emDim, emLag = emLag)

# Unthresholded Matrix
rp_plot(RM, plotDimensions = TRUE)

# Thresholded Matrix, based on 5% RR
emRad <- crqa_radius(RM = RM, targetValue = .05)$Radius
RP <- di2bi(RM, emRad = emRad)
rp_plot(RP, plotDimensions = TRUE)

# The diagonal profile
drp <- crqa_diagProfile(RP, diagWin = 100)

```




<!--chapter:end:ASSIGNMENTS_P3B.Rmd-->

---
title: "DCS Assignments Part 4"
author: "Fred Hasselman, Maarten Wijnants & Merlijn Olthof"
date: "1/14/2018"
output: 
  bookdown::html_document2: 
    variant: markdown+hard_line_breaks
    fig_caption: yes
    highlight: pygments
    keep_md: yes
    number_sections: yes
    theme: spacelab
    toc: yes
    toc_float: true
    collapsed: false
    smooth_scroll: true
    code_folding: show
    bibliography: [refs.bib, packages.bib]
    biblio-style: apalike
    csl: apa.csl
    includes:
        before_body: assignmentstyle.html
    pandoc_args: ["--number-offset","7"]
    
---


```{r, include=FALSE}
knitr::opts_chunk$set(include=TRUE, tidy = FALSE, warning = FALSE, message = FALSE)
```


# **Quick Links** {-}

* [Main Assignments Page](https://darwin.pwo.ru.nl/skunkworks/courseware/1718_DCS/assignments/)
* [Assignments Part 1A: Introduction to the mathematics of change](https://darwin.pwo.ru.nl/skunkworks/courseware/1718_DCS/assignments/ASSIGNMENTS_P1A.html)
* [Assignments Part 1B: Fitting Parameters and Potential Functions](https://darwin.pwo.ru.nl/skunkworks/courseware/1718_DCS/assignments/ASSIGNMENTS_P1B.html)
* [Assignments Part 2: Time Series Analysis: Temporal Correlations and Fractal Scaling](https://darwin.pwo.ru.nl/skunkworks/courseware/1718_DCS/assignments/ASSIGNMENTS_P2.html)
* [Assignments Part 3: Quantifying Recurrences in State Space](https://darwin.pwo.ru.nl/skunkworks/courseware/1718_DCS/assignments/ASSIGNMENTS_P3.html)
  
</br>
</br>


# **Early Warning Signals and Complex Networks**


## **Early Warning Signals in a Clinical Case Study**

Dynamic assessment (e.g. ecological momentary assesment) is increasingly used in the mental health field. Process monitoring with dynamic assessment can empower patients and benefit self-insight (van Os et al., 2017). Additionally, ecological momentary assesment (EMA) time series can be analysed and the results can be used to inform the treatment process. 
In this assignment, we will use complex systems theory and methodology to perform a quantitative case study for a real patient. 

### First Steps {.tabset .tabset-fade .tabset-pills}

You will analyse EMA data from a patient who received impatient psychotherapy for mood disorders. The patient answered questions from the therapy process questionnaire on a daily basis for the full 101 days of his treatment. 
The learning objectives of the assignments are as follows:
1) You can interpret plots of raw EMA time series, as well as complexity resonance diagrams and critical instability diagrams of EMA time series.
2) You can describe what possible change processes in complex systems could explain the quantitative results from the case study. 


#### Questions {-}

* Open R and load the data 'clinical.case.study.csv' to a data frame called df.  The data file is on [Github](https://github.com/FredHasselman/The-Complex-Systems-Approach-Book/blob/master/assignments/assignment_data/EarlyWarningSignals/clinical.case.study.csv)
* Look at the data frame. Where is time in this data? In the rows or the columns?
* Add a time variable called 'days' to the data frame. 
* Factor 7 yields the scores on *Impairment by Symptoms and Problems* for this patient on each day. 
    + Plot the time series.
    + What pattern do you see?


#### Answers {-}


* Open R and load the data 'clinical.case.study.csv' to a data frame called df.  The data file is on [Github](https://github.com/FredHasselman/The-Complex-Systems-Approach-Book/blob/master/assignments/assignment_data/EarlyWarningSignals/clinical.case.study.csv)


```{r}
library(rio)
df <- import("https://raw.githubusercontent.com/FredHasselman/The-Complex-Systems-Approach-Book/master/assignments/assignment_data/EarlyWarningSignals/clinical.case.study.csv")

```

* Look at the data frame. Where is time in this data? In the rows or the columns?

> In the rows.

* Add a time variable called 'days' to the data frame. 

```{r}
df$days <- (1:nrow(df))
```

* Factor 7 yields the scores on *Impairment by Symptoms and Problems* for this patient on each day. 
    + Plot the time series.
    + What pattern do you see?

```{r}
plot(df$days, df$factor7, type='l')
```


### Phase Transitions and Early-Warning Signals {.tabset .tabset-fade .tabset-pills}

Before we continue, read this short summary of what you (should) know about phase transitions and early-warning signals: 

```{block2,type="rmdkennen"}
Change from one attractor to another is called a phase transition. Phase transitions are preceded by a destabilization period in which the stability of the existing state decreases (as a consequence of increasing control parameters). During destabilization, the system loses its resilience to external influences leading to increased fluctuations and disorder in the systems behaviour (critical fluctuations) and an increased return time to the existing state after perturbation (critical slowing down). Destabilization ends abruptly when the system makes a phase transition towards a new stable state. Critical fluctuations and critical slowing down can therefore serve as early-warning signals (EWS) for phase transitions. 
```

#### Questions {-}

Answer the following questions, please think and discuss with your neighbours before turning to the answers. You can use the literature and what you've learned in the lecture:

* Can you think of possible phase transitions in psychopathology?
   + Give examples. 
* What is the role of destabilization in the treatment of psychopathology?
* Look at the plot of factor 7 for our patient. 
   + Can you visualize a possible phase transition?
   + Do you expect to find early-warning signals in the data of our patient? 
   + Where?

#### Answers {-}


* Can you think of possible phase transitions in psychopathology?
   + Give examples. 

> Onset of pathology (e.g. psychosis), relapse in addiction, suicide attempts, sudden gains and losses, etc.
   
* What is the role of destabilization in the treatment of psychopathology?

> Destabilization is most of the time a good thing in treatment: the goal of treatment is to change (destabilize) the existing state. During destabilization, the patient is increasingly sensitive to external influences (including treatment). Destabilization can be a target period for intervention.

* Look at the plot of factor 7 for our patient. 
   + Can you visualize a possible phase transition?
   
> There is a sudden gain (abrupt and persisting decrease in symptoms) somewhere around day 40.
   
   + Do you expect to find early-warning signals in the data of our patient? 
   + Where?

> Before the sudden gain

### Phase transitions in symptom severity {.tabset .tabset-fade .tabset-pills}

In the last question, you might have hypothesized the occurrence of a phase transition in the impairment by symptoms and problems of our patient (sudden gain). Let's try a method to quantify level shifts, including sudden gains, in time series data.  

#### Questions {-}

* Analyse the time series of factor 7 using the function `ts_levels()` in package `casnet`. 
    + Look at the manual pages to get an idea what is going. 
    + Use this code: 
    
```{r}
library(casnet)
library(ggplot2)
library(tidyr)

ts_levels(df$factor7, minDataSplit = 12, minLevelDuration = 7, changeSensitivity = 0.01, maxLevels=30, method="anova")
```

* Look at the `summary()` of the `tree` field in the output object
    + Look for `primary splits`

*  Plot the results in the `pred` field of the output object (`x` is `time`, `y` is the original series, `p` is the predicted level of the original series.
*  Describe verbally the change trajectory in the symptom severity of our patient. 

#### Answers {-}

* Analyse the time series of factor 7 using the function `ts_levels()` in package `casnet`. 
    + Look at the manual pages to get an idea what is going. 

> This method uses regression trees to quantify segments of the time series. This is OK for descriptive purposes. Ideally, we would classify a phase transition in a qualitative manner (remember, it is a qualitative shift) but the ts_levels function is very useful to identify discontinous shifts in time series data. 

    + Use this code: 

```{r}
library(casnet)
library(ggplot2)
library(tidyr)

(lvl <- ts_levels(df$factor7, minDataSplit = 12, minLevelDuration = 7, changeSensitivity = 0.05, maxLevels=30, method="anova"))
```

* Look at the `summary()` of the `tree` field in the output object

```{r}
summary(lvl$tree)
```

    + Look for `primary splits`

> 39.5 = 40 days

*  Plot the results in the `pred` field of the output object (`x` is `time`, `y` is the original series, `p` is the predicted level of the original series.

```{r}
# ggplot likes the long data format
lvl_long <- lvl$pred %>% 
  gather(key=variable, value=value, -x)

ggplot(lvl_long, aes(x=x, y=value, colour=variable)) + geom_line() + theme_bw()
```

*  Describe verbally the change trajectory in the symptom severity of our patient. 

> Fluctuations around some level then a shift and then fluctuations again, around another level

### Early Warning Signals {.tabset .tabset-fade .tabset-pills}

Now it is time to analyse the dynamic complexity of the ES time series and see if we can find early-warning signals. 

Dynamic complexity can be calculated from short time series of ordered categorical data (see [Schiepek & Strunk, 2010](https://link.springer.com/content/pdf/10.1007%2Fs00422-009-0362-1.pdf)). We can distinguish between different types of fluctuation (see Figure \@ref(fig:dyncomp))

```{r dyncomp, echo=FALSE, fig.cap="Calculation of Dynamic Complexity (F * D) from a Likert scale time series (figure from Schiepek & Strunk (2010)", message=FALSE, warning=FALSE}
knitr::include_graphics("critical_fluctuation.png")
```



#### Questions {-}

* Some questions in the data were answered on a scale from 0 to 100, others from 0 to 6. We must rescale all questions that are 0-100 to the scale 0-6 before we can analyse EWS. 
    + If you like, you can try to write a for loop for this. You can also look at the answers.
* Analyse the dynamic complexity of the ES time series using the function `dyn_comp()`, ask for a plot.
    + The function returns a list, what is in the list?
* Look at the complexity resonance diagram, do you see early-warning signals for the sudden gain?
* Analyse the presence of critical instabilities using the function `crit_in()`, set `doPlot=TRUE`
    + Look at the critical instability diagram, do you see early-warning signals for the sudden gain?
* Plot the time series of the average dynamic complexity of all items 
* Plot the time series of factor 7 and the time series of the average dynamic complexity in one plot
    + You get a warning that there are NA values. Why are there NA's?

#### Answers {-}


* Some questions in the data were answered on a scale from 0 to 100, others from 0 to 6. We must rescale all questions that are 0-100 to the scale 0-6 before we can analyse EWS. 
    + If you like, you can try to write a for loop for this. You can also look at the answers.


```{r}
#  rescale predictors to max=6
for(j in (9:50)){
  if(max(df[,j])>6){
    df[,j] <- elascer(df[,j],mn = 0, mx = 100, lo = 0, hi = 6)
  }
}
```
    
* Analyse the dynamic complexity of the ES time series using the function `dyn_comp()`, ask for a plot.
    + The function returns a list, what is in the list?

```{r}
dc.case <- dyn_comp(df, win=7, col_first=9, col_last=50, scale_min=0, scale_max=6, doPlot = TRUE)
summary(dc.case)
```

> dataframe + plot object

* Look at the complexity resonance diagram, do you see early-warning signals for the sudden gain?

> Yes

* Analyse the presence of critical instabilities using the function `crit_in()`, set `doPlot=TRUE`
    + Look at the critical instability diagram, do you see early-warning signals for the sudden gain?
    
```{r}
ci.case <- crit_in(dc.case$df.comp, 7)
```

* Plot the time series of the average dynamic complexity of all items 

```{r}
dc.mean <- rowMeans(as.matrix(dc.case$df.comp))
plot(dc.mean, type='l')
```   

* Plot the time series of factor 7 and the time series of the average dynamic complexity in one plot

```{r}
library(dplyr)

# Create a long dataset for ggplot
# Here we use the syntax from the `tidyverse` set of packages to create a 
df %>% 
  mutate(MeanDC=ts_standardise(dc.mean)) %>% 
  select(days,factor7,MeanDC) %>% 
  gather(key=variable,value=value,-days) %>%
  ggplot(aes(x=days, y=value, colour=variable)) + geom_line() + theme_bw()
```

    + You get a warning that there are NA values. Why are there NA's?

> Because dynamic complexity is analyzed over a backwards window. The first value is at day 7.



# **Complex Networks**



To complete the network assignments you need to load:
```{r}
library(igraph)
library(qgraph)
library(devtools)
library(casnet)
```

## **Basic graphs**


```{block2,type="rmdnote"}
The following set of assignments serve as a demonstration to get you aquainted with graph models, just follow the instructions.
```


### Plot basic graph types {.tabset .tabset-fade .tabset-pills}

#### Demos {-}

* Create a small ring graph in `igraph` and plot it.

```{r}
g <- graph.ring(20)
plot(g)
```

    + Look in the manual of the `igraph`package for a function that will get you the `degree` of each node (vertex).
    + Also, find a function to get the average path length (mean distcance).


* Create a "Small world" graph and plot it:

```{r}
g_ws <- watts.strogatz.game(1, 20, 5, 0.05)
plot(g_ws)
```

    + Run `degree(g_ws)`, also look at average path length and `transitivity`


*Get a directed scale-free  graph:

```{r}
g_bara <- barabasi.game(20)
plot(g_bara)
```

    - Get the `degree`, `average path length` and `transitivity`
    - There should be a power law relation between nodes number and degree ... (but this is a very small network)

```{r}
# We can add properties to nodes:
V(g_bara)$degree <- degree(g_bara)
get.vertex.attribute(g_bara)

V(g_bara)$name <- 1:20

df.deg <- data.frame(Node=V(g_bara)$name, Degree = V(g_bara)$degree)
df.deg <- df.deg %>% arrange(Degree)

ggplot(df.deg,aes(x=Node,y=Degree)) + geom_point() + stat_smooth(method = "lm") + theme_bw()
```

### Social Networks {.tabset .tabset-fade .tabset-pills}

The package igraph contains data on a Social network of friendships between 34 members of a karate club at a US university in the 1970s.

> See W. W. Zachary, An information flow model for conflict and fission in small groups, *Journal of Anthropological Research 33*, 452-473 (1977).

#### Demos {-}

* Get a graph of the communities within the social network. 

```{r, eval=TRUE}
# Community membership
karate <- graph.famous("Zachary")
wc <- walktrap.community(karate)
plot(wc, karate)
modularity(wc)
membership(wc)
```

    + Check the `igraph` manual, or any on-line source to figure out what the `modularity` and `membership` measures might mean and how they are calculated.


* It is also possible to get the adjacency matrix of the graph

```{r, eval=TRUE}
get.adjacency(karate)
```

    + What do the columns, rows and 0s and 1s stand for?


### Package `qgraph`

Learn about the functionality of the `qgraph` from its author Sacha Epskamp.

Try running the examples, e.g. of the `Big 5`: 

[http://sachaepskamp.com/qgraph/examples](http://sachaepskamp.com/qgraph/examples)


#### `qgraph` tutorials / blog

Great site by Eiko Fried:
[http://psych-networks.com ](http://psych-networks.com )


### Early warning signals

Have a look at the site [Early Warning Systems](http://www.early-warning-signals.org/home/)

There is an accompanying [`R` package `earlywarnings`](https://cran.r-project.org/web/packages/earlywarnings/index.html)

<!--chapter:end:ASSIGNMENTS_P4.Rmd-->

---
title: "DCS Assignments Part X"
author: "Fred Hasselman & Maarten Wijnants"
date: "1/14/2018"
output: 
  bookdown::html_document2: 
    variant: markdown+hard_line_breaks
    fig_caption: yes
    highlight: pygments
    keep_md: yes
    number_sections: yes
    theme: spacelab
    toc: yes
    toc_float: true
    collapsed: false
    smooth_scroll: true
    code_folding: show
    bibliography: [refs.bib, packages.bib]
    biblio-style: apalike
    csl: apa.csl
    includes:
        before_body: assignmentstyle.html
    pandoc_args: ["--number-offset","7"]
    
---


```{r, include=FALSE}
knitr::opts_chunk$set(include=TRUE, tidy = FALSE)
```


# **Quick Links** {-}

* [Main Assignments Page](https://darwin.pwo.ru.nl/skunkworks/courseware/1718_DCS/assignments/)
* [Assignments Part 1A: Introduction to the mathematics of change](https://darwin.pwo.ru.nl/skunkworks/courseware/1718_DCS/assignments/ASSIGNMENTS_P1A.html)
* [Assignments Part 1B: Fitting Parameters and Potential Functions](https://darwin.pwo.ru.nl/skunkworks/courseware/1718_DCS/assignments/ASSIGNMENTS_P1B.html)
* [Assignments Part 2: Time Series Analysis: Temporal Correlations and Fractal Scaling](https://darwin.pwo.ru.nl/skunkworks/courseware/1718_DCS/assignments/ASSIGNMENTS_P2.html)
* [Assignments Part 3: Quantifying Recurrences in State Space](https://darwin.pwo.ru.nl/skunkworks/courseware/1718_DCS/assignments/ASSIGNMENTS_P3.html)
  
</br>
</br>


# **Early Warning Signals and Complex Networks**


## **Early Warning Signals in a Clinical Case Study**

Dynamic assessment (e.g. experience sampling) is increasingly used in the mental health field. Process monitoring with dynamic assessment can empower patients and benefit self-insight (van Os et al., 2017). Additionally, the experience sampling (ES) time series can be analyzed and the results can be used to inform the treatment process. 
In this assignment, we will use complex systems theory and methodology to perform a quantitative case study for a real patient. 

### First Steps {.tabset .tabset-fade .tabset-pills}

You will analyze ES data from a patient who received impatient psychotherapy for mood disorders. The patient answered questions from the therapy process questionnaire on a daily basis for the full 101 days of his treatment. 
The learning objectives of the assignments are as follows:
1) You can interpret plots of raw ES time series, as well as complexity resonance diagrams and critical instability diagrams of ES time series.
2) You can describe what possible change processes in complex systems could explain the quantitative results from the case study. 


#### Questions {-}

a) Open R and load the data 'clinical.case.study.csv' 
b) Look at the data frame. Where is time in this data? In the rows or the columns?
c) Factor 7 yields the scores on 'Impairment by Symptoms and Problems' for this patient on each day. Plot the time series. What do you see?


#### Answers {-}




### First think, then analyze (,then aggregate) {.tabset .tabset-fade .tabset-pills}

Think about what you know from the literature and the lecture. Since the goal of psychotherapy is to change the existing state of a patient, we expect to find periods of destabilization in which the attraction of the existing psychopathological state is decreased. During a destabilization period, new, possibly more healthy states can emerge. Clinical improvement than constitutes the phase transition from a pathological to a more healthy state.  

Earlier in the course, we learned that destabilization and phase transitions occur as a function of changing control parameters. Additionally, strong external influences can induce destabilization and phase transitions.

#### Questions {-}

Answer the following questions, there are no wrong answers, just try to think from a complex systems perspective:
a)	Can you think of possible control parameters for phase transitions in psychotherapy? 
b)	Can you think of possible external influences and events that relate to destabilization in psychotherapy?
c)	Based on your answers on a) and b), where in the psychotherapeutic process do you expect to find periods of destabilization?
d)	Take a look again at your plot of factor 7 for our patient. Where in the psychotherapeutic process of our patient do you expect to find periods of destabilization?

#### Answers {-}


### Phase transitions in symptom severity {.tabset .tabset-fade .tabset-pills}

In question 2, you might have hypothesized the occurrence of a phase transition in the impairment by symptoms and problems of our patient. Remember, that a phase transition is an abrupt qualitative shift in the systems behaviour. Unfortunately, we cannot see qualitative shifts in our quantitative data. However, we can try to quantify abrupt shifts in factor 7 of our patient. 

#### Questions {-}

a)	Analyze the time series of factor 7 using the function ts_levels()
b)	Describe verbally the change trajectory in the symptom severity of our patient. 

#### Answers {-}




<!--chapter:end:Assignments_P4tmp.Rmd-->

---
title: "DCS Assignments Session 7"
author: "Fred Hasselman, Maarten Wijnants & Merlijn Olthof"
date: "1/14/2019"
output: 
  bookdown::html_document2: 
    variant: markdown+hard_line_breaks
    fig_caption: yes
    highlight: pygments
    keep_md: yes
    number_sections: yes
    theme: spacelab
    toc: yes
    toc_float: true
    collapsed: false
    smooth_scroll: true
    code_folding: show
    bibliography: [refs.bib, packages.bib]
    biblio-style: apalike
    csl: apa.csl
    includes:
        before_body: assignmentstyle.html
    pandoc_args: ["--number-offset","6"]
    
---


```{r, include=FALSE}
knitr::opts_chunk$set(include=TRUE, tidy = FALSE)
```


# **Quick Links** {-}

* [Main Assignments Page](https://darwin.pwo.ru.nl/skunkworks/courseware/1819_DCS/assignments/)

</br>
</br>




# **Potential Theory, Catastrophe Theory and Applied Synergetics: Dynamic Complexity**

Start by doing the **Applied Synergetics: Dynamic Complexity** assignments. The assignments on Potential Functions and Catastrophe Theory are extra.



## **Potential Theory** [EXTRA]


### The Phoneme Boundary Model {.tabset .tabset-fade .tabset-pills}

In this assignment we will look at the potential function used to study speech perception in these studies:

[Tuller, B., Case, P., Ding, M., & Kelso, J. A. (1994). The nonlinear dynamics of speech categorization. Journal of Experimental Psychology: Human perception and performance, 20(1), 3.](https://s3.amazonaws.com/academia.edu.documents/41486712/The_Nonlinear_Dynamics_of_Speech_Categor20160123-5343-1c1fpf2.pdf?AWSAccessKeyId=AKIAIWOWYYGZ2Y53UL3A&Expires=1518665892&Signature=B2jjfU7EbMTFATVYtiZxYAxw608%3D&response-content-disposition=inline%3B%20filename%3DThe_nonlinear_dynamics_of_speech_categor.pdf)

[Case, P., Tuller, B., Ding, M., & Kelso, J. S. (1995). Evaluation of a dynamical model of speech perception. Perception & Psychophysics, 57(7), 977-988.](https://link.springer.com/content/pdf/10.3758%2FBF03205457.pdf)

You do not have to read these articles to do the assignment, but they do provide a great introduction to the kind phenomena you can study empirically, guided by a formal model, without fitting model parameters to data. For example, the model predicts phenomena that are unexpected and inexplicable by other theories of speech perception (e.g. hysteresis), most prominent, if the boundary is approached and the same stimulus is presented a number of times, a switch in perception occurs as if the phoneme boundary is crossed. *Prospective prediction by simulation* is an excellent way to test the plausibility of a model.

#### Questions {-}

Have a look at [the spreadsheet in which the model is implemented](https://docs.google.com/spreadsheets/d/1L6mpHx9SewGGfjValMJ2CzQz3uzOx1PWNaNVnZ4zkew/edit?usp=sharing), copy it, and see how the function changes with values of parameter `k` (between $-1$ and $1$, be sure to check $0$. Note that this is not an iterative function, `V` is 'just' a function of `x`.

* Create the function in `R`
    + remember, no iteration needed!
    + plot the shape for different values of `k`, use $-1,\ -0.5,\ 0 0.5,\ 1$

#### Answers {-}

```{r}
# Variable k
ks = c(-1,-.5,0,.5,1)
x = seq(-2,2,by=.1)
# the function
V <- function(x, k){return(k*x -.5*x^2 + .25*x^4)}

# Plot x against V
op<-par(mfrow=c(1,5))
for(k in ks){
  plot(x,V(x,k),type="l",xlim=c(-2,2),ylim=c(-1.5,2),frame.plot =FALSE,lwd=2)
  abline(h=0,v=0,col="grey")
  }
par(op)

```


### A model of risky choice

Have a look at the paper below, it uses a potential function to model non-linear dynamics in decision making and fits model parameters based on experimental data:

[van Rooij, M. M., Favela, L. H., Malone, M., & Richardson, M. J. (2013). Modeling the dynamics of risky choice. Ecological Psychology, 25(3), 293-303.](http://www.tandfonline.com/doi/abs/10.1080/10407413.2013.810502)

See e.g. p 299 and beyond...


More details [in her PhD thesis](https://etd.ohiolink.edu/!etd.send_file?accession=ucin1382951052&disposition=inline).


## ***Catastrophe Theory** [EXTRA]

* These assignments use nonlinear regression andis written to be completed in `SPSS`, but you can try to use the nonlinear regression functions of 'R' as well.
    - Look at the solutions of the *Extra* [Basic Time Series Analysis](https://darwin.pwo.ru.nl/skunkworks/courseware/1819_DCS/assignments/ASSIGNMENTS_P2.html#2_fitting_parameters_of_analytic_solutions_[extra]) assignments.
* You can also use the R-package `cusp`, or [GEMCAT](https://www.researchgate.net/publication/228299348_An_algorithm_for_estimating_multivariate_catastrophe_models_GEMCAT_II) available as a [Windows  executable](https://www.degruyter.com/view/j/snde.2000.4.3/snde.2000.4.3.1062/snde1062_supplementary_0.zip)


### Fitting the cusp catastrophe in `SPSS` {.tabset .tabset-fade .tabset-pills}

In the file [Cusp Attitude.sav](https://github.com/FredHasselman/DCS/raw/master/assignmentData/Potential_Catastrophe/Cusp%20attitude.sav), you can find data from a (simulated) experiment. Assume the experiment tried to measure the effects of explicit predisposition and [affective conditioning](http://onlinelibrary.wiley.com/doi/10.1002/mar.4220110207/abstract) on attitudes towards Complexity Science, measured in a sample of psychology students using a specially designed Implicit Attitude Test (IAT).


#### Questions {-}

1. Look at a Histogram of the difference score (post-pre) $dZY$. This should look quite normal (pun intended).
   - Perform a regular linear regression analysis predicting $dZY$ (Change in Attitude) from $\alpha$ (Predisposition). 
   - Are you happy with the $R^2$?
2. Look for Catastrophe Flags: Bimodality. Examine what the data look like if we split them across the conditions.
  - Use $\beta$ (Conditioning) as a Split File Variable (`Data >> Split File`). And again, make a histogram of $dZY$.
  - Try to describe what you see in terms of the experiment.
  -	Turn Split File off. Make a Scatterplot of $dZY$ (x-axis) and $\alpha$ (y-axis). Here you see the bimodality flag in a more dramatic fashion. 
   - Can you see another flag?
3. Perhaps we should look at a cusp Catastrophe model:
   - Go to `Analyse >> Regression >> Nonlinear` (also see [Basic Time Series Analysis](#bta)). First we need to tell SPSS which parameters we want to use, press `Parameters.` 
    - Now you can fill in the following: 
        + `Intercept` (Starting value $0.01$)
        + `B1` through `B4` (Starting value $0.01$)
        + Press `Continue` and use $dZY$ as the dependent. 
    - Now we build the model in `Model Expression`, it should say this: 
    ```
    Intercept + B1 * Beta * ZY1 + B2 * Alpha + B3 * ZY1 ** 2 + B4 * ZY1 ** 3
    ```
    - Run! And have a look at $R^2$.
4. The model can also be fitted with linear regression in SPSS, but we need to make some extra (nonlinear) variables using `COMPUTE`:

```
BetaZY1 = Beta*ZY1	*(Bifurcation, splitting parameter).
ZY1_2 = ZY1 ** 2	*(ZY1 Squared).
ZY1_3 = ZY1 ** 3	*(ZY1 Cubed).
```
  - Create a linear regression model with $dZY$ as dependent and $Alpha$, $BetaZY1$ and $ZY1_2$ en $ZY1_3$ as predictors. 
  - Run! The parameter estimates should be the same.
5. Finally try to can make a 3D-scatterplot with a smoother surface to have look at the response surface.
  - HINT: this is a lot easier in `R` or `Matlab` perhaps you can export your `SPSS` solution.

6. How to evaluate a fit? 
   - Check the last slides of lecture 8 in which the technique is summarised. 
   - The cusp has to outperform the `pre-post` model.

#### Answers {-}

Some syntax (figures) might fail on the newest versions of SPSS.

```
GRAPH
  /HISTOGRAM=dZY .

*Linear regression.

REGRESSION
  /MISSING LISTWISE
  /STATISTICS COEFF OUTS R ANOVA
  /CRITERIA=PIN(.05) POUT(.10)
  /NOORIGIN
  /DEPENDENT dZY
  /METHOD=ENTER Alpha.

*Flags 1.

SORT CASES BY Beta .
SPLIT FILE
  SEPARATE BY Beta .

GRAPH
  /HISTOGRAM=dZY .

SPLIT FILE
  OFF.

*Flags 2.

GRAPH
  /SCATTERPLOT(BIVAR)=dZY WITH Alpha
  /MISSING=LISTWISE .

*Cusp with nonlinear regression.

MODEL PROGRAM Intercept=0.01 B1=0.01 B2=0.01 B3=0.01 B4=0.01 .
COMPUTE PRED_ = Intercept + B1 * Beta * ZY1 + B2 * Alpha + B3 * ZY1 ** 2 + B4 * ZY1 ** 3.
NLR dZY
  /PRED PRED_
  /CRITERIA SSCONVERGENCE 1E-8 PCON 1E-8 .

*Cusp with linear regression.

COMPUTE BetaZY1 = Beta*ZY1 .
COMPUTE ZY1_2 = ZY1 ** 2 .
COMPUTE ZY1_3 = ZY1 ** 3 .
EXECUTE .

REGRESSION
  /MISSING LISTWISE
  /STATISTICS COEFF OUTS R ANOVA
  /CRITERIA=PIN(.05) POUT(.10)
  /NOORIGIN
  /DEPENDENT dZY
  /METHOD=ENTER Alpha BetaZY1 ZY1_2 ZY1_3  .

*3-D scatter.

IGRAPH /VIEWNAME='Scatterplot' /X1 = VAR(Beta) TYPE = SCALE /Y = VAR(dZY) TYPE = SCALE /X2 = VAR(Alpha) TYPE = SCALE
 /COORDINATE = THREE  /FITLINE METHOD = LLR NORMAL BANDWIDTH = FAST X1MULTIPLIER = 1.00  X2MULTIPLIER = 1.00 LINE = TOTAL
  SPIKE=OFF /X1LENGTH=3.0 /YLENGTH=3.0 /X2LENGTH=3.0 /CHARTLOOK='D:\Program Files\SPSS\Looks\dots.clo' /SCATTER COINCIDENT =
  NONE.
EXE.
```

Download this [SPSS syntax from GitHub](https://raw.githubusercontent.com/FredHasselman/DCS/master/assignmentData/Potential_Catastrophe/Cusp%20attitude%20solution.SPS)



### The `cusp` package in `R`

Use this tutorial paper to fit the cusp in 'R' according to a maximum likelihood criterion.

[Grasman, R. P. P. P., Van der Maas, H. L. J., & Wagenmakers, E. (2007). Fitting the Cusp Catastrophe in R: A cusp-Package.](https://cran.r-project.org/web/packages/cusp/vignettes/Cusp-JSS.pdf)

1. Start R and install package `cusp` 

2. Work through the *Example I* (attitude data) in the paper (*Section 4: Examples*, p. 12).

3. *Example II* in the same section is also interesting, but is based on simulated data.
    * Try to think of an application to psychological / behavioural science.


# **Applied Synergetics: Dynamic Complexity**


## *Early Warning Signals in a Clinical Case Study*

Dynamic assessment (e.g. ecological momentary assesment) is increasingly used in the mental health field. Process monitoring with dynamic assessment can empower patients and benefit self-insight (van Os et al., 2017). Additionally, ecological momentary assesment (EMA) time series can be analyzed and the results can be used to inform the treatment process. 
In this assignment, we will use complex systems theory and methodology to perform a quantitative case study for a real patient. 

### First Steps {.tabset .tabset-fade .tabset-pills}

You will analyze EMA data from a patient who received impatient psychotherapy for mood disorders. The patient answered questions from the therapy process questionnaire on a daily basis for the full 101 days of his treatment. 
The learning objectives of the assignments are as follows:
1) You can interpret plots of raw EMA time series, as well as complexity resonance diagrams and critical instability diagrams of EMA time series.
2) You can describe what possible change processes in complex systems could explain the quantitative results from the case study. 


#### Questions {-}

a) Open R and load the data 'clinical.case.study.csv' to a dataframe called df. 
b) Look at the data frame. Where is time in this data? In the rows or the columns?
c) Add a time variable called 'days' to the dataframe. 
d) Factor 7 yields the scores on 'Impairment by Symptoms and Problems' for this patient on each day. Plot the time series. What do you see?


#### Answers {-}

a) The data file is on [Github](https://github.com/FredHasselman/The-Complex-Systems-Approach-Book/blob/master/assignments/assignment_data/EarlyWarningSignals/clinical.case.study.csv)
```{r}
library(rio)
df <- import("https://raw.githubusercontent.com/FredHasselman/The-Complex-Systems-Approach-Book/master/assignments/assignment_data/EarlyWarningSignals/clinical.case.study.csv")

```

b) In the rows.

c) 
```{r}
df$days <- (1:nrow(df))
```
d) 
```{r}
plot(df$days, df$factor7, type='l')
```


### Phase Transitions and Early-Warning Signals {.tabset .tabset-fade .tabset-pills}

Before we continue, read this short summary of what you (should) know about phase transitions and early-warning signals: 

```{block2,type="rmdkennen"}
Change from one attractor to another is called a phase transition. Phase transitions are preceded by a destabilization period in which the stability of the existing state decreases (as a consequence of increasing control parameters). During destabilization, the system loses its resilience to external influences leading to increased fluctuations and disorder in the systems behaviour (critical fluctuations) and an increased return time to the existing state after perturbation (critical slowing down). Destabilization ends abruptly when the system makes a phase transition towards a new stable state. Critical fluctuations and critical slowing down can therefore serve as early-warning signals (EWS) for phase transitions. 
```

#### Questions {-}

Answer the following questions, please think and discuss with your neighbours before turning to the answers. You can use the literature and what you've learned in the lecture:

a) Can you think of possible phase transitions in psychopathology? Give examples. 
b)What is the role of destabilization in the treatment of psychopathology?
c) Look at the plot of factor 7 for our patient. Can you visualize a possible phase transition?
d) Do you expect to find early-warning signals in the data of our patient? And where?

#### Answers {-}

a) Onset of pathology (e.g. psychosis), relapse in addiction, suicide attempts, sudden gains and losses, etc.
b) Destabilization is most of the time a good thing in treatment: the goal of treatment is to change (destabilize) the existing state.
During destabilization, the patient is increasingly sensitive to external influences (including treatment). Destabilization can be a target period for intervention.
c) There is a sudden gain (abrupt and persisting decrease in symptoms) somewhere around day 40.
d) Before the sudden gain

### Phase transitions in symptom severity {.tabset .tabset-fade .tabset-pills}

In the last question, you might have hypothesized the occurrence of a phase transition in the impairment by symptoms and problems of our patient (sudden gain). Let's try a method to quantify level shifts, including sudden gains, in time series data.  

#### Questions {-}

a)	Analyze the time series of factor 7 using the function `ts_levels()` in casnet. Look at the manual pages to get an idea what is going. Use this code: `casnet::ts_levels(df$factor7, minLevelChange=1, minLevelDuration = 7, changeSensitivity = 0.01, maxLevels=30, method="anova")`.
b) Look at the `summary()` of the `tree` field in the output object.
c) Plot the results in the `pred` field of the output object (`x` is `time`, `y` is the original series, `p` is the predicted level of the original series.
d) Describe verbally the change trajectory in the symptom severity of our patient. 

#### Answers {-}

a) This method uses regression trees to quantify segments of the time series. This is OK for descriptive purposes. Ideally, we would classify a phase transition in a qualitative manner (remember, it is a qualitative shift) but the ts_levels function is very useful to identify discontinous shifts in time series data. 

```{r}
library(casnet)
library(ggplot2)
library(tidyr)
lvl <- ts_levels(df$factor7, minDataSplit = 12, minLevelDuration = 7, changeSensitivity = 0.05, maxLevels=30, method="anova") 
```

b) Look for `primary splits`

```{r}
summary(lvl$tree)
```

c)
```{r}
# ggplot likes the long data format
lvl_long <- lvl$pred %>% 
  gather(key=variable, value=value, -x)

ggplot(lvl_long, aes(x=x, y=value, colour=variable)) + geom_line() + theme_bw()
```

d) Flutucations, shift, fluctuations.

### Early Warning Signals {.tabset .tabset-fade .tabset-pills}

Now it is time to analyze the dynamic complexity of the ES time series and see if we can find early-warning signals. 

#### Questions {-}

a) Some questions in the data where answered on a scale from 0 to 100, others from 0 to 6. We must rescale all questions that are 0-100 to the scale 0-6 before we can analyze EWS. If you like, you can try to write a for loop for this. You can also look at the first answer.  
b) Analyze the dynamic complexity of the ES time series using the function dyn_comp(), ask for a plot.
c) The function returns a list, what is in the list?
d) Look at the complexity resonance diagram, do you see early-warning signals for the sudden gain?
e) Analyze the presence of critical instabilities using the function crit_in(), set doPlot=TRUE
f) Look at the critical instability diagram, do you see early-warning signals for the sudden gain?
g) Plot the time series of the average dynamic complexity of all items 
h) Plot the time series of factor 7 and the time series of the average dynamic complexity in one plot
i) You get a warning that there are NA values. Why are there NA's?

#### Answers {-}

a) rescale predictors to max=6
```{r}
for(j in (9:50)){
  if(max(df[,j])>6){
    df[,j] <- (df[,j]/100)*6
  }
}

#cID <- which(plyr::colwise(max)(df[,-c(1,51)])>6)+1
#df[,cID] <- elascer(df[cID,],lo = 0, hi = 6)

```
b) 
```{r}
dc.case <- dyn_comp(df, win=7, col_first=9, col_last=50, scale_min=0, scale_max=6, doPlot = TRUE)
```
c) dataframe + plot object
d) 
```{r}
dc.case$plot.dc
```
e) 
```{r}
ci.case <- crit_in(dc.case$df.comp, 7)
```
f) 
```{r}
ci.case$plot.ci
```
g) 
```{r}
dc.mean <- rowMeans(as.matrix(dc.case$df.comp))
plot(dc.mean, type='l')
```   
h) 
```{r}
library(dplyr)
library(ggplot2)

# Create a long dataset for ggplot
# Here we use the syntax from the `tidyverse` set of packages to create a 
df %>% 
  mutate(MeanDC=ts_standardise(dc.mean)) %>% 
  select(days,factor7,MeanDC) %>% 
  gather(key=variable,value=value,-days) %>%
  ggplot(aes(x=days, y=value, colour=variable)) + geom_line() + theme_bw()
```
i) Because dynamic complexity is analyzed over a backwards window. The first value is at day 7.




<!--chapter:end:Assignments_P5.Rmd-->

---
title: "DCS Assignments Session 8"
author: "Fred Hasselman"
date: "1/14/2019"
output: 
  bookdown::html_document2: 
    variant: markdown+hard_line_breaks
    fig_caption: yes
    highlight: pygments
    keep_md: yes
    number_sections: yes
    theme: spacelab
    toc: yes
    toc_float: true
    collapsed: false
    smooth_scroll: true
    code_folding: show
    bibliography: [refs.bib, packages.bib]
    biblio-style: apalike
    csl: apa.csl
    includes:
        before_body: assignmentstyle.html
    pandoc_args: ["--number-offset","7"]
    
---


```{r, include=FALSE}
knitr::opts_chunk$set(include=TRUE, tidy = FALSE)
```


# **Quick Links** {-}

* [Main Assignments Page](https://darwin.pwo.ru.nl/skunkworks/courseware/1819_DCS/assignments/)

</br>
</br>


# **Complex Networks**

To complete these assignments you need these packages:
```{r, message=FALSE, warning=FALSE}
library(igraph)
library(qgraph)
library(casnet) # 
# devtools::install_github("FredHasselman/casnet")
```


* There are many frameworks for analysing and plotting graphs and they are implemented on different platforms often in more-or-less the same way. We'll use the `igraph` framework: https://igraph.org for `R`, but there many more options, see e.g. https://github.com/briatte/awesome-network-analysis#r
* The manual pages of package `igraph` in `R` provide a lot of information, but not everything the package can do. One thing that is missing are  the parameters available to customise the graphs for plotting. If you can't find something in the `R` manual, have a look at the online documentation: https://igraph.org/r/doc/plot.common.html
* Package `qgraph` was developped to fit and analyse the so-called `Gaussian Graph Model`, e.g in the symptom network studies. It is built on the `igraph` framework and you can transform `qgraph` output to an `igraph` object using the function `qgraph::as.igraph.qgraph()`
* The functions in package `casnet` developped to create, analyse and plot **recurrencde networks** are also based on the `igraph` framework. 


## Creating, plotting and describing simple graphs {.tabset .tabset-fade .tabset-pills}

We'll start using `igraph` and create some simple graphs. 

To create a small ring graph and plot it run the code below.

```{r}
gR <- graph.ring(20)

# This will display the igraph structure
gR

# To plot, simply call:
plot(gR)
```

You can request to represent the graph as a so-called *edge-list* which just lists the edges in a graph by reporting the nodes the edge connects *from* - *to*

```{r}
get.edgelist(gR)

```

Other useful functions are `E()` and `V()` for manipulating attributes of *Edges* (connections) and *Vertices* (nodes). See  https://igraph.org/r/doc/plot.common.html for all available attributes.

```{r}
# List vertices
V(gR) 
# List edges
E(gR)

# Set vertex color
V(gR)$color <- "red3"
# set vertex label
V(gR)$label.color <- "white"
# Set vertex shape
V(gR)$shape <- "square"
# Set edge color
E(gR)$color <- "steelblue"
# Set efge size
E(gR)$width <- 4

# Plot
plot(gR)
```


### Questions {-}

* What will be the degree of each node in the ring graph?
    - To check your answer, look in the manual pages of the `igraph` package for a function that will get you the degree of each node
    - Also, get the average path length.
* Create a "small world" graph like the one descibed by [Watts & Strogatz (1998)](https://www.nature.com/articles/30918) and plot it, using the `igraph` function `watts.strogatz.game()`. 
     - Use dimension `dim = 1`, number of nodes `size = 20`, neighbourhood `nei = 5`, and connection probability `p = .05`
     - Get the degree, average path length and transitivity of this graph.
     - `qgraph` has a function called `smallworldness()` that allows you to calculate the so-called *Small Worls Index* [(Humphries & Gurney, 2008)](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0002051). Calulate the smallwordlness of the graph, a value `> 1` implies a small world network. 
     - 
* Create a directed scale-free graph using `igraph` function `barabasi.game()` with 100 nodes `n=100`.
     - Get the degree, average path length and transitivity
     - There should be a power-law relation between the number og nodes that have a specific degree... (but this is a very small network)


### Answers {-}


```{r}
# Ring graph degree
degree(gR)

# avergage path length
average.path.length(gR)
```



```{r}
# Small world network
gSW <- igraph::watts.strogatz.game(dim = 1, size = 20, nei = 5, p =0.05)

# Plot
plot(gSW)

# Degree
cbind(node = V(gSW), connections = degree(gSW))

# Average path length
igraph::average.path.length(gSW)

# Transitivity
igraph::transitivity(gSW)

# Complute the Small World Index, aka 'smallworldness'
qgraph::smallworldness(gSW)
```


```{r}
# Scale-free network
gSF <- barabasi.game(100)

## Change some aesthetics
# No numbers in nodes
V(gSF)$name <- ""
# Change node size
V(gSF)$size = 5
# Change Edge size
E(gSF)$arrow.size = .5
E(gSF)$arrow.width = .5


# Plot
plot(gSF)

# Degree
rbind(node = V(gSF), connections = degree(gSF))

# Average path length
average.path.length(gSF)

# Transitivity
transitivity(gSF)

## Power-law?
# Get degree distribution
dist <- degree_distribution(gSF)
# Note nonzero elements for log trasnsform
ID <- dist>0
# Get an index for node groups
nodes <- seq_along(dist)

# Fit a line in log-log space
lf  <- lm(log(dist[ID])~log(nodes[ID]))
lfit <- predict(lf)

# Plot it!
plot(nodes,dist,pch=16,log = "xy")
lines(x = nodes[ID],y = exp(lfit), col="red")
```

  
### Small World Index Demo {-}

* Select and run all the code below, this will display three graphs with different topology:
    + Many 'local' connections, only between neighbouring nodes.
    + A few nodes with many connections, many nodes with a few connections
    + Relatively many nodes with many connections, a few nodes with just a few connections
* The average number of neighbours of the first order (*FON*) for each graph is the same (`7`)
* The differences between the connectivity are adequately captured by the Small World Index
  
The Small World Index is calculated by comparing the average path length and transitiviy of randomly connected graphs, it is similar to a surrogsate test. If you look at the code of function `SWtestE()` you can an idea how it is done.

```{r}
# Initialize
set.seed(660)
layout1=layout.kamada.kawai
k=3
n=50

# Setup plots
op <- par(mfrow=c(1,3),mar= c(5,2,2,2))

# Strogatz rewiring probability = .00001
p   <- 0.00001
p1  <- plotNET_SW(n=n,k=k,p=p,doPlot = FALSE)
p11 <- plot(p1, main="p = 0.00001",
            layout=layout1,
            xlab=paste("FON = ",round(mean(neighborhood.size(p1,order=1)),digits=1), "\nSWI = ", round(SWtestE(p1,N=100)$valuesAV$SWI,digits=2)))

# Strogatz rewiring probability = .01
p   <- 0.01
p2  <- plotNET_SW(n=n,k=k,p=p,doPlot = FALSE)
p22 <- plot(p2,main="p = 0.01",layout=layout1,
            xlab=paste("FON = ",round(mean(neighborhood.size(p2,order=1)),digits=1),"\nSWI = ", round(SWtestE(p2,N=100)$valuesAV$SWI,digits=2)))
# Strogatz rewiring probability = 1
p   <- 1
p3  <- plotNET_SW(n=n,k=k,p=p,doPlot = FALSE)
p33 <- plot(p3,main="p = 1",layout=layout1,xlab=paste("FON = ",round(mean(neighborhood.size(p3,order=1)),digits=1),"\nSWI = ", round(SWtestE(p3,N=100)$valuesAV$SWI,digits=2)))

# restore graphics parameters
par(op)

```


## Social Networks 

The package `igraph` contains data on a Social network of friendships between 34 members of a karate club at a US university in the 1970s.

> See W. W. Zachary, An information flow model for conflict and fission in small groups, *Journal of Anthropological Research 33*, 452-473 (1977).


* Get a graph of the **community structure** within the social network. 
    - There are many procedures for finding communities, or, subgraphs, in graphs, try to understand how the`walktrap` procedure finds them.
    - Check the manual pages to figure out what kind of information the functions `sizes`, `communities`, `membership` and `modularity` provide
    

```{r}
# Community membership
karate <- graph.famous("Zachary")
wc     <- cluster_walktrap(karate)
plot(wc, karate)

# Measures
modularity(wc)

sizes(wc)
communities(wc)
cbind(node = V(karate) , community = membership(wc))

# Is the structure a hierarchy?
if(is_hierarchical(wc)){
  plot(as.dendrogram(wc))
}
```


* It is also possible to get the adjacency matrix of a graph
    - What do the columns, rows and 0s and 1s stand for?
    - Looks a bit like a recurrence matrix, except, the rows and columns don't represent an (embedded) time series 

```{r}
(am <-get.adjacency(karate))


rp_plot(am)
```


## Recurrence Networks {.tabset .tabset-fade .tabset-pills}

Recurrence networks are graphs created from a recurrence matrix. This means the nodes of the graph represent time points and the connections between nodes represent a recurrence relation betwen the values observed at those time points. That is, often the matrix represents recurrences in a reconstructed state space, the values are coordinates and therefore we would say the edges of a recurrence network represent a temporal relation between recurring states. The ultimate reference for learning everything about recurrence networks is:

> [Zou, Y., Donner, R. V., Marwan, N., Donges, J. F., & Kurths, J. (2018). Complex network approaches to nonlinear time series analysis. Physics Reports](https://www.sciencedirect.com/science/article/abs/pii/S037015731830276X?via%3Dihub)


Package `casnet` has some functions to create recurrence networks, they are similar to the functions used for CRQA:
* `rn()` is very similar to `rp()`, it will create a matrix based on embedding parameters. One difference is the option to create a weighted matrix. This is a matrix in which non-recurring values are set to 0, but the recurring values are not replaced by a 1, the distance value is retained and acts as an edge-weight
* `rn_plot()` will produce the same as `rp_plot()`

We can turn the recurrence matrix into an adjecency matrix, an `igraph` object. This means we can use all the `igraph` functions to calculate network measures.

```{r}
# Reload the data we used earlier
series <- rio::import("https://github.com/FredHasselman/The-Complex-Systems-Approach-Book/raw/master/assignments/assignment_data/BasicTSA_arma/series.sav", setclass = "tbl_df")

# Lets use a shorter dataset to speed things up
series <- series[1:500,]
```

We'll analyse the three time series as a recurrence network:
* Compare the results, look at the SMall World Index and other measures
   - Remember: TS_1 was white noise, TS_2 was a sine with added noise, TS_3 was the logistic map in the chaotic regime.
* Note that some of the RQA measures can be exactly calculated from the measures of the network representation.
   - Try to understand why the Recurrence is reprented as the *degree centrality() of the network (`igraph::centr_degree()`)


### TS 1 {-}

```{r message=FALSE, warning=FALSE}
#----------------------
# Adjacency matrix TS_1
#----------------------
plot(ts(series$TS_1))

# By passing emRad = NA, a radius will be calculated
RN1 <- rn(y1 = series$TS_1, emDim = 1, emLag = 1, emRad = NA)
rn_plot(RN1)

# Get RQA measures
rqa1 <- crqa_rp(RN1)
knitr::kable(rqa1,digits = 2)

g1 <- graph_from_adjacency_matrix(RN1, mode="undirected", diag = FALSE)
g1 <- plotNET_prep(g1)

# Network measures
average.path.length(g1)
transitivity(g1)
smallworldness(g1)

recs1 <- centr_degree(g1)

(RP_N <- sum(recs1$res))
rqa1$RP_N

RP_N / recs1$theoretical_max
rqa1$RR
```

### TS 2 {-}

```{r message=FALSE, warning=FALSE}
#----------------------
# Adjacency matrix TS_2
#----------------------
plot(ts(series$TS_2))

RN2 <- rn(y1 = series$TS_2, emDim = 1, emLag = 1, emRad = NA)
rn_plot(RN2)

# Get RQA measures
rqa2 <- crqa_rp(RN2)
knitr::kable(rqa2,digits = 2)

g2 <- graph_from_adjacency_matrix(RN2, mode="undirected", diag = FALSE)
g2 <- plotNET_prep(g2)

# Network measures
average.path.length(g2)
transitivity(g2)
smallworldness(g2)

recs2 <- centr_degree(g2)

(RP_N <- sum(recs2$res))
rqa2$RP_N

RP_N / recs2$theoretical_max
rqa2$RR
```

### TS 3 {-}

```{r message=FALSE, warning=FALSE, paged.print=TRUE}
#----------------------
# Adjacency matrix TS_3
#----------------------
plot(ts(series$TS_3))

RN3 <- rn(y1 = series$TS_3, emDim = 1, emLag = 1, emRad = NA)
rn_plot(RN3)

# Get RQA measures
rqa3 <- crqa_rp(RN3)
knitr::kable(rqa3,digits = 2)

g3 <- graph_from_adjacency_matrix(RN3, mode="undirected", diag = FALSE)
g3 <- plotNET_prep(g3)

average.path.length(g3)
transitivity(g3)
smallworldness(g3)

recs3 <- centr_degree(g3)

(RP_N <- sum(recs3$res))
rqa3$RP_N

RP_N / recs3$theoretical_max
rqa3$RR
```





## Symptom Networks: Package `qgraph`

Learn about the functionality of the `qgraph` frm its author Sacha Epskamp.

Try running the examples, e.g. of the `Big 5`: 

[http://sachaepskamp.com/qgraph/examples](http://sachaepskamp.com/qgraph/examples)


### `qgraph` tutorials / blog

Great site by Eiko Fried:
[http://psych-networks.com ](http://psych-networks.com )




<!-- ## Early Warnings -->

<!-- Have a look at the site [Early Warning Systems](http://www.early-warning-signals.org/home/) -->

<!-- There is an accompanying [`R` package `earlywarnings`](https://cran.r-project.org/web/packages/earlywarnings/index.html) -->



<!--chapter:end:Assignments_S8.Rmd-->

---
output:
  html_document: default
  pdf_document: default
---

```{r, include=FALSE}
knitr::opts_chunk$set(include=TRUE)
```

# **Assignments: How to ... ** {-}

These assignments were designed to prepare you for "real world" modelling and data analysis problems. That is, after completing the assignments you should be able to decide whether the phenomenon you study could benefit from a complex systems approach and which type of analyses would be a good place to start. The models and techniques discussed here are **not** a definite collection of available techniques, this is really just the tip of the iceberg.

### General Guidelines {-}

* Read the instructions carefully.
* Do not skip any of the steps.
* Do not copy-paste from the assignment text into a spreadsheet or syntax editor (except for text in `code blocks`).
* Study the solutions and lecture notes.


# (PART) Introduction to the mathematics of change {-}

# **Modelling (nonlinear) growth**  {#moc1ass} 

In this assignment you will build two (relatively) simple one-dimensional maps in `R`, based on an example in a spreadsheet. 
Go to the followin [GoogleSheet](https://docs.google.com/spreadsheets/d/10PpXKHHcbuwtWsTWvBKPFj2TqwpCH_BGxtRZhtMo71E/edit#gid=1744375290) and save a copy on your computer, you can use your favourite spreadsheet software (e.g., Excel, Numbers), or, copy it to a new GoogleSheet if you prefer Google.  

We will start with the *Linear Map* and then proceed to the slightly more complicated *Logistic Map* (aka *Quadratic map*). 

```{block2, useR, type='rmdimportant'}
Be sure to check [the solutions of the assigment](#moc1Rsol) and the examples of [different ways to visualize the time series in `R`](#tsPlot)
```

## The Linear Map in a Spreadsheet

Equation \@ref(eq:linmap) is the ordinary difference equation (ODE) discussed on the slides and is called the *Linear Map*:

\begin{equation}
Y_{t+1} = Y_{t=0} + r*Y_t
(\#eq:linmap)
\end{equation}

In these excersises you will *simulate* time series produced by the change process in equation \ref@(eq:linmap) for different parameter settings of the growth-rate parameter $r$ (the *control parameter*) and the initial conditions $Y_0$. Simulation is obvioulsy different from a statistical analysis in which parameters are estimated from a data set. The goal of these assignments is to get a feeling for what a dynamical model is, and how it is different from a linear statistical regression model like GLM.

```{block2, spreadset, type='rmdcaution'}
Before you begin, be sure to check the following settings: 

* Open a Microsoft Excel worksheet, a [Google sheet](https://www.google.com/docs/about/), or other spreadsheet.
* Check whether the spreadsheet uses a 'decimal-comma' ($0,05$) or 'decimal-point' ($0.05$) notation. 
    + The numbers given in the assignments of this course all use a 'decimal-point' notation.
* Check if the `$` symbol fixes rows and columns when it used in a formula in your preferred spreadsheet program. 
    + This is the default setting in Microsoft Excel, Numbers and Google. If you use one of those programs you are all set, otherwise you will have to replace the `$` used in the assignments with the one used by your software.
```

* Open the spreadsheet and look at the **Linear map** tab.
* The `r` in cell `A5` is the *control parameter*. It currently has the value $1.08$ in cell`B5`.
* The cell labelled $Y_0$ in `A6` is the *initial value* at $t=0$. It has the value $0.1$ in cell `B6`.   
* The `A` column also contains $Y_t$, the output of the linear map. 
    + Click on `A10`, in the fomula bar you can see this refers to the initial value `B6`.
    + Click on `A11`, in the fomula bar you can see the very simple implementation of the linear map.
    + The value of cell `A11` (i.e. $Y_{t=1}$) will be calculated by multiplying the value of cell `B5` (parameter `r`) with the value of cell `A10` (the previous value, here: $Y_{t=0}$).
* Remember what it is we are doing! We are calculating $Y_{t=2}$ (i.e. 'behaviour' in the future) and this is completely determined by $Y_{t=1}$ (i.e., the previous step) and the parameter `r`. 


### The control parameter {-}

* If you change the values in cells `B5` and `B6` you will see an immediate change in the graph. To study model behaviour, try the following growth parameters:

    + $r = -1.08$
    + $r = 1,08$
    + $r = 1$
    + $r = -1$

### Dependence on initial conditions? {-}

* Change the initial value $Y_0$ in cell `B6` to $10$.
* Subsequently give the growth parameter in cell `B5` the following values:

    + $0.95$
    + $-0.95$.


## The Logistic Map in a spreadsheet

The Logistic Map takes the following functional form: 

\begin{equation}
Y_{t+1} = r*Y_t*(1-Y_t)
(\#eq:logmap)
\end{equation}

* Open the spreadsheet and look at the **Logistic map** tab.
* The setup is the same as for the linear map, except of course the change process that is implemented.
* The `A` column also contains $Y_t$, the output of the Logistic map, currently the behaviour looks a lot like an S-curve, a logistic function. 
    + Click on `A10`, in the fomula bar you can see this refers to the initial value `B6`.
    + Click on `A11`, in the fomula bar you can see the very simple implementation of the Logistic map.
    + Again, the value of cell `A11` ($Y_{t=1}$) is calculated by multiplying the value of cell `B5` (parameter `r`) with the value of cell `A10` (the current value $Y_t$, in this case $Y_{t=0}$). The only difference is we are also multiplying by $(1-Y_t)$.

```{block2, QLinMap, type='rmdimportant'}
We established that `r` controls growth, what could be the function of $(1-Y_t)$?
```


### The control parameter {-}

To study the behavior of the Logistic Map you can start playing around with the values in `B5`. 

* Try the following settings for $r$:
    + $r = 0.9$
    + $r = 1.9$
    + $r = 2.9$
    + $r = 3.3$
    + $r = 3.52$
    + $r = 3.9$


### The return plot {-}

* Set $r$ at $4.0$:
    + Now copy `A10:A310` to `B9:B309` (i.e., move it one cell to the right, and one cell up)
    + Select both columns (`A10` to `B309`!) and make a scatter-plot

The plot you just produced is a so called **return plot**, in which you have plotted $Y_{t+1}$ against $Y_t$. 

```{block2, QReturnPlot, type='rmdimportant'}
Can you explain the pattern you see (a 'parabola') by looking closely at the functional form of the Logistic Map? (hint: it's also called **Quadratic Map**)
```

* What do you expect the return plot of the Linear Map will look like? Go back and try it!


### Sensitive dependence on initial conditions

Go to the following [GoogleSheet](https://docs.google.com/spreadsheets/d/1pKFhe5JcXXo6vK-Nx-GvxafaYkEZx31ya57I6xpWWMA/edit?usp=sharing) and download or copy it.

This sheet displays the extreme sensitivity on initial conditions of systems displaying chaotic behaviour, more commonly known as: **The Butterfly Effect**

* Imagine these are observed timseries from two subjects in a response time experiment. These subjects are perfect 'twins':
   + The formula describing their behaviour is exactly the same (it's the Quadratic Map, check it!)
   + The control parameter, controlling the behavioural mode is exactly the same ($r=4$).
   + The only difference is that they have a tiny discrepancy in intitial conditions in cell `D6` of $0.01$.
* Not tiny enough? Change it to become even smaller:
   + $0.001$
   + $0.0001$
   + $0.00001$
   + etc.
* What happens as you make the discrepency smaller and smaller?


## Using `R` or `Matlab` to simulate these maps. {#moc1R}

The best (and easiest) way to simulate these simple models is to create a function which takes as input the parameters ($Y_0$, $r$) and a variable indicating the length of the time series.

For example for the Linear Map:
```{r, eval=FALSE, include=TRUE, tidy=FALSE}
# A function with three arguments
linearMap <- function(Y0 = 0, r = 1, N = 100){
    
    # Initialize Y as vector of Y0, followed by N-1 empty slots (NA).
    Y <- c(Y0, rep(NA,N-1))
    
    # Then, you need create the iteratation
    for(i in 1:(N-1)){
        
    Y[i+1] <- # !!Implement the linear map here!!
        
    }
    
    return(Y)
}
```


* Copy the code and implement the linear map.
* When you are done, you need to initialize the function, select the code and run it. 
    + The environment will now contain a function called `linearMap`
    + Generate some data by calling the function using $Y0=0.1$, $r=1.08$ and $N=100$ and store the result in a variable. 


### Plot the timeseries

Creating the time series graphs and the return plot should be easy if the function `linearMap` returns the time series.  `R` and `Matlab` have specialized objects to represent timeseries, and functions and packages for timeseries analysis. They are especially convenient for plotting time and date information on the X-axis. See the notes on the [time series object](#tsPlot) for more information.


# **EXTRA** {#moc2ass}

In this assignment you'll build a more sophisticated growth model and look at its properties. The model will be the growth model by Van Geert (1991). You can try to code the models following the hints in section \@ref(moc1R).

[| jump straight to the solutions |](#tvpar)


## The growth model by Van Geert (1991) {-}

The growth model by Van Geert has the following form: 

\begin{equation}
L_{t+1} = L_t * (1 + r - r * \frac{L_t}{K})
(\#eq:vanG)
\end{equation}

Note the similarities to Equation \@ref(eq:logmap), the (stylized) logistic map.


### The growth model in a spreadsheet

```{block2, spreadset2, type='rmdimportant'}
Before you begin, be sure to check the following settings (same as first asignment): 

* Open a Microsoft Excel worksheet or a [Google sheet](https://www.google.com/docs/about/)
* Check whether the spreadsheet uses a 'decimal-comma' ($0,05$) or 'decimal-point' ($0.05$) notation. 
    + The numbers given in the assignments of this course all use a 'decimal-point' notation.
* Check if the `$` symbol fixes rows and columns when it used in a formula in your preferred spreadsheet program. 
    + This is the default setting in Microsoft Excel and Google Sheets. If you use one of those programs you are all set.
```

To build it take the sheet in assignment \@ref(moc1ass) as a template. 

* Define the appropriate constants ($r$ in `A5`, $L_0$ in `A6`) and prepare the necessary things you need for building an iterative process. 
* In particular, add the other parameter that appears in Van Geert's model:
    + Type $K$ in cell `A7`. This is the *carrying capacity*. It receives the value $1$ in cell `B7`.
* Start with the following values:
    + $r = 1.2$
    + $L_0 = 0.01$

Take good notice of what is constant (parameters $r$ and $K$), for which the `$` must be used, and what must change on every iterative step (variable $L_t$). Take about $100$ steps.

* Create the graphs
* You can start playing with the values for the parameters and the initial values in cells `B5`, `B6` and `B7`. To study its behavior, be sure to try the following growth parameters:
    + $r = 1.2$
    + $r = 2.2$
    + $r = 2.5$
    + $r = 2.7$
    + $r = 2.9$
    
* For the carrying capacity $K$ (cell `B7`) you can try the following values:
    + $K = 1.5$
    + $K = 0.5$. (Leave $r = 2.9$. Mind the value on the vertical axis!)
    
* Changes in the values of $K$ have an effect on the height of the graph. The pattern itself also changes a bit. 
    + Can you explain why this is so?

## Conditional growth: Jumps and Stages 

### Auto-conditional jumps {-}

Suppose we want to model that the growth rate $r$ increases after a certain amount has been learned. In general, this is a very common phenomenon, for instance: when becoming proficient at a skill, growth (in proficiency) is at first slow, but then all of a sudden there can be a jump to the appropriate (and sustained) level of proficiency.

* Take the model you just built as a starting point with $r = 0.1$ (`B5`)
    + Type $0.5$ in `C5`. This will be the new parameter value for $r$. 
    + Build your new model in column `B` (leave the original in `A`).
    
*  Suppose we want our parameter to change when a growth level of $0.2$ is reached. We???ll need an `IF` statement which looks something like this: `IF` $L > 0.2$ then use the parameter value in `C5`, otherwise use the parameter value in `B5`. 
    + Excel has a built in `IF` function (may be `ALS` in Dutch). 
    + In the first cell in which calculations should start, press $=$ and then from the formula list choose the `IF` function, or just type it. 
    + Try to figure out what you have to do. In the Logical_test box you should state something which expresses $L > 0.2$.
    + The other fields tell Excel what to do when this statement is `TRUE` (then use parameter value in `C5`) or when it is `FALSE` (then use paramter value in `B5`).
    + __Note:__ the word *value* might be misleading; you can also input new statements.
* Make a graph in which the old and the new conditional models are represented by lines.
    + Try changing the value of $r$ in `C5` into: $1, 2, 2.8, 2.9, 3$. 

### Auto-conditional stages {-}

Another conditional change we might want to explore is that when a certain growth level is reached the carrying capacity K increases, reflecting that new resources have become available to support further growth.

* Now we want $K$ to change, so type $1$ in `B7`, $2$ in `C7`.
* Build your model in column C. Follow the same steps as above, but now make sure that when $L > 0.99$, $K$ changes to the value in `C7`. Keep $r = 0.2$ (`B5`).

* If all goes well you should see two stages when you create a graph of the timeseries in column `C`. Change $K$ in `C7` to other values.
    + Try to also change the growth rate r after reaching $L > 0.99$ by referring to `C5`. Start with a value of $0.3$ in `C5`. Set $K$ in `C7` to $2$ again. 
    + Also try $1, 2.1, 2.5, 2.6, 3$.

### Connected growers {-}

You can now easly model coupled growth processes, in which the values in one series serve as the trigger for for parameter changes in the other process. Try to recreate the Figure of the connected growers printed in the chapter by Van Geert.

#### Demonstrations of dynamic modeling using spreadsheets

See the website by [Paul Van Geert](http://www.paulvangeert.nl/research.htm), scroll down to see models of:

* Learning and Teaching
* Behaviour Modification
* Connected Growers
* Interaction during Play


# **Predator-Prey dynamics**

In this assignment we will look at a 2D coupled dynamical system: **the Predator-Prey model** (aka [Lotka-Volterra equations](https://en.wikipedia.org/wiki/Lotka???Volterra_equations)).

## Predator-prey model 

The dynamical system is given by the following set of first-order differential equations, one represents changes in a population of predators, (e.g., **F**oxes: $f_F(R_t,F_t)$ ), the other represents changes in a population of prey, (e.g., **R**abbits: $f_R(R_t,F_t)$ ).


\begin{align}
\frac{dR}{dt}&=(a-b*F)*R \\
\\
\frac{dF}{dt}&=(c*R-d)*F
(\#eq:lv)
\end{align}


This is not a *difference* equation but a *differential* equation, which means building this system is not as straightforward as was the case in the previous assignments. Simulation requires a numerical method to 'solve' this differential equation for time, which means we need a method to approach, or estimate continuous time in discrete time. Below you will receive a speed course in one of the simplest numerical procedures for integrating differential equations, the [Euler method](https://en.wikipedia.org/wiki/Euler_method).    


### Euler Method

A general differential equation is given by:

\begin{equation}
\frac{dx}{dt} = f(x)
(\#eq:diff)
\end{equation}

Read it as saying: "_a change in $x$ over a change in time is a function of $x$ itself_". This can be approximated by considering the change to be over some constant, small time step $\Delta$:

\begin{equation}
\frac{(x_{t+1} = x_t)}{\Delta} = f(x_t)
(\#eq:Euler)
\end{equation}


After rearranging the terms a more familiar form reveals itself:

\begin{align}
x_{t+1} &= x_t &= f(x_t) * \Delta \\
x_{t+1} &= f(x_t) * \Delta + x_t
(\#eq:Euler2)
\end{align}


This looks like an ordinary iterative process, $\Delta$ the *time constant* determines the size of time step taken at every successive iteration. For a 2D system with variables **R** and **F** on would write:


\begin{align}
R_{t+1} &= f_R(R_t,Ft) * \Delta + R_t \\
F_{t+1} &= f_F(R_t,F_t) * \Delta + F_t
(\#eq:EulerRF)
\end{align}


### Coupled System in a spreadsheet

Implement the model in a spreadsheet by substituting $f_R(R_t,Ft)$ and $f_F(R_t,F_t)$ by the differential equations for Foxes and Rabbits given above.

* Start with $a = d = 1$ and $b = c = 2$ and the initial conditions $R_0 = 0.1$ and $F_0 = 0.1$. Use a time constant of $0.01$ and make at least $1000$ iterations.
* Visualize the dynamics of the system by plotting:
    + $F$ against $R$ (i.e., the state space)
    + $R$ and $F$ against time (i.e., the timeseries) in one plot.
* Starting from the initial predator and prey population represented by the point $(R, F) = (0.1, 0.1)$, how do the populations evolve over time?
* Try to get a grip on the role of the time constant by increasing and decreasing it slightly (e.g. $\Delta = 0.015$) for fixed parameter values. (You might have to add some more iterations to complete the plot). What happens to the plot? 
    + Hint: Remember that $\Delta$ is not a fundamental part of the dynamics, but that it is only introduced by the numerical integration (i.e., the approximation) of the differential equation. It should not change the dynamics of the system, but it has an effect nonetheless.
    
[| jump to solution |](#ppdsol)


### Tip of the iceberg

There is much more to tell about dynamic modelling, see [the notes](#advmodels) for some more advanced topics. 



# **Basic Timeseries Analysis** 

Most of the basic timeseries analyses can be performed in `SPSS`, because many of you will be familiar with the software we present the first assignments mainly as `SPSS` instructions, but you can go ahead an try your preferred environment for (statistical) computing.


## Time series analysis in SPSS (17 and higher) 

### Nonlinear Growth curves in SPSS {#bta}

 * Open the file [Growthregression.sav](https://github.com/FredHasselman/DCS/blob/master/assignmentData/BasicTSA_nonlinreg/GrowthRegression.sav), it contains two variables: `Time` and `Y(t)`. 

This is data from an iteration of the logistic growth differential equation you are familiar with by now, but lets pretend its data from one subject measured on 100 occasions.

1. Plot Y(t) against Time Recognize the shape?
2. To get the growth parameter well try to fit the solution of the logistic flow with SPSS nonlinear regression
     - Select nonlinear from the `Analysis` >> `Regression` menu.
     - Here we can build the solution equation. We need three parameters:
            a. **Yzero**, the initial condition.
            b. *K*, the carrying capacity.
            c. *r*, the growth rate.
    - Fill these in where it says `parameters` give all parameters a starting value of  $0.01$

4.	Take a good look at the analytic solution of the (stilized) logistic flow:

$$
Y(t)  =  \frac{K * Y_0}{Y_0 + \left(K-Y_{0}\right) * e^{(-K*r*t)} }
$$

Tr to build this equation, the function fo $e$ is called `EXP` in `SPSS` (`Function Group` >> `Arithmetic`)
Group terms by using parentheses as shown in the equation.

5. If you think you have built the model correctly, click on `Save` choose `predicted values`. Then paste your syntax and run it!
    - Check the estimated parameter values.
    - Check $R^2$!!!

6. Plot a line graph of both the original data and the predicted values. (Smile)

7. A polynomial fishing expedition:
     - Create time-varying covariates of $Y(t)$:
```
COMPUTE T1=Yt * Time.
COMPUTE T2=Yt * (Time ** 2). 
COMPUTE T3=Yt * (Time ** 3). 
COMPUTE T4=Yt * (Time ** 4). 
EXECUTE.
```
    - Use these variables as predictors of $Y(t)$ in a regular linear regression analysis. This is called a *polynomial regression*: Fitting combinations of curves of different shapes on the data.
    -  Before you run the analysis: Click `Save` Choose `Predicted Values: Unstandardized`

8. Look at $R^2$. This is also almost 1. Which model is better? Think about this: Based o the results o the linear regression what can yo tell about the *growth rate*, the *carrying capacity* or the *initial condition*?

9.	Create a line graph of $Y(t)$, plot the predicted values of the nonlinear regression and the unstandardized predicted values of the linear polynomial regression against `time` in one figure.

10. Now you can see that the shape is approximated by the polynomials, but it is not quite the same. Is this really a model of a growth process as we could encounter it in nature?

[| jump to solution |](#btasol)

### Correlation functions and AR-MA models {#pacf}

1. Download the file [`series.sav`](https://github.com/FredHasselman/DCS/blob/master/assignmentData/BasicTSA_arma/series.sav) from blackboard. It contains three time series `TS_1`, `TS_2` and `TS_3`. As a first step look at the mean and the standard deviation (`Analyze` >> `Descriptives`).  Suppose these were time series from three subjects in an experiment, what would you conclude based on the means and SDs?  

2. Lets visualize these data. Go to `Forecasting` >> `Time Series` >> `Sequence Charts`. Check the box One chart per variable and move all the variables to Variables. Are they really the same?  

3. Lets look at the `ACF` and `PCF`
    * Go to `Analyze` >> `Forecasting` >> `Autocorrelations`. 
    * Enter all the variables and make sure both *Autocorrelations* (ACF) and *Partial autocorrelations* (PACF) boxes are checked. Click `Options`, and change the `Maximum Number of Lags` to `30`. 
    * Use the table to characterize the time series:  


|                    SHAPE                | INDICATED MODEL |
|-----------------------------------------|-------------------------------------------------------------------------------------------------|
|       Exponential, decaying to zero     | Autoregressive model. Use the partial autocorrelation plot to identify the order of the autoregressive model|
| Alternating positive and negative, decaying to zero  | Autoregressive model. Use the partial autocorrelation plot to help identify the order.|
| One or more spikes, rest are essentially zero | Moving average model, order identified by where plot becomes zero. |
| Decay, starting after a few lags | Mixed autoregressive and moving average model.|
All zero or close to zero  | Data is essentially random.|
| High values at fixed intervals | Include seasonal autoregressive term. |
| No decay to zero  | Series is not stationary. |


4. You should have identified just one time series with autocorrelations: `TS_2`. Try to fit an `ARIMA(p,0,q)` model on this time series. 
    - Go to `Analyze` >> `Forecasting` >> `Create Model`, and at `Method` (Expert modeler) choose `ARIMA`. 
    - Look back at the `PACF` to identify which order (`p`) you need (last lag value at which the correlation is still significant). This lag value should go in the Autocorrelation p box. 
    - Start with a Moving Average `q` of one. The time series variable `TS_2` is the `Dependent`. 
    - You can check the statistical significance of the parameters in the output under `Statistics`, by checking the box `Parameter Estimates`. 
    - This value for `p` is probably too high, because not all AR parameters are significant. 
    - Run ARIMA again and decrease the number of AR parameters by leaving out the non-significant ones.  

5. By default `SPSS` saves the predicted values and 95% confidence limits (check the data file). We can now check how well the prediction is: Go to `Graphs` >> `Legacy Dialogs` >> `Line.` Select `Multiple` and `Summaries of Separate Variables`. Now enter `TS_2`, `Fit_X`, `LCL_X` and `UCL_X` in `Lines Represent`. `X` should be the number of the last (best) model you fitted, probably 2. Enter `TIME` as the `Category Axis`.  

6. In the simulation part of this course we have learned a very simple way to explore the dynamics of a system: The return plot. The time series is plotted against itself shifted by 1 step in time. 
    - Create return plots (use a Scatterplot) for the three time series. Tip: You can easily create a `t+1` version of the time series by using the LAG function in a `COMPUTE` statement. For instance: 
```
COMPUTE TS_1_lag1 = LAG(TS_1)
``` 
    - Are your conclusions about the time series the same as in 3. after interpreting these return plots? 

[| jump to solution |](#pacfsol)

## Notes on TSA in `R` {#bTSAinR}

If you use `R` the command below will install all the packages we will use during the entire course on you private computer. This might take too long on a university PC, just install the packages you need for an assignment each session.

### Importing data in `R`

If you have package `rio` installed in `R`, you can load the data directly into the local environment.

1. Follow the link, e.g. for [`series.sav`](https://github.com/FredHasselman/DCS/blob/master/assignmentData/BasicTSA_arma/series.sav).
2. On the Github page, find a button marked **Download** (or **Raw** for textfiles).
3. Copy the `url` associated with the **Download**  button on Github (right-clik).
4. The copied path should contain the word 'raw' somewhere in the url.
5. Call `import(url)`:
```{r, include=TRUE, eval=FALSE}
series <- import("https://github.com/FredHasselman/DCS/raw/master/assignmentData/BasicTSA_arma/series.sav")
```

You can use the function `arima()`, `acf()` and `pacf()` in `R` (`Matlab` has functions that go by slightly different names, check the [Matlab Help pages](https://nl.mathworks.com/help/econ/autocorr.html)). 

There are many extensions to these linear models, check the [`CRAN Task View` on `Time Series Analysis`](https://cran.r-project.org/web/views/TimeSeries.html) to learn more (e.g. about package `zoo` and `forecast`).


[| jump to solution |](#bTSAinRsol)


<!-- ## Heartbeat dynamics {#hrv} -->
<!-- Download three different time series of heartbeat intervals (HBI) [here](https://github.com/FredHasselman/DCS/tree/master/assignmentData/RelativeRoughness). If you use `R` and have package `rio` installed you can run this code and the load the data into a `data.frame` directly from `Github`.  -->
<!-- ```{r, echo=TRUE, eval=FALSE, include=TRUE} -->
<!-- library(rio) -->
<!-- TS1 <- rio::import("https://github.com/FredHasselman/DCS/raw/master/assignmentData/RelativeRoughness/TS1.xlsx", col_names=FALSE) -->
<!-- TS2 <- rio::import("https://github.com/FredHasselman/DCS/raw/master/assignmentData/RelativeRoughness/TS2.xlsx", col_names=FALSE) -->
<!-- TS3 <- rio::import("https://github.com/FredHasselman/DCS/raw/master/assignmentData/RelativeRoughness/TS3.xlsx", col_names=FALSE) -->
<!-- ``` -->

<!-- The Excel files did not have any column names, so let's create them in the `data.frame` -->
<!-- ```{r, eval=FALSE, include=TRUE} -->
<!-- colnames(TS1) <- "TS1" -->
<!-- colnames(TS2) <- "TS2" -->
<!-- colnames(TS3) <- "TS3" -->
<!-- ``` -->

<!-- ### The recordings -->
<!-- These HBIs were constructed from the R-R intervals in electrocardiogram (ECG) recordings, as defined in Figure \@ref(fig:RRf1).  -->

<!-- ```{r RRf1, fig.cap="Definition of Heart Beat Periods.", fig.align='center',echo=FALSE, include=TRUE} -->
<!-- knitr::include_graphics('images/RRfig1.png') -->
<!-- ``` -->


<!--  * One HBI series is a sample from a male adult, 62 years old (called *Jimmy*). Approximately two years before the recording, the subject has had a coronary artery bypass, as advised by his physician following a diagnosis of congestive heart failure. *Jimmy* used antiarrhythmic medicines at the time of measurement. -->

<!--  * Another HBI series is a sample from a healthy male adult, 21 years old (called *Tommy*). This subject never reported any cardiac complaint. Tommy was playing the piano during the recording. -->

<!--  * A third supposed HBI series is fictitious, and was never recorded from a human subject (lets call this counterfeit *Dummy*).  -->
<!-- Your challenge -->

<!-- The assignment is to scrutinise the data and find out which time series belongs to *Jimmy*, *Tommy*, and *Dummy* respectively. ^[The HBI intervals were truncated (not rounded) to a multiple of 10 ms (e.g., an interval of 0.457s is represented as 0.450s), and to 750 data points each. The means and standard deviations among the HBI series are approximately equidistant, which might complicate your challenge.] -->


<!-- ### First inspection -->
<!-- The chances that you are an experienced cardiologist are slim. We therefore suggest you proceed your detective work as follows: -->

<!-- *	Construct a graphical representation of the time series, and inspect their dynamics visually (use the code examples provided in the [solutions to previous sessions](#moc1Rsol) to plot your time series).  -->
<!-- * Write down your first guesses about which time series belongs to which subject. Take your time for this visual inspection (i.e., which one looks more like a line than a plane, which one looks more 'smooth' than 'rough').  -->
<!-- *	Next, explore some measures of central tendency and dispersion, etc. -->
<!-- *	Third, compute the Relative Roughness for each time series, use Equation \@ref(eq:RR) -->

<!-- \begin{equation} -->
<!-- RR = 2\left[1\frac{\gamma_1(x_i)}{Var(x_i)}\right] -->
<!-- (\#eq:RR) -->
<!-- \end{equation} -->

<!-- The numerator in the formula stands for the `lag 1` autocovariance of the HBI time series $x_i$. The denominator stands for the (global) variance of $x_i$. Most statistics packages can calculate these variances, `R` and `Matlab` have built in functions. Alternatively, you can create the formula yourself. -->

<!-- *	Compare your (intuitive) visual inspection with these preliminary dynamic quantifications, and find out where each of the HIB series are positions on the colorful spectrum of noises (i.e., line them up with Figure \@ref(fig:RRf3)). -->

<!-- ```{r RRf3, fig.cap="Coloured Noise versus Relative Roughness", fig.align='center',echo=FALSE, include=TRUE} -->
<!-- knitr::include_graphics('images/RRfig3.png') -->
<!-- ``` -->


<!-- ### What do we know now, that we didnt knew before? -->
<!-- Any updates on Jimmys, Tommys and Dummys health? You may start to wonder about the 'meaning' of these dynamics, and not find immediate answers.  -->

<!-- Dont worry; well cover the interpretation over the next two weeks in further depth. Lets focus the dynamics just a little further for now. It might give you some clues. -->

<!-- * Use the `randperm` function (in `Matlab` or in package  [`pracma`](http://www.inside-r.org/packages/cran/pracma) in `R`) to randomize the temporal ordering of the HBI series. -->
<!-- * Visualize the resulting time series to check whether they were randomized successfully -->
<!-- * Next estimate the Relative Roughness of the randomized series. Did the estimates change compared to your previous outcomes (if so, why)? -->

<!-- * Now suppose you would repeat what you did the previous, but instead of using shuffle you would integrate the fictitious HBI series (i.e., normalize, then use `x=cumsum(x))`. You can look up `cumsum` in `R` or `Matlab`s Help documentation). Would you get an estimate of Relative Roughness that is approximately comparable with what you got in another HBI series? If so, why? -->

<!-- [| jump to solution |](#hrvsol) -->



<!-- # **Fluctuation and Disperion analyses I** {#fda1} -->

<!-- ```{block2, L5, type='rmdimportant'} -->
<!-- Before you begin, look at the notes for [Lecture 4](#lecture-4). -->
<!-- ``` -->

<!-- ## The Spectral Slope {#psd} -->

<!-- We can use the power spectrum to estimate a **self-affinity parameter**, or scaling exponent. -->

<!-- * Download `ts1.txt`, `ts2.txt`, `ts3.txt` [here](https://github.com/FredHasselman/DCS/tree/master/assignmentData/Fluctuation_PSDslope). If you use `R` and have package `rio` installed you can run this code. It loads the data into a `data.frame` object directly from `Github`.  -->
<!-- ```{r, echo=TRUE, eval=FALSE, include=TRUE} -->
<!-- library(rio) -->
<!-- TS1 <- rio::import("https://raw.githubusercontent.com/FredHasselman/DCS/master/assignmentData/Fluctuation_PSDslope/ts1.txt") -->
<!-- TS2 <- rio::import("https://raw.githubusercontent.com/FredHasselman/DCS/master/assignmentData/Fluctuation_PSDslope/ts2.txt") -->
<!-- TS3 <- rio::import("https://raw.githubusercontent.com/FredHasselman/DCS/master/assignmentData/Fluctuation_PSDslope/ts3.txt") -->

<!-- # These objects are now data.frames with one column named V1.  -->
<!-- # If you want to change the column names -->
<!-- colnames(TS1) <- "TS1" -->
<!-- colnames(TS2) <- "TS2" -->
<!-- colnames(TS3) <- "TS3" -->
<!-- ``` -->

<!-- * Plot the three 'raw' time series. -->

<!-- ### Basic data checks and preparations -->

<!-- For spectral analysis we need to check some data assumptions (see [notes on data preparation, Lecture 4](#data-considerations)). -->

<!-- #### Normalize {-} -->
<!-- 1. Are the lengths of the time series a power of 2? (Use `log2(length of var)` ) -->
<!--   + Computation of the frequency domain is greatly enhanced if data length is a power (of 2). -->
<!-- 2. Are the data normalized? (we will *not* remove datapoints outside 3SD) -->
<!--     + To normalize we have to subtract the mean from each value in the time series and divide it by the standard deviation, the function `scale()` can do this for you, but you could also use `mean()` and `sd()` to construct your own function. -->
<!-- 3. Plot the normalized time series. -->

<!-- #### Detrend {-} -->
<!-- Before a spectral analysis you should remove any linear trends (it cannot deal with nonstationary signals!) -->

<!-- 1. Detrend the normalized data (just the linear trend).  -->
<!--     + This can be done using the function `pracma::detrend()`.  -->
<!--     + Extra: Try to figure out how to detrend the data using `stats::lm()` or `stats::poly()` -->
<!-- 2. Plot the detrended data. -->

<!-- #### Get the log-log slope in Power Spectral Density {-} -->
<!-- The function `fd.psd()` will perform the spectral slope fitting procedure.  -->

<!-- 1. Look at the manual pages to figure out how to call the function. The manual is on blackboard and [Github](https://github.com/FredHasselman/DCS/blob/master/functionLib/) -->
<!--     + Remember, we have already normalized and detrended the data. -->
<!--     + You can also look at the code itself by selecting the function name in`R` and pressing `F2`  -->
<!-- 2. Calculate the spectral slopes for the three normalized and detrended time series. -->
<!--     + Call with `plot = TRUE` -->
<!--     + Compare the results... What is your conclusion? -->


<!-- [| jump to solution |](#psdsol) -->

<!-- ## DFA and SDA {#dfa} -->

<!-- * Use the functions `fd.dfa()` and `fd.sda()` to estimate the self-affinity parameter and Dimension of the series.  -->
<!--     + Check what kind of data preparation is required for SDA and DFA in [notes on data preparation, Lecture 4](#data-considerations). -->
<!--     + Compare the results between the three different methods. -->

<!-- [| jump to solution |](#dfasol) -->

<!-- ## ACF/PACF, Relative Roughness {#pacfrel} -->

<!-- * Also calculate the ACF, PACF ([see assignment](#pacf)) and [Relative Roughness](#relR) -->
<!--     + Compare the results. -->

<!-- [| jump to solution |](#pacfrelrsol) -->

<!-- ## Heartbeat dynamics II {#hrv2} -->
<!-- In the [previous assignment](#relR), you were presented with three different time series of heartbeat intervals (HBI), and you analyzed them using a measure of Relative Roughness (RR; cf. Marmelat & Delignires, 2012).  -->

<!-- A logical step is to unleash the full force of your new analytic toolbox onto the HBI series.  -->

<!-- * Keep track of the outcomes of each time series for 4 different analyses (RR, PSD, SDA, DFA).  -->
<!--     + Do the outcomes of the different methods converge on the continuum of blue, white and pink, to Brownian and black noise? That is, do they indicate the same type of temporal structure? -->
<!-- * As a final step, construct return plots for each time series and try to interpret what you observe, given the outcomes of the scaling parameter estimates. -->

<!-- [| jump to solution |](#hrv2sol) -->

<!-- ## Analysis of Deterministic Chaos {#chaos} -->

<!-- * Generate a chaotic timeseries (e.g. $r = 4$ ) of equal length as the time series used above (use the function `growth.ac( ..., type = "logistic")` in `nlRtsa_SOURCE`, see the [solutions of Lecture 1 and 2](#linear-and-logistic-growth)) -->
<!--     + This is in fact one of the series you analysed in a [previous assignment](#pacf). If you still have the results use them for the next part. -->
<!-- * Get all the scaling quantities for this series as well as the ACF and PACF and [some return plots](#the-return-plot) just like in the previous assignments. -->
<!--     + Compare the results to e.g. the heartbeat series.  -->

<!-- [| jump to solution |](#chaossol) -->

<!-- # Fluctuation and Disperion analyses II {#fda2} -->

<!-- There were no assignments for this Lecture. -->


# (PART) II. Quantifying Recurrences in State Space

# **Phase Space Reconstruction and RQA** 

You can use `R` to run `RQA` analyses. These assignments assume you'll use `R`, but you can also find a `Matlab` toolbox for `RQA` here: [CRP toolbox](http://tocsy.pik-potsdam.de/CRPtoolbox/)

You'll need:
```{r}
library(rio)
library(crqa)
library(fractal)
library(devtools)
source_url("https://raw.githubusercontent.com/FredHasselman/DCS/master/functionLib/nlRtsa_SOURCE.R")
```

**R-packages for Phase Space Reconstruction**

We'll use package `fractal` and `rgl` to reconstruct some phase spaces.

* If you have sourced the functions in `nlRtsa_SOURCE.R` you can install and load these packages by running: `in.IT(c("fractal","rgl"))`. The function `in.IT()` will first check if the requested packages are installed on the machine and only install them if they are not present. 

## Reconstruct the Lorenz attractor {#RQA} 

* Package `fractal` includes the 3 dimensions of the Lorenz system in the chaotic regime. 
    + Run this code `rgl::plot3d(lorenz)` with both packages loaded to get an interactive 3D plot ^[You'll need the [X Window System](http://www.x.org/wiki/) for interactive 3D plotting. This Linux desktop system comes installed in some way or form on most Mac and Windows systems.You can test if it is present by running `rgl::open3d()`, which will try to open an interactive plotting device]
 of this strange attractor.


We'll reconstruct the attractor based on just dimension `X` of the system using functions from package `fractal`, be sure to look at the manual pages of these functions.

```{block2, samepack, type='rmdimportant'}
Package `fractal` and package `nonlinearTseries` use functions with similar names, do not load them together.
```

* Use `lx <- lorenz[1:2048,1]` to reconstruct the phase space based on `lx`. 
    + Find an optimal embedding lag using `timeLag()`, use `method = "mutual"`.
    + Find the embedding dimension, using `FNN()`
    + Embed the timeseries using `embedSeries()`.
    + Plot the reconstructed phase space. (You'll need to use `as.matrix()` on the object created by `embedSeries()`)
        - Use  `rgl::plot3d()` to plot the reconstructed space.

## Reconstruct the Predator-Prey model

Use the same procedure as above to reconstruct the state space of the predator-prey system. (Look [at the solutions](#ppdsol) to get a `Foxes` or `Rabbit` series).

 * You should get a 2D state space, so 3D plotting might be a bit too much for this system.


## (auto-) Recurrence Quantification Analysis
There are several packages which can perform (C)RQA analysis, we'll use [`crqa`](https://cran.r-project.org/web/packages/crqa/index.html) because it can perform both continuous and categorical analyses. If you only have continuous data, you migh be better off using package [`nonlinearTseries`](https://cran.r-project.org/web/packages/nonlinearTseries/index.html), in this course we will only use package `crqa`.

```{block2, crqa, type='rmdimportant'}
 Package `crqa()` was designed to run categorical Cross-Recurrence Quantification Analysis (see [Coco & Dale (2014)](http://journal.frontiersin.org/Journal/10.3389/fpsyg.2014.00510/abstract) and for R code see appendices in [Coco & Dale (2013)](http://arxiv.org/abs/1310.0201)). We can trick it to run auto-RQA by providing the same timeseries for `ts1` and `ts2` and setting the parameter `side` to either `"upper"` or `"lower"`
```

 * Perform an RQA on the reconstructed state space of the Lorenz system.
    + You'll need a radius (also called: threshold) in order to decide which points are close together (recurrent).
        - `crqa` provides a function which will automatically select the best parameter settings: `optimizeParam()`
        - Best way to ensure you are using the same parameters in each function is to create some lists with parameter settings (check the `crqa` manual to figure out what these parameters mean):


```{r, eval=FALSE, tidy = FALSE}
# General settings for `crqa()`
par0 <- list(rescale = 1,
             normalize = 0,
             mindiagline = 2,
             minvertline = 2,
             tw = 0,
             whiteline = FALSE,
             recpt = FALSE,
             side = "lower",
             checkl = list(do = FALSE, thrshd = 3, datatype = "categorical",pad = TRUE)
             )

# Settings for `optimizeParam()`
par <- list(lgM =  20, steps = seq(1, 6, 1),
           radiusspan = 100, radiussample = 40,
           normalize = par0$normalize, 
           rescale = par0$rescale, 
           mindiagline = par0$mindiagline, minvertline = par0$minvertline,
           tw = par0$tw, 
           whiteline = par0$whiteline, 
           recpt = par0$recpt, 
           fnnpercent = 10, typeami = "mindip")
```

* Get the optimal parameters using a radius which will give us 2\%-5\% recurrent points.

```{r, eval=FALSE}
ans <- optimizeParam(ts1 = lx, ts2 = lx, par = par, min.rec = 2, max.rec = 5)
```

* Run the RQA analysis using the same settings with which the parameters were found.

```{r, eval=FALSE, tidy=FALSE}
crqaOutput <- crqa(ts1 = lx, ts2 = lx, 
                  delay = ans$delay, 
                  embed = ans$emddim, 
                  radius = ans$radius, 
                  normalize = par0$normalize,
                  rescale = par0$rescale, 
                  mindiagline = par0$mindiagline, minvertline = par0$minvertline,
                  tw = par0$tw, 
                  whiteline = par0$whiteline, 
                  recpt = par0$recpt, 
                  side = par0$side, checkl = par0$checkl
                  )
```

* The output of `crqa` is a list with recurrence measures, the last entry is the recurrence plot. It is represented as a so-called `sparse-matrix`. 
    + This representation severely decreases the amount of memory occupied by the recurrence matrix. It is basically a list of indices of cells that contain a $1$. The $0$ do not need to be stored.
    + In order to plot this matrix you could use `image()`, but this does not produce the recurrence plot as they are usually displayed, the y-axis needs to be flipped.
    + We created a function which will take as input the list output of `crqa`, which wil be used to plot the recurrence matrix. If you have  sourced the `nlRtsa` functions you can call `plotRP.crqa(crqaOutput)`.
    
    
[| Jump to solutions |](#PSRsol)


## RQA of circle-tracing data

* Analyse the [circle tracing data](https://github.com/FredHasselman/DCS/tree/master/assignmentData/RQA_circletrace) we recorded during the lecture.
* Study what happens to the RQA measures if you shuffle the temporal order (see e.g. the solution to th [HRV assignments](#hrvsol)).
* Package `fractal` contains a function `surrogate`. This will create so-called *constrained* realisations of the time series. Look at the help pages of the function, or study the *Surrogates Manual* of the [TISEAN software](http://www.mpipks-dresden.mpg.de/~tisean/Tisean_3.0.1/index.html) and create two surrogate series, one based on `phase` and one on `aaft`.
     + Look at the RQA measures and think about which $H_0$ should probably be rejected.
     + If you want to be more certain, you'll have to create a test (more surrogates). The [TISEAN manual](http://www.mpipks-dresden.mpg.de/~tisean/Tisean_3.0.1/docs/surropaper/node5.html#SECTION00030000000000000000) provides all the info you need to construct such a test:
     
     > "For a minimal significance requirement of 95\% , we thus need at least 19 or 39 surrogate time series for one- and two-sided tests, respectively. The conditions for rank based tests with more samples can be easily worked out. Using more surrogates can increase the discrimination power."
  

[| Jump to solutions |](#RQAsol)


# **Categorical and Cross-RQA (CRQA)** 

```{r, include=FALSE}
SOLUTION = FALSE
```

You can use `R` or `Matlab` to run `RQA` analyses. These assignments assume you'll use `R`. You can find a `Matlab` toolbox for `RQA` here: [CRP toolbox](http://tocsy.pik-potsdam.de/CRPtoolbox/)

For `R` you'll probably need:
```{r, message=FALSE, warning=FALSE}
library(rio)
library(crqa)
library(fractal)
library(devtools)
source_url("https://raw.githubusercontent.com/FredHasselman/DCS/master/functionLib/nlRtsa_SOURCE.R")
```


## Assignment: CRQA and Diagonal Profile {#CRQA}

1. Create two variables for CRQA analysis, or use the $x$ and $y$ coordinates we roecorded during the lecture:
```{r}
y1 <- sin(1:900*2*pi/67)
y2 <- sin(.01*(1:900*2*pi/67)^2)

# Here are the circle trace data
xy <- import("https://raw.githubusercontent.com/FredHasselman/DCS/master/assignmentData/RQA_circletrace/mouse_circle_xy.csv") 

y1 <- xy$x
y2 <- xy$y
```

2. You have just created two sine(-like) waves. Well examine if and how they are coupled in a shared phase space. As a first step plot them.

3. Find an embedding delay (using mutual information) and an embedding dimension (if you calculate an embedding dimension using package `fractal` for each signal seperately, as a rule of thumb use the highest embedding dimension you find in further analyses).

```{r, tidy=FALSE, eval=SOLUTION}
# General settings for `crqa()`
par0 <- list(rescale = 0,
             normalize = 0,
             mindiagline = 2,
             minvertline = 2,
             tw = 0,
             whiteline = FALSE,
             recpt = FALSE,
             side = "both",
             checkl = list(do = FALSE, thrshd = 3, datatype = "categorical",pad = TRUE)
             )

# Settings for `optimizeParam()`
par <- list(lgM =  20, steps = seq(1, 6, 1),
           radiusspan = 100, radiussample = 40,
           normalize = par0$normalize, 
           rescale = par0$rescale, 
           mindiagline = par0$mindiagline, minvertline = par0$minvertline,
           tw = par0$tw, 
           whiteline = par0$whiteline, 
           recpt = par0$recpt, 
           fnnpercent = 10, typeami = "mindip")
```

4. We can now create a cross recurrence matrix. Fill in the values you decided on. You can choose a radius automatically, look in the `crqa` manual.
* Get the optimal parameters using a radius which will give us 2\%-5\% recurrent points.

> Note: There is no rescaling of data, the sines were created in the same range. You can plot a matrix using `image()`. You could also check package `nonlinearTseries`. If you sourced the `nlRtsa` library, use `plotRP.crqa()`


```{r, eval=SOLUTION}
(ans <- optimizeParam(ts1 = y1, ts2 = y2, par = par, min.rec = 2, max.rec = 5))
```

5. Run the CRQA and produce a plot of the recurrence matrix.

```{r, tidy=FALSE, echo=SOLUTION, eval=SOLUTION}
crqaOutput <- crqa(ts1 = y1, ts2 = y2,  
                  delay = ans$delay, 
                  embed = ans$emddim, 
                  radius = ans$radius, 
                  normalize = par0$normalize,
                  rescale = par0$rescale, 
                  mindiagline = par0$mindiagline, minvertline = par0$minvertline,
                  tw = par0$tw, 
                  whiteline = par0$whiteline, 
                  recpt = par0$recpt, 
                  side = par0$side, checkl = par0$checkl
                  )
```

6. Can you understand what is going on? 
  * For the simulated data: Explain the the lack of recurrent points at the beginning of the time series.
  * For the circle trace: How could one see these are not determisnistic sine waves?

```{r, echo=SOLUTION, eval=SOLUTION}
plotRP.crqa(crqaOutput)
```

7. Examine the synchronisation under the diagonal LOS. Look in the manual of `crqa` or  [Coco & Dale (2014)](http://journal.frontiersin.org/Journal/10.3389/fpsyg.2014.00510/abstract). 
 * To get the diagonal profile from the recurrence plot, use `spdiags()`.
 * Make a plot of the diagonal profile. How far is the peak in RR removed from 0 (Line of Synchronisation)? 

```{r, echo=SOLUTION, eval=SOLUTION}
win <- 50
(res <-  drpdfromts(y1, y2, ws = win, datatype = 'continuous', radius = ans$radius))[2:3]
plot(-win:win,res$profile,type = "l", lwd = 5, xlab = "Delays", ylab = "Recurrence")
abline(v=0)
```

```{r, echo=SOLUTION, eval=SOLUTION}
dprofile <- spdiags(crqaOutput$RP)
plot(-win:win,colMeans(dprofile$B[, between(dprofile$d, -win,win)]), type="l", 
     lwd = 5, xlab = "Delays", ylab = "Recurrence")
abline(v=0)
```

8. Perform the same steps with a shuffled version (or use surrogate analysis!) of the data of timeseries $y1$. You can use the embedding parameters you found earlier. 

> NOTE: If you generate surrogate timeseries, make sure the RR is the same for all surrogates. Try to keep the RR in the same range by using the `min.rec` and `max.rec` settings of `optimizeParam` for each surrogate.

[| Jump to solutions |](#CRQAsol)


## Categorical CRQA {#catCRQA}

1. Package CRQA contains two categorical trial series from the "Friends"study.
   * Lookup `RDts1` and `RDts2` in the help file. 
   * Also lookup the articles discussing the study to get an idea about these series [here](http://cognaction.org/rdmaterials/php.cv/pdfs/article/richardson_dale_2005.pdf) and [here](http://cognaction.org/rdmaterials/php.cv/pdfs/article/richardson_dale_kirkham.pdf))  

2.  In the paper accompanying the package by [Coco & Dale (2014)](http://journal.frontiersin.org/Journal/10.3389/fpsyg.2014.00510/abstract) demonstrate the analysis of these series. 
    * Recreate this analysis, e.g. Figure 10 in the article including the CRQA measures.

[| Jump to solutions |](#catCRQAsol)

# Potential and Catastrophe Models {#cusp}


* The nonlinear regression assignment is written for `SPSS`, but you can try to use 'R' as well. Be sure to look at the solutions of the [Basic Time Series Analysis](#bta)) assignments.
* Using the `cusp` package 

## Fitting the cusp catastrophe in `SPSS`

In the file [Cusp Attitude.sav](https://github.com/FredHasselman/DCS/raw/master/assignmentData/Potential_Catastrophe/Cusp%20attitude.sav), you can find data from a (simulated) experiment. Assume the experiment tried to measure the effects of explicit predisposition and [affective conditioning](http://onlinelibrary.wiley.com/doi/10.1002/mar.4220110207/abstract) on attitudes towards Complexity Science measured in a sample of psychology students using a specially designed Implicit Attitude Test (IAT).

1. Look at a Histogram of the difference score (post-pre) $dZY$. This should look quite normal (pun intended).
   -	Perform a regular linear regression analysis predicting $dZY$ (Change in Attitude) from $\alpha$ (Predisposition). 
   - Are you happy with the $R^2$?
2. Look for Catastrophe Flags: Bimodality. Examine what the data look like if we split them across the conditions.
  - Use $\beta$ (Conditioning) as a Split File Variable (`Data >> Split File`). And again, make a histogram of $dZY$.
  - Try to describe what you see in terms of the experiment.
  -	Turn Split File off. Make a Scatterplot of $dZY$ (x-axis) and $\alpha$ (y-axis). Here you see the bimodality flag in a more dramatic fashion. 
   - Can you see another flag?
3. Perhaps we should look at a cusp Catastrophe model:
   - Go to `Analyse >> Regression >> Nonlinear` (also see [Basic Time Series Analysis](#bta)). First we need to tell SPSS which parameters we want to use, press `Parameters.` 
    - Now you can fill in the following: 
        + `Intercept` (Starting value $0.01$)
        + `B1` through `B4` (Starting value $0.01$)
        + Press `Continue` and use $dZY$ as the dependent. 
    - Now we build the model in `Model Expression`, it should say this: 
    ```
    Intercept + B1 * Beta * ZY1 + B2 * Alpha + B3 * ZY1 ** 2 + B4 * ZY1 ** 3
    ```
    - Run! And have a look at $R^2$.
4. The model can also be fitted with linear regression in SPSS, but we need to make some extra (nonlinear) variables using `COMPUTE`:

```
BetaZY1 = Beta*ZY1	*(Bifurcation, splitting parameter).
ZY1_2 = ZY1 ** 2	*(ZY1 Squared).
ZY1_3 = ZY1 ** 3	*(ZY1 Cubed).
```
  - Create a linear regression model with $dZY$ as dependent and $Alpha$, $BetaZY1$ and $ZY1_2$ en $ZY1_3$ as predictors. 
  - Run! The parameter estimates should be the same.
5. Finally try to can make a 3D-scatterplot with a smoother surface to have look at the response surface.
  - HINT: this is a lot easier in `R` or `Matlab` perhaps you can export your `SPSS` solution.

6. How to evaluate a fit? 
   - Check the last slides of lecture 8 in which the technique is summarised. 
   - The cusp has to outperform the `pre-post` model.


## The `cusp` package in `R`


Use this tutorial paper to fit the cusp in 'R' according to a maximum likelihood criterion.

[Grasman, R. P. P. P., Van der Maas, H. L. J., & Wagenmakers, E. (2007). Fitting the Cusp Catastrophe in R: A cusp-Package.](https://cran.r-project.org/web/packages/cusp/vignettes/Cusp-JSS.pdf)

1. Start R and install package `cusp` 

2. Work through the *Example I* (attitude data) in the paper (*Section 4: Examples*, p. 12).

3. *Example II* in the same section is also interesting, but is based on simulated data.
    * Try to think of an application to psychological / behavioural science.


# **Complex Networks** {#nets}

To complete these assignments you need:
```{r, message=FALSE, warning=FALSE}
library(igraph)
library(qgraph)
library(devtools)
source_url("https://raw.githubusercontent.com/FredHasselman/DCS/master/functionLib/nlRtsa_SOURCE.R")
```

## Basic graphs

1. Create a small ring graph in `igraph` and plot it.

```{r}
g <- graph.ring(20)
```

    - Look in the manual of `igraph` for a function that will get you the degree of each node.
    - ALso, get the average path length.


2. Create a "Small world" graph and plot it.
     - Get the degree, average path length and transitivity


```{r}
g <- watts.strogatz.game(1, 20, 5, 0.05)
```


3. Directed scale-free  graph
     - Get the degree, average path length and transitivity

```{r}
g <- barabasi.game(20)
```

     - There should be a power law relation between nodes and degree ... (but this is a very small network)
```{r, eval=FALSE}
plot(log(1:20),log(degree(g)))
```

## Social Networks 

The package igraph contains data on a Social network of friendships between 34 members of a karate club at a US university in the 1970s.

> See W. W. Zachary, An information flow model for conflict and fission in small groups, *Journal of Anthropological Research 33*, 452-473 (1977).


1. Get a graph of the cpmmunities within the social network. 
    - Check the manual, or any online source to figure out what these measures mean and how they are calulated.

```{r, eval=FALSE}
# Community membership
karate <- graph.famous("Zachary")
wc <- walktrap.community(karate)
plot(wc, karate)
modularity(wc)
membership(wc)
```


2. It is also possible to get the adjacency matrix

```{r, eval=FALSE}
get.adjacency(karate)
```

     - What do the columns, rows and 0s and 1s stand for?


## Small World Index and Degree Distribution 

Select and run all the code below
This will compute the *Small World Index* and compute the Power Law slope of Small world networks and Scale-free networks

1. Compare the measures!

2. Try to figure out how these graphs are constructed by looking at the functions in `nlRtsa_SOURCE.R`

```{r, eval = FALSE}
# Initialize
set.seed(660)
layout1=layout.kamada.kawai
k=3
n=50

# Setup plots
par(mfrow=c(2,3))
# Strogatz rewiring probability = .00001
p   <- 0.00001
p1  <- plotSW(n=n,k=k,p=p)
PLF1<- PLFsmall(p1)
p11 <- plot(p1,main="p = 0.00001",layout=layout1,xlab=paste("FON = ",round(mean(neighborhood.size(p1,order=1)),digits=1),"\nSWI = ", round(SWtestE(p1,N=100)$valuesAV$SWI,digits=2),"\nPLF = NA",sep=""))

# Strogatz rewiring probability = .01
p   <- 0.01
p2  <- plotSW(n=n,k=k,p=p)
PLF2<- PLFsmall(p2)
p22 <- plot(p2,main="p = 0.01",layout=layout1,xlab=paste("FON = ",round(mean(neighborhood.size(p2,order=1)),digits=1),"\nSWI = ", round(SWtestE(p2,N=100)$valuesAV$SWI,digits=2),"\nPLF = ",round(PLF2,digits=2),sep=""))
# Strogatz rewiring probability = 1
p   <- 1
p3  <- plotSW(n=n,k=k,p=p)
PLF3<- PLFsmall(p3)
p33 <- plot(p3,main="p = 1",layout=layout1,xlab=paste("FON = ",round(mean(neighborhood.size(p3,order=1)),digits=1),"\nSWI = ", round(SWtestE(p3,N=100)$valuesAV$SWI,digits=2),"\nPLF = ",round(PLF3,digits=2),sep=""))

set.seed(200)
# Barabasi power = 0
p4  <- plotBA(n=n,pwr=0,out.dist=hist(degree(p1,mode="all"),breaks=(0:n),plot=F)$density)
PLF4<- PLFsmall(p4)
p44 <- plot(p4,main="power = 0",layout=layout1,xlab=paste("FON = ",round(mean(neighborhood.size(p4,order=1)),digits=1),"\nSWI = ", round(SWtestE(p4,N=100)$valuesAV$SWI,digits=2),"\nPLF = ",round(PLF4,digits=2),sep=""))

# Barabasi power = 2
p5  <- plotBA(n=n,pwr=2,out.dist=hist(degree(p2,mode="all"),breaks=(0:n),plot=F)$density)
PLF5<- PLFsmall(p5)
p55 <- plot(p5,main="power = 2",layout=layout1,xlab=paste("FON = ",round(mean(neighborhood.size(p5,order=1)),digits=1),"\nSWI = ", round(SWtestE(p5,N=100)$valuesAV$SWI,digits=2),"\nPLF = ",round(PLF5,digits=2),sep=""))

# Barabasi power = 4
p6  <- plotBA(n=n,pwr=4,out.dist=hist(degree(p3,mode="all"),breaks=(0:n),plot=F)$density)
PLF6<- PLFsmall(p6)
p66 <- plot(p6,main="power = 4",layout=layout1,xlab=paste("FON = ",round(mean(neighborhood.size(p6,order=1)),digits=1),"\nSWI = ", round(SWtestE(p6,N=100)$valuesAV$SWI,digits=2),"\nPLF = ",round(PLF6,digits=2),sep=""))
par(mfrow=c(1,1))
```


[| Jump to solutions |](#netssol)


## Package `qgraph`

Learn about the functionality of the `qgraph` frm its author Sacha Epskamp.

Try running the examples, e.g. of the `Big 5`: 

[http://sachaepskamp.com/qgraph/examples](http://sachaepskamp.com/qgraph/examples)


## `qgraph` tutorials / blog

Great site by Eiko Fried:
[http://psych-networks.com ](http://psych-networks.com )


## EXTRA - early warnings

Have a look at the site [Early Warning Systems](http://www.early-warning-signals.org/home/)

There is an accompanying [`R` package `earlywarnings`](https://cran.r-project.org/web/packages/earlywarnings/index.html)




<!--chapter:end:DSC_ASSIGNMENTS.Rmd-->

---
title: "An Introduction to Analytic Toolbox of Complexity Science"
author: "Fred Hasselman & Maarten Wijnants"
date: "1/14/2019"
output: 
  bookdown::html_document2: 
    fig_caption: yes
    highlight: pygments
    keep_md: yes
    self_contained: yes
    number_sections: yes
    theme: spacelab
    toc: yes
    toc_float: true
    collapsed: false
    smooth_scroll: true
    code_folding: show
    bibliography: [refs.bib, packages.bib]
    biblio-style: apalike
    csl: apa.csl
    includes:
        before_body: assignmentstyle.html
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(include=TRUE)
```

**Quick links**

* [Assignments Session 1&2: Introduction to the mathematics of change](https://darwin.pwo.ru.nl/skunkworks/courseware/1819_DCS/assignments/ASSIGNMENTS_P1A.html)
* [Assignments Session 3: Basic Time series Analysis](https://darwin.pwo.ru.nl/skunkworks/courseware/1819_DCS/assignments/ASSIGNMENTS_P2.html)
* [Assignments Session 4: Time Series Analysis: Temporal Correlations and Fractal Scaling](https://darwin.pwo.ru.nl/skunkworks/courseware/1819_DCS/assignments/ASSIGNMENTS_P2B.html)
* [Assignments Session 5: Quantifying Recurrences in State Space](https://darwin.pwo.ru.nl/skunkworks/courseware/1819_DCS/assignments/ASSIGNMENTS_P3.html)
* [Assignments Session 6: CRQA](https://darwin.pwo.ru.nl/skunkworks/courseware/1819_DCS/assignments/ASSIGNMENTS_P4.html)
* [Assignments Session 7: Detecting Early Warning Signals of Phase Transitions in Categorical and Continuous Time series](https://darwin.pwo.ru.nl/skunkworks/courseware/1819_DCS/assignments/ASSIGNMENTS_P5.html)
* [Assignments Session 8: Complex Networks](https://darwin.pwo.ru.nl/skunkworks/courseware/1819_DCS/assignments/ASSIGNMENTS_P6.html)


# **Assignments for the Complex Systems Approach Book** {-}

These assignments were designed to prepare you for "real world" modelling and data analysis problems. That is, after completing the assignments you should be able to decide whether the phenomenon you study could benefit from a complex systems approach and which type of analyses would be a good place to start. The models and techniques discussed here are **not** a definite collection of available techniques, this is really just the tip of the iceberg.

### General Guidelines {-}

* Read the instructions carefully.
* Do not skip any of the steps.
* Do not copy-paste from the assignment text into a spreadsheet or syntax editor (except for text in `code blocks`).
* Study the solutions and lecture notes.



```{r setup2, message=FALSE, warning=FALSE, include=FALSE}
knitr::opts_chunk$set(
	message = FALSE,
	warning = FALSE,
	include = FALSE
)

# Install casnet if necessary: https://github.com/FredHasselman/casnet
# !!Warning!! Very Beta...

# Uncomment and run
# devtools::install_github("FredHasselman/casnet")

library(plyr)
library(tidyverse)
library(png)
library(readxl)
library(rio)
library(lattice)
library(htmlTable)
library(htmlwidgets)
library(lubridate)
library(DT)
```   

## **Four main topics** {-}

**I. Introduction to the mathematics of change**

- Modelling (nonlinear) growth and Deterministic Chaos ([Lecture 1 - Assignments](https://darwin.pwo.ru.nl/skunkworks/courseware/1819_DCS/assignments/ASSIGNMENTS_P1A.html))
- Multivariate systems: Predator-Prey dynamics ([Lecture 2 - Assignments](https://darwin.pwo.ru.nl/skunkworks/courseware/1819_DCS/assignments/ASSIGNMENTS_P1A.html))
- Nonlinear regression: Fitting parameters of analytic solutions. ([Lecture 3 - Assignments](https://darwin.pwo.ru.nl/skunkworks/courseware/1819_DCS/assignments/ASSIGNMENTS_P2.html))

**II. Time Series Analysis: Temporal Correlations and Fractal Scaling**

- Basic timeseries analysis: Quantifying order (auto/cross correlation functions) and disorder (sample entropy, relative roughness) in time series ([Lecture 3 - Assignments](https://darwin.pwo.ru.nl/skunkworks/courseware/1819_DCS/assignments/ASSIGNMENTS_P2.html#3_basic_timeseries_analysis))
- Scaling phenomena I: How to measure complexity? Fluctuation analyses (DFA, SDA and spectral slope) and global fractal dimension ([Lecture 4 - Assignments](https://darwin.pwo.ru.nl/skunkworks/courseware/1819_DCS/assignments/ASSIGNMENTS_P2B.html))
- Scaling phenomena II: Global versus local fractal dimension, fractal physiology and complexity matching ([Lecture 4 - Assignments](https://darwin.pwo.ru.nl/skunkworks/courseware/1819_DCS/assignments/ASSIGNMENTS_P2.html))

**III. Quantifying Recurrences in State Space**

- Takens Theorem and State-Space reconstruction ([Lecture 5 - Assignments](https://darwin.pwo.ru.nl/skunkworks/courseware/1819_DCS/assignments/ASSIGNMENTS_P3.html))
- Recurrence Quantification Analysis (RQA) of continuous and categorical data ([Lecture 5 & 6 - Assignments](https://darwin.pwo.ru.nl/skunkworks/courseware/1819_DCS/assignments/ASSIGNMENTS_P3.html))
- Cross-Recurrence Quantification Analysis (CRQA) of dyadic interaction ([Lecture 6 - Assignments](https://darwin.pwo.ru.nl/skunkworks/courseware/1819_DCS/assignments/ASSIGNMENTS_P3B.html))
- Potential Functions, Catastrophe Theory & Early Warning Signals of order transitions in complex systems ([Lecture 7 - Assignments](https://darwin.pwo.ru.nl/skunkworks/courseware/1819_DCS/assignments/ASSIGNMENTS_P5.html))

**IV. Complex Networks**

- Small-world and Scale-free networks ([Lecture 8 - Assignments](https://darwin.pwo.ru.nl/skunkworks/courseware/1819_DCS/assignments/ASSIGNMENTS_P6.html))
- Symptom networks and Networks of Quantified Recurrences ([Lecture 8 - Assignments ](https://darwin.pwo.ru.nl/skunkworks/courseware/1819_DCS/assignments/ASSIGNMENTS_P6.html))
- Early Warning Signals of order transitions in clinical settings ([Lecture 8 - Assignments](https://darwin.pwo.ru.nl/skunkworks/courseware/1819_DCS/assignments/ASSIGNMENTS_P6.html))


## Using `R`! {-}

We recommend installing the latest version of [**R**](https://www.r-project.org) and [**RStudio**](https://www.rstudio.com). Rstudio is not strictly necessary, but especially new users will have a somewhat more comfortable expe**R**ience. If you are completeley new to `R` you might want to [check these notes in the course book](https://darwin.pwo.ru.nl/skunkworks/courseware/1819_DCS/some-notes-on-using-r.html)


### Packages needed for the assignments {-}

If you are working from a private computer, you'll need to install the packages below, just copy and paste the command in `R`. Depending on your computer and internet connection, this might take a while to complete. If you run into any errors, skip the package and try the others, e.g. by adding them through the user interface of **Rstudio** (`Tools >> Installl Packages...`).

These packages are already installed in the computer rooms at the university, except for our custom package `casnet`.

```{r, eval=FALSE, include=TRUE}
# If you install casnet using the code below, you will not need to run this code below
# install.packages(c("devtools", "rio","plyr", "tidyverse","Matrix", "ggplot2", "lattice", "latticeExtra", "grid", "gridExtra", "scales", "dygraphs","rgl", "plot3D","fractal", "nonlinearTseries", "crqa","signal", "sapa", "ifultools", "pracma", "nlme", "lme4", "lmerTest", "minpack.lm", "igraph","qgraph","graphicalVAR","bootGraph","IsingSampler","IsingFit"), dependencies = TRUE)

# Install casnet if necessary: https://github.com/FredHasselman/casnet
# !!Warning!! Very Beta...
# Auto config of Marwan's commandline rqa works on Windows, MacOS and probably Linux as well
library(devtools)
devtools::install_github("FredHasselman/invctr")
devtools::install_github("FredHasselman/casnet")
```

> NOTE: Sometimes R will ask whether you want to install a newer version of a package which still has to be built from the source code. I would suggest to select NO, because this will take more time and might cause problems. 


### Files on GitHub {-}

All the files (data, scripts and files that generated this document) are in a repository on [Github](https://github.com/FredHasselman/The-Complex-Systems-Approach-Book). Github keeps track of all the different versions of the files in a repository.

* If you want to download a file that is basically a text file (e.g. an `R` script), find a button on the page named `raw`, press it and copy the text in your browser, or save it as a text file.
* For non-text files, a `download` button will be present somewhere on the page.

The main package we use will be `casnet` the [**Complex Adaptive Systems and NETworks**](https://github.com/FredHasselman/casnet) toolbox. Here is a nice page which contains all the manual entries https://fredhasselman.github.io/casnet/index.html. 

If this toolbox for some reason does install correctly you can try to download and source the file containing most of the functions we'll use.
First, download the files `nlRtsa_SOURCE.R` and `prepare_command_line_crp.R` from [Github](https://github.com/FredHasselman/casnet/R) and type `source('nlRtsa_SOURCE.R')` and `source('prepare_command_line_crp.R')` in `R`. Alternatively, source the directly from Github if you have package `devtools` installed.
```{r intro1, echo=TRUE, message=FALSE, warning=FALSE, include=TRUE}
# Only use this if the regular installation procedure does not work!!!
library(devtools)
source_url("https://raw.githubusercontent.com/FredHasselman/casnet/master/R/prepare_command_line_crp.R")
source_url("https://raw.githubusercontent.com/FredHasselman/casnet/master/R/nlRtsa_SOURCE.R")
```

### Working with timeseries in `R` {-}

There are many different ways to handle and plot timeseries data in `R`, I summarised some of them [in the course book](https://darwin.pwo.ru.nl/skunkworks/courseware/1718_DCS/plotTS.html)

<!--chapter:end:index_old.Rmd-->

---
title: "Untitled"
author: "Fred Hasselman"
date: "2/8/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r cars}
summary(cars)
```

## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.

<!--chapter:end:trst.Rmd-->

